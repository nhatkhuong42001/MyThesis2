{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff47cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vnstock in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.2.9.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U vnstock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb8fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ta in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ta) (1.21.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ta) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->ta) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->ta) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3f982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nhatk\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.32.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.5.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b3fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-self-attention in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.51.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-self-attention) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40e2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-tuner) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nhatk\\appdata\\roaming\\python\\python37\\site-packages (from keras-tuner) (24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-tuner) (2.31.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (2022.5.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe155f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (2022.5.18.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from attrs>=20.1.0->trio~=0.17->selenium) (6.7.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->attrs>=20.1.0->trio~=0.17->selenium) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b9a113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 👋 Chào mừng bạn đến với Vnstock!\n",
       "\n",
       "Cảm ơn bạn đã sử dụng package phân tích chứng khoán #1 tại Việt Nam\n",
       "\n",
       "* Tài liệu: [Sổ tay hướng dẫn](https://vnstocks.com/docs/category/s%E1%BB%95-tay-h%C6%B0%E1%BB%9Bng-d%E1%BA%ABn)\n",
       "* Cộng đồng: [Nhóm Facebook](https://www.facebook.com/groups/vnstock.official)\n",
       "\n",
       "Khám phá các tính năng mới nhất và tham gia cộng đồng để nhận hỗ trợ.\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Phiên bản Vnai 2.1.9 đã có mặt, vui lòng cập nhật với câu lệnh : `pip install vnai --upgrade`.\n",
       "Lịch sử phiên bản: https://pypi.org/project/vnai/#history\n",
       "Phiên bản hiện tại 2.0.4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ta.momentum import RSIIndicator\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, RNN, concatenate\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from vnstock import *\n",
    "from ta.volatility import BollingerBands\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, Flatten, BatchNormalization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from ta.momentum import StochasticOscillator\n",
    "from ta.volume import OnBalanceVolumeIndicator\n",
    "from ta.trend import CCIIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ta.volume import ChaikinMoneyFlowIndicator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras import regularizers\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2aea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "548b71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f21814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_previous_and_last_year_quarter(input_date):\n",
    "    # Lấy tháng và năm từ input_date\n",
    "    month = input_date.month\n",
    "    year = input_date.year\n",
    "\n",
    "    # Xác định quý hiện tại dựa vào tháng\n",
    "    if 1 <= month <= 3:\n",
    "        current_quarter = 1\n",
    "    elif 4 <= month <= 6:\n",
    "        current_quarter = 2\n",
    "    elif 7 <= month <= 9:\n",
    "        current_quarter = 3\n",
    "    else:\n",
    "        current_quarter = 4\n",
    "\n",
    "    # Tìm quý trước đó\n",
    "    previous_quarter = current_quarter - 1\n",
    "    previous_year = year\n",
    "\n",
    "    # Nếu quý trước là 0, thì sẽ chuyển về quý 4 của năm trước\n",
    "    if previous_quarter == 0:\n",
    "        previous_quarter = 4\n",
    "        previous_year -= 1\n",
    "\n",
    "    # Xác định quý cùng kỳ năm trước\n",
    "    last_year = previous_year - 1\n",
    "\n",
    "    # Format kết quả\n",
    "    previous_quarter_str = f'Q{previous_quarter}-{previous_year}'\n",
    "    last_year_quarter_str = f'Q{previous_quarter}-{last_year}'\n",
    "\n",
    "    # return [previous_quarter_str, last_year_quarter_str]\n",
    "    return previous_quarter, previous_year, last_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16fd5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2023 2022\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ sử dụng\n",
    "previous_quarter, previous_year, last_year = get_previous_and_last_year_quarter(datetime.date(2024, 1, 10)) #This was changed from datetime.date to datetime.date\n",
    "print(previous_quarter, previous_year, last_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc0afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_quarters(input_date, n=8):\n",
    "    \"\"\"\n",
    "    Trả về danh sách [(quý, năm)] của n quý GẦN NHẤT, không bao gồm quý hiện tại.\n",
    "    \"\"\"\n",
    "    quarters = []\n",
    "    current_date = pd.to_datetime(input_date)\n",
    "\n",
    "    # Bắt đầu từ quý TRƯỚC của ngày input\n",
    "    current_date -= relativedelta(months=3)\n",
    "\n",
    "    for _ in range(n):\n",
    "        quarter = (current_date.month - 1) // 3 + 1\n",
    "        year = current_date.year\n",
    "        quarters.append((quarter, year))\n",
    "        current_date -= relativedelta(months=3)\n",
    "\n",
    "    return quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "044dd64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 2024), (2, 2024), (1, 2024), (4, 2023), (3, 2023), (2, 2023), (1, 2023), (4, 2022)]\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ sử dụng\n",
    "quarter = get_last_n_quarters(datetime.date(2024, 10, 10), n=8)\n",
    "print(quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab35f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_quarter_end_date(input_date):\n",
    "    \"\"\"\n",
    "    Trả về ngày kết thúc của quý gần nhất TRƯỚC input_date\n",
    "    \"\"\"\n",
    "    input_date = pd.to_datetime(input_date)\n",
    "    year = input_date.year\n",
    "    month = input_date.month\n",
    "\n",
    "    if month <= 3:\n",
    "        return pd.Timestamp(year=year-1, month=12, day=31)\n",
    "    elif month <= 6:\n",
    "        return pd.Timestamp(year=year, month=3, day=31)\n",
    "    elif month <= 9:\n",
    "        return pd.Timestamp(year=year, month=6, day=30)\n",
    "    else:\n",
    "        return pd.Timestamp(year=year, month=9, day=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8823b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "latest_quarter_end_date = get_latest_quarter_end_date(datetime.date(2024, 1, 10))\n",
    "print(latest_quarter_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed1be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm thu thập dữ liệu phân tích cở bản và phân tích kỹ thuật theo mã cổ phiếu và ngày giao dịch.\n",
    "def crawl_stock_data_FA_TA(symbol, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Crawl dữ liệu lịch sử giá cổ phiếu cho danh sách mã, lưu mỗi mã vào 1 file Excel riêng.\n",
    "\n",
    "    Parameters:\n",
    "        symbols (list): Danh sách mã cổ phiếu (ví dụ: ['VCB', 'FPT', 'MWG'])\n",
    "        start_date (str): Ngày bắt đầu (ví dụ: '2020-01-01')\n",
    "        end_date (str): Ngày kết thúc (ví dụ: '2025-01-01')\n",
    "        save_folder (str): Tên thư mục để lưu file Excel (mặc định là 'stock_data')\n",
    "    \"\"\"\n",
    "\n",
    "    # Tạo thư mục lưu file nếu chưa tồn tại\n",
    "    # os.makedirs(save_folder, exist_ok=True)\n",
    "    indicators = {\n",
    "    'P/B': ('Chỉ tiêu định giá', 'P/B'),\n",
    "    'P/E': ('Chỉ tiêu định giá', 'P/E'),\n",
    "    'P/S': ('Chỉ tiêu định giá', 'P/S'),\n",
    "    'P/Cash Flow': ('Chỉ tiêu định giá', 'P/Cash Flow'),\n",
    "    'EPS (VND)': ('Chỉ tiêu định giá', 'EPS (VND)'),\n",
    "    'BVPS (VND)': ('Chỉ tiêu định giá', 'BVPS (VND)'),\n",
    "    'EV/EBITDA': ('Chỉ tiêu định giá', 'EV/EBITDA'),\n",
    "    'ROE (%)': ('Chỉ tiêu khả năng sinh lợi', 'ROE (%)'),\n",
    "    'ROIC (%)': ('Chỉ tiêu khả năng sinh lợi', 'ROIC (%)'),\n",
    "    'ROA (%)': ('Chỉ tiêu khả năng sinh lợi', 'ROA (%)'),\n",
    "}\n",
    "\n",
    "    # for symbol in symbols:\n",
    "    try:\n",
    "            print(f\"Đang lấy dữ liệu cho: {symbol}...\")\n",
    "            stock = Vnstock().stock(symbol=symbol)\n",
    "            df = stock.quote.history(start=start_date, end=end_date)\n",
    "            finance_data = stock.finance.ratio(period='quarter', lang='vi', dropna=True)\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                input_date = row['time']\n",
    "                prev_quarter, prev_year, year_before = get_previous_and_last_year_quarter(input_date)\n",
    "\n",
    "                for name, multi_idx in indicators.items():\n",
    "                    col_prev = f\"{name}_Previous_Quarter\"\n",
    "                    col_last_year = f\"{name}_Same_Period_Last_Year\"\n",
    "\n",
    "                    # Giá trị quý trước\n",
    "                    try:\n",
    "                        value_prev = finance_data.loc[\n",
    "                            (finance_data[('Meta', 'Năm')].astype(str) == str(prev_year)) &\n",
    "                            (finance_data[('Meta', 'Kỳ')].astype(str) == str(prev_quarter)),\n",
    "                            multi_idx\n",
    "                        ].values\n",
    "                        df.at[index, col_prev] = value_prev[0] if len(value_prev) > 0 else None\n",
    "                    except:\n",
    "                        df.at[index, col_prev] = None\n",
    "\n",
    "                    # Giá trị cùng kỳ năm trước\n",
    "                    try:\n",
    "                        value_last_year = finance_data.loc[\n",
    "                            (finance_data[('Meta', 'Năm')].astype(str) == str(year_before)) &\n",
    "                            (finance_data[('Meta', 'Kỳ')].astype(str) == str(prev_quarter)),\n",
    "                            multi_idx\n",
    "                        ].values\n",
    "                        df.at[index, col_last_year] = value_last_year[0] if len(value_last_year) > 0 else None\n",
    "                    except:\n",
    "                        df.at[index, col_last_year] = None\n",
    "\n",
    "                    # Thêm 8 quý gần nhất\n",
    "                    recent_quarters = get_last_n_quarters(input_date, n=8)\n",
    "                    for i, (q, y) in enumerate(recent_quarters):\n",
    "                        col_recent = f\"{name}_d_{i+1}\"\n",
    "                        try:\n",
    "                            value = finance_data.loc[\n",
    "                                (finance_data[('Meta', 'Năm')].astype(str) == str(y)) &\n",
    "                                (finance_data[('Meta', 'Kỳ')].astype(str) == str(q)),\n",
    "                                multi_idx\n",
    "                            ].values\n",
    "                            df.at[index, col_recent] = value[0] if len(value) > 0 else 0\n",
    "                        except:\n",
    "                            df.at[index, col_recent] = 0\n",
    "                    # Tính khoảng cách từ input_date đến cuối quý gần nhất\n",
    "                    try:\n",
    "                        end_of_last_quarter = get_latest_quarter_end_date(input_date)\n",
    "                        distance = (pd.to_datetime(input_date) - end_of_last_quarter).days\n",
    "                        df.at[index, 'distance_to_nearest_quarter'] = distance\n",
    "                    except:\n",
    "                        df.at[index, 'distance_to_nearest_quarter'] = None\n",
    "\n",
    "            df['ticket'] = symbol  # 👉 Thêm cột 'ticket'\n",
    "            # file_path = os.path.join(save_folder, f\"{symbol}.xlsx\")\n",
    "            # df.to_excel(file_path, index=False)\n",
    "            print(f\"✅ Đã thu thập dữ liệu Phân tích cơ bản và phân tích kỹ thuật cho {symbol} ngày {input_date}\")\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi xử lý mã {symbol}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd6445e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(symbol, start_date, end_date, drop_na=True):\n",
    "    data = crawl_stock_data_FA_TA(symbol, start_date, end_date)\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    df = data\n",
    "\n",
    "    columns_to_drop = ['EV/EBITDA_Previous_Quarter','EV/EBITDA_Same_Period_Last_Year','ROIC (%)_Previous_Quarter','ROIC (%)_Same_Period_Last_Year']  # thay bằng danh sách cột bạn muốn xóa\n",
    "    # doi ten o day\n",
    "    \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "     # Tính toán khối lượng trung bình\n",
    "    volume_ma_period = 20  # Ví dụ: khối lượng trung bình 20 ngày\n",
    "    df['volume_ma'] = df['volume'].rolling(window=volume_ma_period).mean()\n",
    "\n",
    "\n",
    "    # Tính tỷ lệ khối lượng hiện tại / khối lượng trung bình\n",
    "    df['volume_to_volume_ma_ratio'] = df['volume'] / df['volume_ma']\n",
    "\n",
    "\n",
    "    # Tính toán EMA ngắn hạn (12 ngày)\n",
    "    df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "\n",
    "    # Tính toán EMA dài hạn (26 ngày)\n",
    "    df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # Tính toán SMA trung hạn (20 ngày)\n",
    "    df['sma_20'] = df['close'].rolling(window=20).mean()\n",
    "\n",
    "    # Tính toán SMA dài hạn (50 ngày)\n",
    "    df['sma_50'] = df['close'].rolling(window=50).mean()\n",
    "\n",
    "     # Tính toán ROC\n",
    "\n",
    "    df['roc_1'] = ((df['close'] - df['close'].shift(1)) / df['close'].shift(1)) * 100\n",
    "    df['roc_5'] = ((df['close'] - df['close'].shift(5)) / df['close'].shift(5)) * 100\n",
    "    df['roc_9'] = ((df['close'] - df['close'].shift(9)) / df['close'].shift(9)) * 100\n",
    "\n",
    "    # Tính toán %K (Stochastic Oscillator)\n",
    "    stoch_period = 14\n",
    "    df['%K'] = ((df['close'] - df['low'].rolling(window=stoch_period).min()) /\n",
    "                (df['high'].rolling(window=stoch_period).max() - df['low'].rolling(window=stoch_period).min())) * 100\n",
    "\n",
    "    # Tính toán %R (Williams %R)\n",
    "    df['%R'] = ((df['high'].rolling(window=stoch_period).max() - df['close']) /\n",
    "                (df['high'].rolling(window=stoch_period).max() - df['low'].rolling(window=stoch_period).min())) * -100\n",
    "\n",
    "    # Tính toán CCI (Commodity Channel Index)\n",
    "    cci_period = 20\n",
    "    df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3\n",
    "    df['cci'] = CCIIndicator(high=df['high'], low=df['low'], close=df['close'], window=cci_period).cci()\n",
    "\n",
    "    # Tính toán OBV (On Balance Volume)\n",
    "    df['obv'] = OnBalanceVolumeIndicator(close=df['close'], volume=df['volume']).on_balance_volume()\n",
    "\n",
    "\n",
    "    # Tính toán MACD\n",
    "    df['macd'] = df['ema_12'] - df['ema_26']\n",
    "    # Tính toán Signal line (EMA 9 ngày của MACD)\n",
    "    df['signal_line'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    # Tính toán MACD histogram\n",
    "    df['macd_histogram'] = df['macd'] - df['signal_line']\n",
    "\n",
    "\n",
    "    # Tính toán RSI\n",
    "    rsi_period = 14\n",
    "    df['rsi'] = RSIIndicator(df['close'], window=rsi_period).rsi()\n",
    "    ma_period = 9\n",
    "    df['rsi_base_ma'] = df['rsi'].rolling(window=ma_period).mean()\n",
    "    df['rsi_rsi_base_ma_ratio'] = df['rsi'] / df['rsi_base_ma']\n",
    "\n",
    "\n",
    "    # Bollinger Bands\n",
    "    indicator_bb = BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "    df['bb_bbm'] = indicator_bb.bollinger_mavg()\n",
    "    df['bb_bbh'] = indicator_bb.bollinger_hband()\n",
    "    df['bb_bbl'] = indicator_bb.bollinger_lband()\n",
    "    df['bb_bbp'] = indicator_bb.bollinger_pband()\n",
    "\n",
    "    df['bb_bbh_bb_bbl_ratio'] = df['bb_bbh'] / df['bb_bbl']\n",
    "\n",
    "    df['hl_ratio'] = df['high'] / (df['low'] + 1e-8)\n",
    "    df['co_ratio'] = df['close'] / (df['open'] + 1e-8)\n",
    "    df['price_range'] = (df['high'] - df['low']) / (df['close'] + 1e-8)\n",
    "\n",
    "    if 'sma_20' in df.columns and 'sma_50' in df.columns:\n",
    "        df['sma_ratio_20_50'] = df['sma_20'] / (df['sma_50'] + 1e-8)\n",
    "    if 'ema_12' in df.columns and 'ema_26' in df.columns:\n",
    "        df['ema_ratio_12_26'] = df['ema_12'] / (df['ema_26'] + 1e-8)\n",
    "\n",
    "    if all(col in df.columns for col in ['bb_bbh', 'bb_bbl', 'bb_bbm']):\n",
    "        df['bb_width'] = (df['bb_bbh'] - df['bb_bbl']) / (df['bb_bbm'] + 1e-8)\n",
    "        df['bb_position'] = (df['close'] - df['bb_bbl']) / (df['bb_bbh'] - df['bb_bbl'] + 1e-8)\n",
    "\n",
    "    if 'rsi' in df.columns:\n",
    "        df['rsi_overbought'] = (df['rsi'] > 70).astype(int)\n",
    "        df['rsi_oversold'] = (df['rsi'] < 30).astype(int)\n",
    "        df['rsi_neutral'] = ((df['rsi'] >= 30) & (df['rsi'] <= 70)).astype(int)\n",
    "\n",
    "    if all(col in df.columns for col in ['macd', 'signal_line']):\n",
    "        df['macd_signal_diff'] = df['macd'] - df['signal_line']\n",
    "        df['macd_bullish'] = (df['macd'] > df['signal_line']).astype(int)\n",
    "\n",
    "    for period in [5, 10]:\n",
    "        if len(df) > period:\n",
    "            df[f'momentum_{period}'] = df['close'].pct_change(period)\n",
    "\n",
    "    # =============================================================\n",
    "\n",
    "    # =======================================================================\n",
    "\n",
    "    # 1. Daily log return\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "    # 2. Rolling volatility (biến động trượt)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        df[f'volatility_{window}d'] = df['log_return'].rolling(window=window).std()\n",
    "\n",
    "    # 3. Rolling mean log return\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        df[f'mean_log_return_{window}d'] = df['log_return'].rolling(window=window).mean()\n",
    "\n",
    "    # 4. Sharpe-like ratio (mean return / volatility)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col = f'mean_log_return_{window}d'\n",
    "        vol_col = f'volatility_{window}d'\n",
    "        df[f'sharpe_like_{window}d'] = df[mean_col] / (df[vol_col] + 1e-9)\n",
    "\n",
    "    # 5. Directional Feature - chuỗi ngày tăng liên tiếp\n",
    "    df['up_streak'] = (df['log_return'] > 0).astype(int)\n",
    "    df['up_streak'] = df['up_streak'] * (df['up_streak'].groupby((df['up_streak'] != df['up_streak'].shift()).cumsum()).cumcount() + 1)\n",
    "    df['up_streak'] = df['up_streak'].where(df['log_return'] > 0, 0)\n",
    "\n",
    "    # 6. Tỷ lệ ngày log return dương trong 20 ngày gần nhất\n",
    "    df['pos_log_return_ratio_20d'] = df['log_return'].rolling(window=20).apply(lambda x: np.mean(x > 0), raw=True)\n",
    "\n",
    "    # 7. Z-score của log return hiện tại\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col = f'mean_log_return_{window}d'\n",
    "        vol_col = f'volatility_{window}d'\n",
    "        df[f'z_score_{window}d'] = (df['log_return'] - df[mean_col]) / (df[vol_col] + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "    # Tính Annual Return (lợi nhuận hàng năm)\n",
    "    df['annual_return'] = df['close'] / df['close'].shift(252) - 1  # 252 ngày giao dịch trong năm\n",
    "\n",
    "    # Tính Volatility (Độ biến động) hàng năm (Standard Deviation of Returns)\n",
    "    df['daily_return'] = df['close'].pct_change()  # Tỷ suất lợi nhuận hàng ngày\n",
    "    df['annual_volatility'] = df['daily_return'].rolling(window=252).std() * (252 ** 0.5)  # Độ lệch chuẩn hàng năm (tính từ tỷ suất lợi nhuận hàng ngày)\n",
    "\n",
    "    risk_free_rate = 0.03  # tương đương 3%/năm\n",
    "    # Tính Sharpe ratio: (Rp - Rf) / Volatility\n",
    "    df['sharpe_ratio'] = (df['annual_return'] - risk_free_rate) / df['annual_volatility']\n",
    "\n",
    "\n",
    "    # Tính toán chỉ số cơ bản ==========================================================\n",
    "    df['P/B_change_rate_flag'] = np.where(df['P/B_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/B_change_rate'] = np.where(\n",
    "        df['P/B_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/B_Previous_Quarter'] - df['P/B_Same_Period_Last_Year']) / df['P/B_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # P/E\n",
    "    df['P/E_change_rate_flag'] = np.where(df['P/E_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/E_change_rate'] = np.where(\n",
    "        df['P/E_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/E_Previous_Quarter'] - df['P/E_Same_Period_Last_Year']) / df['P/E_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # P/S\n",
    "    df['P/S_change_rate_flag'] = np.where(df['P/S_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/S_change_rate'] = np.where(\n",
    "        df['P/S_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/S_Previous_Quarter'] - df['P/S_Same_Period_Last_Year']) / df['P/S_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # P/Cash Flow\n",
    "    df['P/Cash Flow_change_rate_flag'] = np.where(df['P/Cash Flow_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/Cash Flow_change_rate'] = np.where(\n",
    "        df['P/Cash Flow_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/Cash Flow_Previous_Quarter'] - df['P/Cash Flow_Same_Period_Last_Year']) / df['P/Cash Flow_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # EPS (VND)\n",
    "    df['EPS (VND)_change_rate_flag'] = np.where(df['EPS (VND)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['EPS (VND)_change_rate'] = np.where(\n",
    "        df['EPS (VND)_Same_Period_Last_Year'] != 0,\n",
    "        (df['EPS (VND)_Previous_Quarter'] - df['EPS (VND)_Same_Period_Last_Year']) / df['EPS (VND)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # BVPS (VND)\n",
    "    df['BVPS (VND)_change_rate_flag'] = np.where(df['BVPS (VND)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['BVPS (VND)_change_rate'] = np.where(\n",
    "        df['BVPS (VND)_Same_Period_Last_Year'] != 0,\n",
    "        (df['BVPS (VND)_Previous_Quarter'] - df['BVPS (VND)_Same_Period_Last_Year']) / df['BVPS (VND)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # ROE (%)\n",
    "    df['ROE (%)_change_rate_flag'] = np.where(df['ROE (%)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['ROE (%)_change_rate'] = np.where(\n",
    "        df['ROE (%)_Same_Period_Last_Year'] != 0,\n",
    "        (df['ROE (%)_Previous_Quarter'] - df['ROE (%)_Same_Period_Last_Year']) / df['ROE (%)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # ROA (%)\n",
    "    df['ROA (%)_change_rate_flag'] = np.where(df['ROA (%)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['ROA (%)_change_rate'] = np.where(\n",
    "        df['ROA (%)_Same_Period_Last_Year'] != 0,\n",
    "        (df['ROA (%)_Previous_Quarter'] - df['ROA (%)_Same_Period_Last_Year']) / df['ROA (%)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # ==================================================================================================\n",
    "\n",
    "\n",
    "    # Xứ lý với VNINDEX =============================================================================\n",
    "    data_time = data['time']\n",
    "    stock = Vnstock().stock(symbol=\"VNINDEX\")\n",
    "    data_vni = stock.quote.history(start=start_date, end=end_date)\n",
    "    data_vni.columns = ['time','open_vni', 'high_vni', 'low_vni', 'close_vni', 'volume_vni']\n",
    "\n",
    "\n",
    "\n",
    "    # Tính toán RSI\n",
    "    rsi_period = 14\n",
    "    data_vni['rsi_vni'] = RSIIndicator(data_vni['close_vni'], window=rsi_period).rsi()\n",
    "    # Tính toán RSI-base-MA\n",
    "    ma_period = 9\n",
    "    data_vni['rsi_base_ma_vni'] = data_vni['rsi_vni'].rolling(window=ma_period).mean()\n",
    "    data_vni['rsi_rsi_base_ma_ratio_vni'] = data_vni['rsi_vni'] / data_vni['rsi_base_ma_vni']\n",
    "\n",
    "    # Tính toán khối lượng trung bình\n",
    "    volume_ma_period = 20  # Ví dụ: khối lượng trung bình 20 ngày\n",
    "    data_vni['volume_ma_vni'] = data_vni['volume_vni'].rolling(window=volume_ma_period).mean()\n",
    "    # Tính tỷ lệ khối lượng hiện tại / khối lượng trung bình\n",
    "    data_vni['volume_to_volume_ma_ratio_vni'] = data_vni['volume_vni'] / data_vni['volume_ma_vni']\n",
    "\n",
    "    # Bollinger Bands\n",
    "    indicator_bb = BollingerBands(close=data_vni['close_vni'], window=20, window_dev=2)\n",
    "    data_vni['bb_bbm_vni'] = indicator_bb.bollinger_mavg()\n",
    "    data_vni['bb_bbh_vni'] = indicator_bb.bollinger_hband()\n",
    "    data_vni['bb_bbl_vni'] = indicator_bb.bollinger_lband()\n",
    "    data_vni['bb_bbp_vni'] = indicator_bb.bollinger_pband()\n",
    "\n",
    "    data_vni['bb_bbh_bb_bbl_ratio_vni'] = data_vni['bb_bbh_vni'] / data_vni['bb_bbl_vni']\n",
    "\n",
    "    \n",
    "\n",
    "    # Tính toán ROC\n",
    "    roc_period = 9  # Chu kỳ 9 ngày, bạn có thể thay đổi tùy ý\n",
    "    # data_vni['roc_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(roc_period)) / data_vni['close_vni'].shift(roc_period)) * 100\n",
    "    data_vni['roc_1_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(1)) / data_vni['close_vni'].shift(1)) * 100\n",
    "    data_vni['roc_5_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(5)) / data_vni['close_vni'].shift(5)) * 100\n",
    "    data_vni['roc_9_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(9)) / data_vni['close_vni'].shift(9)) * 100\n",
    "    # Tính toán %K (Stochastic Oscillator)\n",
    "    stoch_period = 14\n",
    "    data_vni['%K_vni'] = ((data_vni['close_vni'] - data_vni['low_vni'].rolling(window=stoch_period).min()) /\n",
    "                (data_vni['high_vni'].rolling(window=stoch_period).max() - data_vni['low_vni'].rolling(window=stoch_period).min())) * 100\n",
    "\n",
    "    # Tính toán %R (Williams %R)\n",
    "    data_vni['%R_vni'] = ((data_vni['high_vni'].rolling(window=stoch_period).max() - data_vni['close_vni']) /\n",
    "                (data_vni['high_vni'].rolling(window=stoch_period).max() - data_vni['low_vni'].rolling(window=stoch_period).min())) * -100\n",
    "\n",
    "    # Tính toán CCI (Commodity Channel Index)\n",
    "    cci_period = 20\n",
    "    data_vni['typical_price_vni'] = (data_vni['high_vni'] + data_vni['low_vni'] + data_vni['close_vni']) / 3\n",
    "    data_vni['cci_vni'] = CCIIndicator(high=data_vni['high_vni'], low=data_vni['low_vni'], close=data_vni['close_vni'], window=cci_period).cci()\n",
    "\n",
    "    # Tính toán OBV (On Balance Volume)\n",
    "    data_vni['obv_vni'] = OnBalanceVolumeIndicator(close=data_vni['close_vni'], volume=data_vni['volume_vni']).on_balance_volume()\n",
    "\n",
    "    # Tính toán EMA ngắn hạn (12 ngày)\n",
    "    data_vni['ema_12_vni'] = data_vni['close_vni'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    # Tính toán EMA dài hạn (26 ngày)\n",
    "    data_vni['ema_26_vni'] = data_vni['close_vni'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # Tính toán SMA trung hạn (20 ngày)\n",
    "    data_vni['sma_20_vni'] = data_vni['close_vni'].rolling(window=20).mean()\n",
    "\n",
    "    # Tính toán SMA dài hạn (50 ngày)\n",
    "    data_vni['sma_50_vni'] = data_vni['close_vni'].rolling(window=50).mean()\n",
    "\n",
    "    data_vni['hl_ratio_vni'] = data_vni['high_vni'] / (data_vni['low_vni'] + 1e-8)\n",
    "    data_vni['co_ratio_vni'] = data_vni['close_vni'] / (data_vni['open_vni'] + 1e-8)\n",
    "    data_vni['price_range_vni'] = (data_vni['high_vni'] - data_vni['low_vni']) / (data_vni['close_vni'] + 1e-8)\n",
    "\n",
    "    if 'sma_20_vni' in data_vni.columns and 'sma_50_vni' in data_vni.columns:\n",
    "        data_vni['sma_ratio_20_50_vni'] = data_vni['sma_20_vni'] / (data_vni['sma_50_vni'] + 1e-8)\n",
    "    if 'ema_12_vni' in data_vni.columns and 'ema_26_vni' in data_vni.columns:\n",
    "        data_vni['ema_ratio_12_26_vni'] = data_vni['ema_12_vni'] / (data_vni['ema_26_vni'] + 1e-8)\n",
    "\n",
    "    if all(col in data_vni.columns for col in ['bb_bbh_vni', 'bb_bbl_vni', 'bb_bbm_vni']):\n",
    "        data_vni['bb_width_vni'] = (data_vni['bb_bbh_vni'] - data_vni['bb_bbl_vni']) / (data_vni['bb_bbm_vni'] + 1e-8)\n",
    "        data_vni['bb_position_vni'] = (data_vni['close_vni'] - data_vni['bb_bbl_vni']) / (data_vni['bb_bbh_vni'] - data_vni['bb_bbl_vni'] + 1e-8)\n",
    "\n",
    "    if 'rsi_vni' in data_vni.columns:\n",
    "        data_vni['rsi_overbought_vni'] = (data_vni['rsi_vni'] > 70).astype(int)\n",
    "        data_vni['rsi_oversold_vni'] = (data_vni['rsi_vni'] < 30).astype(int)\n",
    "        data_vni['rsi_neutral_vni'] = ((data_vni['rsi_vni'] >= 30) & (data_vni['rsi_vni'] <= 70)).astype(int)\n",
    "\n",
    "    if all(col in data_vni.columns for col in ['macd_vni', 'signal_line_vni']):\n",
    "        data_vni['macd_signal_diff_vni'] = data_vni['macd_vni'] - data_vni['signal_line_vni']\n",
    "        data_vni['macd_bullish_vni'] = (data_vni['macd_vni'] > data_vni['signal_line_vni']).astype(int)\n",
    "\n",
    "    for period in [5, 10]:\n",
    "        if len(data_vni) > period:\n",
    "            df[f'momentum_{period}_vni'] = data_vni['close_vni'].pct_change(period)\n",
    "\n",
    "    \n",
    "    # 1. Daily log return\n",
    "    data_vni['log_return_vni'] = np.log(data_vni['close_vni'] / data_vni['close_vni'].shift(1))\n",
    "\n",
    "    # 2. Rolling volatility (biến động trượt)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vni[f'volatility_{window}d_vni'] = data_vni['log_return_vni'].rolling(window=window).std()\n",
    "\n",
    "    # 3. Rolling mean log return\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vni[f'mean_log_return_{window}d_vni'] = data_vni['log_return_vni'].rolling(window=window).mean()\n",
    "\n",
    "    # 4. Sharpe-like ratio (mean return / volatility)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vni = f'mean_log_return_{window}d_vni'\n",
    "        vol_col_vni = f'volatility_{window}d_vni'\n",
    "        data_vni[f'sharpe_like_{window}d_vni'] = data_vni[mean_col_vni] / (data_vni[vol_col_vni] + 1e-9)\n",
    "\n",
    "    # 5. Directional Feature - chuỗi ngày tăng liên tiếp\n",
    "    data_vni['up_streak_vni'] = (data_vni['log_return_vni'] > 0).astype(int)\n",
    "    data_vni['up_streak_vni'] = data_vni['up_streak_vni'] * (data_vni['up_streak_vni'].groupby((data_vni['up_streak_vni'] != data_vni['up_streak_vni'].shift()).cumsum()).cumcount() + 1)\n",
    "    data_vni['up_streak_vni'] = data_vni['up_streak_vni'].where(data_vni['log_return_vni'] > 0, 0)\n",
    "\n",
    "    # 6. Tỷ lệ ngày log return dương trong 20 ngày gần nhất\n",
    "    data_vni['pos_log_return_ratio_20d_vni'] = data_vni['log_return_vni'].rolling(window=20).apply(lambda x: np.mean(x > 0), raw=True)\n",
    "\n",
    "    # 7. Z-score của log return hiện tại\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vni = f'mean_log_return_{window}d_vni'\n",
    "        vol_col_vni = f'volatility_{window}d_vni'\n",
    "        data_vni[f'z_score_{window}d_vni'] = (data_vni['log_return_vni'] - data_vni[mean_col_vni]) / (data_vni[vol_col_vni] + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "    # Tính Annual Return (lợi nhuận hàng năm)\n",
    "    data_vni['annual_return_vni'] = data_vni['close_vni'] / data_vni['close_vni'].shift(252) - 1  # 252 ngày giao dịch trong năm\n",
    "\n",
    "    # Tính Volatility (Độ biến động) hàng năm (Standard Deviation of Returns)\n",
    "    data_vni['daily_return_vni'] = data_vni['close_vni'].pct_change()  # Tỷ suất lợi nhuận hàng ngày\n",
    "    data_vni['annual_volatility_vni'] = data_vni['daily_return_vni'].rolling(window=252).std() * (252 ** 0.5)  # Độ lệch chuẩn hàng năm (tính từ tỷ suất lợi nhuận hàng ngày)\n",
    "\n",
    "    risk_free_rate = 0.03  # tương đương 3%/năm\n",
    "    # Tính Sharpe ratio: (Rp - Rf) / Volatility\n",
    "    data_vni['sharpe_ratio_vni'] = (data_vni['annual_return_vni'] - risk_free_rate) / data_vni['annual_volatility_vni']\n",
    "\n",
    "\n",
    "    data_vni['time'] = pd.to_datetime(data_vni['time']).dt.date\n",
    "\n",
    "\n",
    "    # Xứ lý với VN30 =============================================================================\n",
    "    stock = Vnstock().stock(symbol=\"VN30\")\n",
    "    data_vn30 = stock.quote.history(start=start_date, end=end_date)\n",
    "    data_vn30.columns = ['time','open_vn30', 'high_vn30', 'low_vn30', 'close_vn30', 'volume_vn30']\n",
    "\n",
    "\n",
    "    # Tính toán RSI\n",
    "    rsi_period = 14\n",
    "    data_vn30['rsi_vn30'] = RSIIndicator(data_vn30['close_vn30'], window=rsi_period).rsi()\n",
    "    # Tính toán RSI-base-MA\n",
    "    ma_period = 9\n",
    "    data_vn30['rsi_base_ma_vn30'] = data_vn30['rsi_vn30'].rolling(window=ma_period).mean()\n",
    "    data_vn30['rsi_rsi_base_ma_ratio_vn30'] = data_vn30['rsi_vn30'] / data_vn30['rsi_base_ma_vn30']\n",
    "\n",
    "    # Tính toán khối lượng trung bình\n",
    "    volume_ma_period = 20  # Ví dụ: khối lượng trung bình 20 ngày\n",
    "    data_vn30['volume_ma_vn30'] = data_vn30['volume_vn30'].rolling(window=volume_ma_period).mean()\n",
    "    # Tính tỷ lệ khối lượng hiện tại / khối lượng trung bình\n",
    "    data_vn30['volume_to_volume_ma_ratio_vn30'] = data_vn30['volume_vn30'] / data_vn30['volume_ma_vn30']\n",
    "\n",
    "    # Bollinger Bands\n",
    "    indicator_bb = BollingerBands(close=data_vn30['close_vn30'], window=20, window_dev=2)\n",
    "    data_vn30['bb_bbm_vn30'] = indicator_bb.bollinger_mavg()\n",
    "    data_vn30['bb_bbh_vn30'] = indicator_bb.bollinger_hband()\n",
    "    data_vn30['bb_bbl_vn30'] = indicator_bb.bollinger_lband()\n",
    "    data_vn30['bb_bbp_vn30'] = indicator_bb.bollinger_pband()\n",
    "\n",
    "    data_vn30['bb_bbh_bb_bbl_ratio_vn30'] = data_vn30['bb_bbh_vn30'] / data_vn30['bb_bbl_vn30']\n",
    "\n",
    "    # Tính toán ROC\n",
    "    roc_period = 9  # Chu kỳ 9 ngày, bạn có thể thay đổi tùy ý\n",
    "    # data_vn30['roc_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(roc_period)) / data_vn30['close_vn30'].shift(roc_period)) * 100\n",
    "    data_vn30['roc_1_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(1)) / data_vn30['close_vn30'].shift(1)) * 100\n",
    "    data_vn30['roc_5_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(5)) / data_vn30['close_vn30'].shift(5)) * 100\n",
    "    data_vn30['roc_9_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(9)) / data_vn30['close_vn30'].shift(9)) * 100\n",
    "\n",
    "    # Tính toán %K (Stochastic Oscillator)\n",
    "    stoch_period = 14\n",
    "    data_vn30['%K_vn30'] = ((data_vn30['close_vn30'] - data_vn30['low_vn30'].rolling(window=stoch_period).min()) /\n",
    "                (data_vn30['high_vn30'].rolling(window=stoch_period).max() - data_vn30['low_vn30'].rolling(window=stoch_period).min())) * 100\n",
    "\n",
    "    # Tính toán %R (Williams %R)\n",
    "    data_vn30['%R_vn30'] = ((data_vn30['high_vn30'].rolling(window=stoch_period).max() - data_vn30['close_vn30']) /\n",
    "                (data_vn30['high_vn30'].rolling(window=stoch_period).max() - data_vn30['low_vn30'].rolling(window=stoch_period).min())) * -100\n",
    "\n",
    "    # Tính toán CCI (Commodity Channel Index)\n",
    "    cci_period = 20\n",
    "    data_vn30['typical_price_vn30'] = (data_vn30['high_vn30'] + data_vn30['low_vn30'] + data_vn30['close_vn30']) / 3\n",
    "    data_vn30['cci_vn30'] = CCIIndicator(high=data_vn30['high_vn30'], low=data_vn30['low_vn30'], close=data_vn30['close_vn30'], window=cci_period).cci()\n",
    "\n",
    "    # Tính toán OBV (On Balance Volume)\n",
    "    data_vn30['obv_vn30'] = OnBalanceVolumeIndicator(close=data_vn30['close_vn30'], volume=data_vn30['volume_vn30']).on_balance_volume()\n",
    "\n",
    "    # Tính toán EMA ngắn hạn (12 ngày)\n",
    "    data_vn30['ema_12_vn30'] = data_vn30['close_vn30'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    # Tính toán EMA dài hạn (26 ngày)\n",
    "    data_vn30['ema_26_vn30'] = data_vn30['close_vn30'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # Tính toán SMA trung hạn (20 ngày)\n",
    "    data_vn30['sma_20_vn30'] = data_vn30['close_vn30'].rolling(window=20).mean()\n",
    "\n",
    "    # Tính toán SMA dài hạn (50 ngày)\n",
    "    data_vn30['sma_50_vn30'] = data_vn30['close_vn30'].rolling(window=50).mean()\n",
    "\n",
    "    data_vn30['hl_ratio_vn30'] = data_vn30['high_vn30'] / (data_vn30['low_vn30'] + 1e-8)\n",
    "    data_vn30['co_ratio_vn30'] = data_vn30['close_vn30'] / (data_vn30['open_vn30'] + 1e-8)\n",
    "    data_vn30['price_range_vn30'] = (data_vn30['high_vn30'] - data_vn30['low_vn30']) / (data_vn30['close_vn30'] + 1e-8)\n",
    "\n",
    "    if 'sma_20_vn30' in data_vn30.columns and 'sma_50_vn30' in data_vn30.columns:\n",
    "        data_vn30['sma_ratio_20_50_vn30'] = data_vn30['sma_20_vn30'] / (data_vn30['sma_50_vn30'] + 1e-8)\n",
    "    if 'ema_12_vn30' in data_vn30.columns and 'ema_26_vn30' in data_vn30.columns:\n",
    "        data_vn30['ema_ratio_12_26_vn30'] = data_vn30['ema_12_vn30'] / (data_vn30['ema_26_vn30'] + 1e-8)\n",
    "\n",
    "    if all(col in data_vn30.columns for col in ['bb_bbh_vn30', 'bb_bbl_vn30', 'bb_bbm_vn30']):\n",
    "        data_vn30['bb_width_vn30'] = (data_vn30['bb_bbh_vn30'] - data_vn30['bb_bbl_vn30']) / (data_vn30['bb_bbm_vn30'] + 1e-8)\n",
    "        data_vn30['bb_position_vn30'] = (data_vn30['close_vn30'] - data_vn30['bb_bbl_vn30']) / (data_vn30['bb_bbh_vn30'] - data_vn30['bb_bbl_vn30'] + 1e-8)\n",
    "\n",
    "    if 'rsi_vn30' in data_vn30.columns:\n",
    "        data_vn30['rsi_overbought_vn30'] = (data_vn30['rsi_vn30'] > 70).astype(int)\n",
    "        data_vn30['rsi_oversold_vn30'] = (data_vn30['rsi_vn30'] < 30).astype(int)\n",
    "        data_vn30['rsi_neutral_vn30'] = ((data_vn30['rsi_vn30'] >= 30) & (data_vn30['rsi_vn30'] <= 70)).astype(int)\n",
    "\n",
    "    if all(col in data_vn30.columns for col in ['macd_vn30', 'signal_line_vn30']):\n",
    "        data_vn30['macd_signal_diff_vn30'] = data_vn30['macd_vn30'] - data_vn30['signal_line_vn30']\n",
    "        data_vn30['macd_bullish_vn30'] = (data_vn30['macd_vn30'] > data_vn30['signal_line_vn30']).astype(int)\n",
    "\n",
    "    for period in [5, 10]:\n",
    "        if len(data_vn30) > period:\n",
    "            df[f'momentum_{period}_vn30'] = data_vn30['close_vn30'].pct_change(period)\n",
    "\n",
    "\n",
    "    # 1. Daily log return\n",
    "    data_vn30['log_return_vn30'] = np.log(data_vn30['close_vn30'] / data_vn30['close_vn30'].shift(1))\n",
    "\n",
    "    # 2. Rolling volatility (biến động trượt)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vn30[f'volatility_{window}d_vn30'] = data_vn30['log_return_vn30'].rolling(window=window).std()\n",
    "\n",
    "    # 3. Rolling mean log return\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vn30[f'mean_log_return_{window}d_vn30'] = data_vn30['log_return_vn30'].rolling(window=window).mean()\n",
    "\n",
    "    # 4. Sharpe-like ratio (mean return / volatility)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vn30 = f'mean_log_return_{window}d_vn30'\n",
    "        vol_col_vn30 = f'volatility_{window}d_vn30'\n",
    "        data_vn30[f'sharpe_like_{window}d_vn30'] = data_vn30[mean_col_vn30] / (data_vn30[vol_col_vn30] + 1e-9)\n",
    "\n",
    "    # 5. Directional Feature - chuỗi ngày tăng liên tiếp\n",
    "    data_vn30['up_streak_vn30'] = (data_vn30['log_return_vn30'] > 0).astype(int)\n",
    "    data_vn30['up_streak_vn30'] = data_vn30['up_streak_vn30'] * (data_vn30['up_streak_vn30'].groupby((data_vn30['up_streak_vn30'] != data_vn30['up_streak_vn30'].shift()).cumsum()).cumcount() + 1)\n",
    "    data_vn30['up_streak_vn30'] = data_vn30['up_streak_vn30'].where(data_vn30['log_return_vn30'] > 0, 0)\n",
    "\n",
    "    # 6. Tỷ lệ ngày log return dương trong 20 ngày gần nhất\n",
    "    data_vn30['pos_log_return_ratio_20d_vn30'] = data_vn30['log_return_vn30'].rolling(window=20).apply(lambda x: np.mean(x > 0), raw=True)\n",
    "\n",
    "    # 7. Z-score của log return hiện tại\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vn30 = f'mean_log_return_{window}d_vn30'\n",
    "        vol_col_vn30 = f'volatility_{window}d_vn30'\n",
    "        data_vn30[f'z_score_{window}d_vn30'] = (data_vn30['log_return_vn30'] - data_vn30[mean_col_vn30]) / (data_vn30[vol_col_vn30] + 1e-9)\n",
    "\n",
    "\n",
    "    # Tính Annual Return (lợi nhuận hàng năm)\n",
    "    data_vn30['annual_return_vn30'] = data_vn30['close_vn30'] / data_vn30['close_vn30'].shift(252) - 1  # 252 ngày giao dịch trong năm\n",
    "\n",
    "    # Tính Volatility (Độ biến động) hàng năm (Standard Deviation of Returns)\n",
    "    data_vn30['daily_return_vn30'] = data_vn30['close_vn30'].pct_change()  # Tỷ suất lợi nhuận hàng ngày\n",
    "    data_vn30['annual_volatility_vn30'] = data_vn30['daily_return_vn30'].rolling(window=252).std() * (252 ** 0.5)  # Độ lệch chuẩn hàng năm (tính từ tỷ suất lợi nhuận hàng ngày)\n",
    "\n",
    "    risk_free_rate = 0.03  # tương đương 3%/năm\n",
    "    # Tính Sharpe ratio: (Rp - Rf) / Volatility\n",
    "    data_vn30['sharpe_ratio_vn30'] = (data_vn30['annual_return_vn30'] - risk_free_rate) / data_vn30['annual_volatility_vn30']\n",
    "\n",
    "\n",
    "    data_vn30['time'] = pd.to_datetime(data_vn30['time']).dt.date\n",
    "\n",
    "\n",
    "\n",
    "    # merge\n",
    "    df['time'] = pd.to_datetime(df['time']).dt.date\n",
    "    df = pd.merge(df, data_vni, on='time')\n",
    "    df = pd.merge(df, data_vn30, on='time')\n",
    "\n",
    "\n",
    "     # Tính tỷ lệ thay đổi giá\n",
    "    threshold=0.01\n",
    "    df['change'] = (df['close'].shift(-1) - df['close']) / df['close']\n",
    "\n",
    "    # Gán nhãn\n",
    "    df['target'] = 1  # Hold mặc định\n",
    "    df.loc[df['change'] > threshold, 'target'] = 2   # Buy\n",
    "    df.loc[df['change'] < -threshold, 'target'] = 0  # Sell\n",
    "\n",
    "    if drop_na:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "672d3c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang lấy dữ liệu cho: ACB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 11:36:47 - vnstock.common.data.data_explorer - INFO - Không phải là mã chứng khoán, thông tin công ty và tài chính không khả dụng.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã thu thập dữ liệu Phân tích cơ bản và phân tích kỹ thuật cho ACB ngày 2025-09-05 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 11:36:47 - vnstock.common.data.data_explorer - INFO - Không phải là mã chứng khoán, thông tin công ty và tài chính không khả dụng.\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>P/B_Previous_Quarter</th>\n",
       "      <th>P/B_Same_Period_Last_Year</th>\n",
       "      <th>P/B_d_1</th>\n",
       "      <th>P/B_d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>z_score_5d_vn30</th>\n",
       "      <th>z_score_10d_vn30</th>\n",
       "      <th>z_score_20d_vn30</th>\n",
       "      <th>z_score_30d_vn30</th>\n",
       "      <th>annual_return_vn30</th>\n",
       "      <th>daily_return_vn30</th>\n",
       "      <th>annual_volatility_vn30</th>\n",
       "      <th>sharpe_ratio_vn30</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>27.60</td>\n",
       "      <td>27.75</td>\n",
       "      <td>27.40</td>\n",
       "      <td>27.65</td>\n",
       "      <td>11279900</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.549748</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.339106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.455021</td>\n",
       "      <td>0.449498</td>\n",
       "      <td>0.490590</td>\n",
       "      <td>0.517766</td>\n",
       "      <td>0.423146</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.191727</td>\n",
       "      <td>2.050548</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>27.95</td>\n",
       "      <td>28.05</td>\n",
       "      <td>27.75</td>\n",
       "      <td>27.75</td>\n",
       "      <td>8852600</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.549748</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.339106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113257</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>-0.052652</td>\n",
       "      <td>-0.037642</td>\n",
       "      <td>0.425283</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.191740</td>\n",
       "      <td>2.061557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time   open   high    low  close    volume  P/B_Previous_Quarter  \\\n",
       "498  2025-09-04  27.60  27.75  27.40  27.65  11279900              1.628569   \n",
       "499  2025-09-05  27.95  28.05  27.75  27.75   8852600              1.628569   \n",
       "\n",
       "     P/B_Same_Period_Last_Year   P/B_d_1   P/B_d_2  ...  z_score_5d_vn30  \\\n",
       "498                   1.549748  1.628569  1.339106  ...         1.455021   \n",
       "499                   1.549748  1.628569  1.339106  ...        -0.113257   \n",
       "\n",
       "     z_score_10d_vn30  z_score_20d_vn30  z_score_30d_vn30  annual_return_vn30  \\\n",
       "498          0.449498          0.490590          0.517766            0.423146   \n",
       "499          0.025803         -0.052652         -0.037642            0.425283   \n",
       "\n",
       "     daily_return_vn30  annual_volatility_vn30  sharpe_ratio_vn30    change  \\\n",
       "498           0.012906                0.191727           2.050548  0.003617   \n",
       "499           0.003870                0.191740           2.061557       NaN   \n",
       "\n",
       "     target  \n",
       "498       1  \n",
       "499       1  \n",
       "\n",
       "[2 rows x 308 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta_fa_result = load_and_process_data('ACB', start_date='2023-09-05', end_date='2025-09-05', drop_na=False)\n",
    "df_ta_fa = df_ta_fa_result.tail(2)\n",
    "df_ta_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0b2a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tính trend cho các feature cơ bản trong 8 quý gần nhất\n",
    "# thêm vào df mới các cột trend tương ứng với các feature cơ bản\n",
    "def add_slope_features(df, feature_base_names, n=8):\n",
    "    \"\"\"\n",
    "    Với mỗi dòng trong df, tính hệ số gốc (slope) của đường hồi quy tuyến tính theo thời gian\n",
    "    cho từng feature trong danh sách feature_base_names dựa trên các giá trị 8 quý gần nhất.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame chứa các cột dạng 'feature_d_1' đến 'feature_d_n'.\n",
    "        feature_base_names (list): Danh sách tên feature cơ bản (string).\n",
    "        n (int): Số quý gần nhất để tính trend (mặc định 8).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame đã thêm các cột hệ số gốc tương ứng.\n",
    "    \"\"\"\n",
    "    for feature in feature_base_names:\n",
    "        col_name = f\"coefficient_{feature}\"\n",
    "        if col_name not in df.columns:\n",
    "            df[col_name] = np.nan\n",
    "\n",
    "    # Duyệt từng hàng\n",
    "    for index, row in df.iterrows():\n",
    "        for feature in feature_base_names:\n",
    "            y_values = []\n",
    "            for i in range(n, 0, -1):  # d_8, ..., d_1\n",
    "                val = row.get(f\"{feature}_d_{i}\", np.nan)\n",
    "                y_values.append(val)\n",
    "\n",
    "            y_values = np.array(y_values, dtype=float)\n",
    "\n",
    "            # Cần ít nhất 2 điểm hợp lệ để fit mô hình\n",
    "            if np.isnan(y_values).sum() > (n - 2):\n",
    "                coef = np.nan\n",
    "            else:\n",
    "                X = np.arange(n).reshape(-1, 1)\n",
    "                mask = ~np.isnan(y_values)\n",
    "                model = LinearRegression()\n",
    "                model.fit(X[mask], y_values[mask])\n",
    "                coef = model.coef_[0]\n",
    "\n",
    "            df.at[index, f\"coefficient_{feature}\"] = coef\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bac44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_load_and_process_data(symbol, start_date, end_date, drop_na=True):\n",
    "  df = load_and_process_data(symbol, start_date, end_date, drop_na = False)\n",
    "  df = df.tail(2)\n",
    "  feature_base_names = ['P/B','P/E','P/S','P/Cash Flow','EPS (VND)','BVPS (VND)','ROE (%)','ROA (%)']\n",
    "  df = add_slope_features(df, feature_base_names)\n",
    "\n",
    "  df.rename(columns={\n",
    "            'P/B_Previous_Quarter': 'p/b_previous_quarter',\n",
    "            'P/B_change_rate': 'p/b_change_rate',\n",
    "            'P/B_change_rate_flag': 'p/b_change_rate_flag',\n",
    "            'P/E_Previous_Quarter': 'p/e_previous_quarter',\n",
    "            'P/E_change_rate': 'p/e_change_rate',\n",
    "            'P/E_change_rate_flag':'p/e_change_rate_flag',\n",
    "            'P/S_Previous_Quarter': 'p/s_previous_quarter',\n",
    "            'P/S_change_rate': 'p/s_change_rate',\n",
    "            'P/S_change_rate_flag': 'p/s_change_rate_flag',\n",
    "            'P/Cash Flow_Previous_Quarter': 'p/cash_flow_previous_quarter',\n",
    "            'P/Cash Flow_change_rate': 'p/cash_flow_change_rate',\n",
    "            'P/Cash Flow_change_rate_flag' : 'p/cash_flow_change_rate_flag',\n",
    "            'EPS (VND)_Previous_Quarter': 'eps_previous_quarter',\n",
    "            'EPS (VND)_change_rate': 'eps_change_rate',\n",
    "            'EPS (VND)_change_rate_flag': 'eps_change_rate_flag',\n",
    "            'BVPS (VND)_Previous_Quarter': 'bvps_previous_quarter',\n",
    "            'BVPS (VND)_change_rate' : 'bvps_change_rate',\n",
    "            'BVPS (VND)_change_rate_flag': 'bvps_change_rate_flag',\n",
    "            'ROE (%)_Previous_Quarter': 'roe_previous_quarter',\n",
    "            'ROE (%)_change_rate': 'roe_change_rate',\n",
    "            'ROE (%)_change_rate_flag': 'roe_change_rate_flag',\n",
    "            'ROA (%)_Previous_Quarter': 'roa_previous_quarter',\n",
    "            'ROA (%)_change_rate': 'roa_change_rate',\n",
    "            'ROA (%)_change_rate_flag': 'roa_change_rate_flag',\n",
    "            'coefficient_P/B': 'coefficient_p/b',\n",
    "            'coefficient_P/E': 'coefficient_p/e',\n",
    "            'coefficient_P/S': 'coefficient_p/s',\n",
    "            'coefficient_P/Cash Flow': 'coefficient_p/cash_flow',\n",
    "            'coefficient_EPS (VND)': 'coefficient_eps',\n",
    "            'coefficient_BVPS (VND)': 'coefficient_bvps',\n",
    "            'coefficient_ROE (%)': 'coefficient_roe',\n",
    "            'coefficient_ROA (%)': 'coefficient_roa'\n",
    "\n",
    "\n",
    "        }, inplace=True)  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3819aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang lấy dữ liệu cho: VHM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 14:57:58 - vnstock.common.data.data_explorer - INFO - Không phải là mã chứng khoán, thông tin công ty và tài chính không khả dụng.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã thu thập dữ liệu Phân tích cơ bản và phân tích kỹ thuật cho VHM ngày 2025-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 14:57:58 - vnstock.common.data.data_explorer - INFO - Không phải là mã chứng khoán, thông tin công ty và tài chính không khả dụng.\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>p/b_previous_quarter</th>\n",
       "      <th>P/B_Same_Period_Last_Year</th>\n",
       "      <th>P/B_d_1</th>\n",
       "      <th>P/B_d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "      <th>coefficient_p/b</th>\n",
       "      <th>coefficient_p/e</th>\n",
       "      <th>coefficient_p/s</th>\n",
       "      <th>coefficient_p/cash_flow</th>\n",
       "      <th>coefficient_eps</th>\n",
       "      <th>coefficient_bvps</th>\n",
       "      <th>coefficient_roe</th>\n",
       "      <th>coefficient_roa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.5</td>\n",
       "      <td>5198000</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016393</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127646</td>\n",
       "      <td>1.19543</td>\n",
       "      <td>0.320346</td>\n",
       "      <td>-9.421164</td>\n",
       "      <td>66.826894</td>\n",
       "      <td>1771.340893</td>\n",
       "      <td>-0.00999</td>\n",
       "      <td>-0.005391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>89.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>88.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7811500</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127646</td>\n",
       "      <td>1.19543</td>\n",
       "      <td>0.320346</td>\n",
       "      <td>-9.421164</td>\n",
       "      <td>66.826894</td>\n",
       "      <td>1771.340893</td>\n",
       "      <td>-0.00999</td>\n",
       "      <td>-0.005391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time  open  high   low  close   volume  p/b_previous_quarter  \\\n",
       "498  2025-07-30  92.1  92.5  89.0   91.5  5198000              1.971749   \n",
       "499  2025-07-31  89.7  91.5  88.2   90.0  7811500              1.971749   \n",
       "\n",
       "     P/B_Same_Period_Last_Year   P/B_d_1   P/B_d_2  ...    change  target  \\\n",
       "498                   1.027068  1.971749  1.776398  ... -0.016393       0   \n",
       "499                   1.027068  1.971749  1.776398  ...       NaN       1   \n",
       "\n",
       "     coefficient_p/b  coefficient_p/e  coefficient_p/s  \\\n",
       "498         0.127646          1.19543         0.320346   \n",
       "499         0.127646          1.19543         0.320346   \n",
       "\n",
       "     coefficient_p/cash_flow  coefficient_eps  coefficient_bvps  \\\n",
       "498                -9.421164        66.826894       1771.340893   \n",
       "499                -9.421164        66.826894       1771.340893   \n",
       "\n",
       "     coefficient_roe  coefficient_roa  \n",
       "498         -0.00999        -0.005391  \n",
       "499         -0.00999        -0.005391  \n",
       "\n",
       "[2 rows x 316 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta_fa = fn_load_and_process_data('VHM', start_date='2023-07-31', end_date='2025-07-31')\n",
    "df_ta_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu thập tin tức"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a3140277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngày dự đoán là 31/07/2025, ngày giao dịch trước đó là 30/07/2025 - > cần lấy các tin tức trong khoản 14:45 ngày 30/07/2025 đến 14:45 ngày 01/08/2025)\n",
    "start_date = '30/07/2025 14:45'\n",
    "end_date = '01/08/2025 14:45'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a07166be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from openpyxl.utils import escape\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64eaff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.binary_location = \"/usr/bin/google-chrome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e0a7f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid escape sequence '\\c'\n",
      "invalid escape sequence '\\c'\n",
      "invalid escape sequence '\\c'\n"
     ]
    }
   ],
   "source": [
    "chrome_driver_path = \"D:\\chromedriver-win64-140\\chromedriver-win64\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ae824c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True  # không hiển thị trình duyệt\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--blink-settings=imagesEnabled=false\")  # tắt hình ảnh\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/115.0 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e10f6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo driver\n",
    "driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0e58dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test thử mở trang Cafef\n",
    "driver.get(\"https://cafef.vn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb74dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu thập title và url của các bài báo từ cafe f trong khoản thời gian nhất định\n",
    "def craw_title_url_stock_news_cafef(ticker, start_date, end_date):\n",
    "  # Khởi tạo driver\n",
    "  driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "  df = pd.DataFrame(data = None, columns = ['ticker','time', 'tittle', 'url'])\n",
    "  driver.get(\"https://cafef.vn/du-lieu/tin-doanh-nghiep/\"+str(ticker)+\"/event.chn\")\n",
    "  print(driver.title)\n",
    "  flag = True\n",
    "  while (flag):\n",
    "    li_elements = driver.find_elements(By.XPATH, \"//div[@id='divEvents']/ul/li\")\n",
    "    for idx, li in enumerate(li_elements, start= 1):\n",
    "      span = li.find_element(By.TAG_NAME, \"span\")\n",
    "      curent_date_of_new = span.text\n",
    "      curent_date_of_new_convert = datetime.strptime(curent_date_of_new, \"%d/%m/%Y %H:%M\")\n",
    "      if (curent_date_of_new_convert < start_date):\n",
    "        print(\"Đã tìm hết các bài báo trong khoản thời gian cần tìm\")\n",
    "        flag = False\n",
    "        break\n",
    "      if curent_date_of_new_convert >= start_date and curent_date_of_new_convert <= end_date:\n",
    "        time = curent_date_of_new\n",
    "        url = li.find_element(By.TAG_NAME, \"a\").get_attribute('href')\n",
    "        title = li.find_element(By.TAG_NAME, \"a\").text\n",
    "        print('Đang lấy bài báo với thông tin như sau: ')\n",
    "        print(time)\n",
    "        print(title)\n",
    "        print(url)\n",
    "        new_row = {'ticker': ticker, 'time':time, 'tittle': title, 'url':url}\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "      if (idx == len(li_elements)):\n",
    "        # print(\"Đây là phần tử cuối cùng\")\n",
    "        # Click vào nút xem tiếp\n",
    "        try:\n",
    "          btn_next_element = driver.find_element(By.XPATH, \"//span[@id='spanNext']/a\")\n",
    "          driver.execute_script(\"arguments[0].click();\", btn_next_element)\n",
    "          sleep(random.uniform(1, 2))\n",
    "          # print(\"Cuyển qua trang tiếp theo\")\n",
    "        except Exception as e:\n",
    "          print(f\"Không thể click nút tiếp: {e}\")\n",
    "          flag = False\n",
    "          break\n",
    "  # 5. Đóng browser\n",
    "  driver.close()\n",
    "  print(f\"Đã thu thập {len(df)} tin tức từ cafe f cho cổ phiếu {ticker} trong khoản thời gian {start_date} đến {end_date}\")\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d5ff97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đã tìm hết các bài báo trong khoản thời gian cần tìm\n",
      "Đã thu thập 0 tin tức từ cafe f cho cổ phiếu ACB trong khoản thời gian 2025-09-04 14:45:00 đến 2025-09-06 14:45:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, time, tittle, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ví dụ thu thập title và url của các bài báo từ cafe f trong khoản thời gian nhất định\n",
    "start_date_cafef = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "end_date_cafef = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "cafef_news_title_url = craw_title_url_stock_news_cafef(\"ACB\", start_date_cafef, end_date_cafef)\n",
    "cafef_news_title_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6a845f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu thập nội dung của các bài báo từ title và url trên cafe f.\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return ILLEGAL_CHARACTERS_RE.sub(\"\", text)\n",
    "    return text\n",
    "def get_content_news_cafef(cafef_news_title_url):\n",
    "  # Khởi tạo driver\n",
    "  driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "  df = cafef_news_title_url\n",
    "  df['Content'] = \"\"\n",
    "  for i in range(len(df)):\n",
    "    text = \"\"\n",
    "    url = df.url[i]\n",
    "    try:\n",
    "        #Mở và thu thập dữ liệu nội dung bài báo thông qua URL của bài báo\n",
    "        driver.get(url)\n",
    "        print(\"Đang xử lý URL: \" + str(url))\n",
    "        print(\"Đang xử lý bài báo thứ: \" + str(i))\n",
    "        try:\n",
    "            title_news = driver.find_element(\"xpath\",\"//div[@class='left_cate totalcontentdetail']/h1[@class='title']\")\n",
    "        except:\n",
    "            title_news = None\n",
    "            # browser.quit()\n",
    "        if title_news is not None:\n",
    "          try:\n",
    "            # description = browser.find_element(\"xpath\", \"/html/body/form/div[2]/div[4]/div[5]/div/div[1]/div[\" + str(j) + \"]/h2\")\n",
    "            description = driver.find_element(\"xpath\",\"//div[@class='w640 fr clear']/h2[@class='sapo'][1]\")\n",
    "\n",
    "          except:\n",
    "              continue\n",
    "\n",
    "          text = text + description.text + \" \"\n",
    "\n",
    "          for k in range(1,30):\n",
    "            try:\n",
    "              article = driver.find_element(\"xpath\", \"//div[@class='detail-content afcbc-body']/p[\"+str(k)+\"]\")\n",
    "              text = text + article.text\n",
    "            except:\n",
    "              break\n",
    "          cleaned_text = clean_text(text)\n",
    "          df.loc[i, 'Content'] = df.loc[i, 'Content'] + cleaned_text\n",
    "    except Exception as e:\n",
    "      print(f\"Lỗi khi xử lý URL {url}: {e}\")\n",
    "      continue\n",
    "  print(f\"Đã lấy nội dung {len(df)} bài báo từ cafe f\")\n",
    "  driver.close()\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6c65066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lấy nội dung 0 bài báo từ cafe f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, time, tittle, url, Content]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ví dụ thu thập nội dung của các bài báo từ title và url trên cafe f.\n",
    "df_content_news_cafef = get_content_news_cafef(cafef_news_title_url)\n",
    "df_content_news_cafef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "56366c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get official statement pdf link and download pdf file\n",
    "def get_official_statement(df_content_news_cafef, destination_file, destination_official_statement_folder):\n",
    "    def init_driver():\n",
    "        # options = webdriver.ChromeOptions()\n",
    "        # options.add_argument('--headless')  # Tùy chọn nếu bạn không cần giao diện\n",
    "        # return webdriver.Chrome(options=options)\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = True  # không hiển thị trình duyệt\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--blink-settings=imagesEnabled=false\")  # tắt hình ảnh\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/115.0 Safari/537.36\")\n",
    "        return webdriver.Chrome(options=options)\n",
    "\n",
    "    driver = init_driver()  # Khởi tạo driver ban đầu\n",
    "\n",
    "    # df = pd.read_excel(original_file)\n",
    "    df = df_content_news_cafef\n",
    "    df_news_official_statement = df[df['Content'].fillna('').str.strip() == ''].copy()\n",
    "    df_news_official_statement['official_statement_pdf_name'] = \"\"\n",
    "    print(f\"Đã đọc file excel chứa danh sách các file pdf nghị quyết, nghị định cần lấy gồm {df_news_official_statement.shape[0]} file pdf.\")\n",
    "\n",
    "    os.makedirs(destination_official_statement_folder, exist_ok=True)\n",
    "    max_retries = 5\n",
    "\n",
    "    for i, row in df_news_official_statement.iterrows():\n",
    "        url = row['url']\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(random.uniform(1, 5))\n",
    "                print(f\"\\n📰 Đang xử lý bài báo thứ {i} (Lần thử {attempt+1}):\\n{url}\")\n",
    "                driver.get(url)\n",
    "                \n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                pdf_element = wait.until(EC.presence_of_element_located((\n",
    "                    By.XPATH,\n",
    "                    \"//div[@id='newscontent']//a[contains(@href, '.pdf')]\"\n",
    "                )))\n",
    "                pdf_url = pdf_element.get_attribute('href')\n",
    "                full_pdf_url = urljoin(url, pdf_url)\n",
    "\n",
    "                df_news_official_statement.at[i, 'Content'] = full_pdf_url\n",
    "                print(f\"📎 Tìm thấy link PDF: {full_pdf_url}\")\n",
    "\n",
    "                pdf_filename = os.path.basename(full_pdf_url)\n",
    "                pdf_path = os.path.join(destination_official_statement_folder, pdf_filename)\n",
    "\n",
    "                response = requests.get(full_pdf_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    with open(pdf_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"📁 Đã tải file PDF: {pdf_filename}\")\n",
    "                    df_news_official_statement.at[i, 'official_statement_pdf_name'] = pdf_filename\n",
    "                else:\n",
    "                    print(f\"⚠️ Không thể tải PDF: {full_pdf_url} (mã trạng thái {response.status_code})\")\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                err_msg = str(e)\n",
    "                print(f\"❌ Lỗi lần {attempt+1} khi xử lý URL {url}: {err_msg}\")\n",
    "\n",
    "                # Nếu lỗi liên quan đến mất kết nối WebDriver\n",
    "                if (\n",
    "                    \"Max retries exceeded\" in err_msg or \n",
    "                    \"Failed to establish a new connection\" in err_msg or \n",
    "                    isinstance(e, WebDriverException)\n",
    "                ):\n",
    "                    print(\"🔌 Phát hiện lỗi kết nối với driver. Đang khởi động lại...\")\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass  # Đảm bảo không crash nếu driver đã bị đóng\n",
    "\n",
    "                    time.sleep(2)\n",
    "                    driver = init_driver()\n",
    "                    continue\n",
    "\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(\"🔁 Đang thử lại sau 2 giây...\")\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    print(\"🚫 Bỏ qua URL này sau nhiều lần thử.\")\n",
    "                    break\n",
    "\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with pd.ExcelWriter(destination_file, engine='openpyxl', mode='w') as writer:\n",
    "        df_news_official_statement.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    print(f\"\\n✅ Đã lưu file Excel chứa {len(df_news_official_statement)} nghị quyết, nghị định của công ty vào : {destination_file}\")\n",
    "    return df_news_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f8a3c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc file excel chứa danh sách các file pdf nghị quyết, nghị định cần lấy gồm 0 file pdf.\n",
      "\n",
      "✅ Đã lưu file Excel chứa 0 nghị quyết, nghị định của công ty vào : D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "      <th>official_statement_pdf_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, time, tittle, url, Content, official_statement_pdf_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ví dụ lấy file pdf official statement\n",
    "destination_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx'\n",
    "destination_official_statement_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\official_statement_pdf'\n",
    "df_news_official_statement = get_official_statement(df_content_news_cafef, destination_file, destination_official_statement_file)\n",
    "df_news_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================crawl news tuoitre - vietnamnet - vnexpress=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e6633028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl news vnexpress\n",
    "def convert_time_format(raw_time):\n",
    "    try:\n",
    "        # Bỏ phần ngày trong tuần (VD: \"Thứ ba,\") và phần (GMT+7)\n",
    "        clean_time = re.sub(r'^.*?,\\s*', '', raw_time)\n",
    "        clean_time = re.sub(r'\\s*\\(.*?\\)', '', clean_time)      # Bỏ \"(GMT+7)\"\n",
    "        clean_time = clean_time.strip()\n",
    "        print(clean_time)\n",
    "\n",
    "        # Chuyển đổi thành datetime\n",
    "        dt = datetime.strptime(clean_time, \"%d/%m/%Y, %H:%M\")\n",
    "\n",
    "        return dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi chuyển đổi thời gian: {e}\")\n",
    "        return raw_time\n",
    "def crawl_news_vnexpress(ticker='VIC', start_date='01/01/2024 00:00', end_date='01/05/2025 23:59', max_pages=5, delay=2):\n",
    "    \"\"\"\n",
    "    Crawl tin tức từ VnExpress theo từ khóa (ticker) sử dụng Selenium và trả về DataFrame.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Từ khóa tìm kiếm, ví dụ 'VIC'.\n",
    "        max_pages (int): Số trang kết quả tìm kiếm muốn thu thập.\n",
    "        delay (int): Thời gian chờ giữa các trang (giây).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame chứa các cột: title, ticker, time, url, Content.\n",
    "    \"\"\"\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    base_url = f\"https://timkiem.vnexpress.net/?search_q={ticker}&cate_code=kinhdoanh&media_type=all&latest=on&fromdate=&todate=&date_format=all\"\n",
    "    start_dt = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        driver.get(f\"{base_url}&page={page}\")\n",
    "        print(f\"Đã vào xử lý trang {page}...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"article.item-news[data-url]\")\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"Không có bài viết nào trên trang {page}.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                print(article.text.strip())\n",
    "                title_tag = article.find_elements(By.CSS_SELECTOR, \"h3.title-news a\")[0]\n",
    "                title = title_tag.text.strip()\n",
    "                url = title_tag.get_attribute(\"href\")\n",
    "                print(f\"Đang xử lý bài viết: {title}\")\n",
    "                print(f\"URL: {url}\")\n",
    "\n",
    "\n",
    "\n",
    "                # Truy cập từng bài viết để lấy nội dung\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(url)\n",
    "                print(f\"Truy cập bài viết: {url}\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "                # Lấy thời gian đăng\n",
    "                try:\n",
    "                    time_tag = driver.find_element(By.CSS_SELECTOR, \"span.date\")\n",
    "                    time_text = time_tag.text.strip()\n",
    "                    time_text = convert_time_format(time_text)\n",
    "                except Exception as e:\n",
    "                    time_text = \"\"\n",
    "                    print(f\"Lỗi lấy thời gian: {e}\")\n",
    "                print(f\"Thời gian đăng: {time_text}\")\n",
    "\n",
    "                # Kiểm tra điều kiện thời gian\n",
    "                pub_time_date = datetime.strptime(time_text, \"%d/%m/%Y %H:%M\")\n",
    "                if pub_time_date > end_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"Bài viết quá mới không nằm trong khoản thời gian cần crawl: {time_text}\")\n",
    "                    continue\n",
    "                if pub_time_date < start_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"Bài viết quá cũ không nằm trong khoản thời gian cần crawl: {time_text}\")\n",
    "                    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "                # Lấy nội dung bài viết (cả mô tả nếu có)\n",
    "                try:\n",
    "                    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"article.fck_detail p\")\n",
    "                    description = driver.find_elements(By.CSS_SELECTOR, \"p.description\")\n",
    "                    content_parts = []\n",
    "\n",
    "                    if description:\n",
    "                        content_parts.extend([desc.text.strip() for desc in description if desc.text.strip()])\n",
    "                    content_parts.extend([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "\n",
    "                    content = \"\\n\".join(content_parts)\n",
    "\n",
    "                except:\n",
    "                    content = \"\"\n",
    "\n",
    "                print(f\"Nội dung bài viết: {content}\")\n",
    "\n",
    "\n",
    "                # Đóng tab bài viết và quay lại\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(\"Đã đóng tab bài viết\")\n",
    "\n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'time': time_text,\n",
    "                    'tittle': title,\n",
    "                    'url': url,\n",
    "                    'Content': clean_text(content)\n",
    "                })\n",
    "                print(\"Đã thêm vào kết quả\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý bài viết: {e}\")\n",
    "                continue\n",
    "\n",
    "    # driver.quit()\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"Đã thu thập tổng cộng {len(df)} bài viết từ VnExpress.\")\n",
    "    driver.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fbc38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl news vietnamnet\n",
    "def convert_time_format_vietnamnet(raw_time):\n",
    "    \"\"\"\n",
    "    Chuyển định dạng 'Thứ Ba, 13/05/2025 - 05:35' => '13/05/2025 05:35'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Loại bỏ \"Thứ ...,\" và tách theo dấu \"-\"\n",
    "        clean_time = re.sub(r'^.*?,\\s*', '', raw_time)  # \"Thứ Ba, \" -> \"\"\n",
    "        date_part, time_part = clean_time.split(\" - \")\n",
    "        dt = datetime.strptime(f\"{date_part.strip()} {time_part.strip()}\", \"%d/%m/%Y %H:%M\")\n",
    "        return dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi chuyển đổi thời gian: {e}\")\n",
    "        return raw_time\n",
    "def crawl_news_vietnamnet(ticker='VIC', start_date='01/01/2024 00:00', end_date='01/05/2025 23:59', max_pages=3, delay=2):\n",
    "    start_dt = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "    results = []\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    for page in range(0, max_pages):\n",
    "        print(f\"Đang xử lý trang {page+1}...\")\n",
    "        search_url = f\"https://vietnamnet.vn/tim-kiem-p{page}?bydaterang=all&cate=000003&newstype=all&od=2&q={ticker}\"\n",
    "        driver.get(search_url)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"div.horizontalPost\")\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"Không có bài viết nào trên trang {page}.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                a_tag = article.find_elements(By.CSS_SELECTOR, \"div.horizontalPost__avt a\")[0]\n",
    "                title = a_tag.get_attribute(\"title\")\n",
    "                url = a_tag.get_attribute(\"href\")\n",
    "                print(f\"Đang xử lý: {title}\")\n",
    "                print(f\"URL: {url}\")\n",
    "\n",
    "                # Mở bài viết để lấy nội dung và thời gian\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(url)\n",
    "                time.sleep(delay)\n",
    "\n",
    "                # Thời gian\n",
    "                try:\n",
    "                    time_element = driver.find_element(By.CSS_SELECTOR, \"div.bread-crumb-detail__time\")\n",
    "                    time_text = convert_time_format_vietnamnet(time_element.text.strip())\n",
    "                except:\n",
    "                    time_text = \"\"\n",
    "                print(f\"Thời gian đăng: {time_text}\")\n",
    "\n",
    "                # Kiểm tra điều kiện thời gian\n",
    "                pub_time_date = datetime.strptime(time_text, \"%d/%m/%Y %H:%M\")\n",
    "                if pub_time_date > end_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"Bài viết quá mới không nằm trong khoản thời gian cần crawl: {time_text}\")\n",
    "                    continue\n",
    "                if pub_time_date < start_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"Bài viết quá cũ không nằm trong khoản thời gian cần crawl: {time_text}\")\n",
    "                    return pd.DataFrame(results)\n",
    "\n",
    "                # Nội dung\n",
    "                try:\n",
    "                    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"div.maincontent p\")\n",
    "                    description = driver.find_elements(By.CSS_SELECTOR, \"h2.content-detail-sapo\")\n",
    "                    content_parts = []\n",
    "\n",
    "                    if description:\n",
    "                        content_parts.extend([desc.text.strip() for desc in description if desc.text.strip()])\n",
    "                    content_parts.extend([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "\n",
    "                    content = \"\\n\".join(content_parts)\n",
    "                except:\n",
    "                    content = \"\"\n",
    "                print(f\"Nội dung bài viết: {content}\")\n",
    "                # Đóng tab\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(\"Đã đóng tab bài viết\")\n",
    "\n",
    "\n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'time': time_text,\n",
    "                    'tittle': title,\n",
    "                    'url': url,\n",
    "                    'Content': clean_text(content)\n",
    "                })\n",
    "                print(\"Đã thêm vào kết quả\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi xử lý bài viết: {e}\")\n",
    "                continue\n",
    "\n",
    "    # driver.quit()\n",
    "    driver.close()\n",
    "    print(f\"Đã thu thập tổng cộng {len(results)} bài viết từ Vietnamnet.\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "056d3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_format_tuoitre(raw_time):\n",
    "    \"\"\"\n",
    "    Chuyển định dạng '30/12/2024 15:30 GMT+7' => '30/12/2024 15:30'\n",
    "    Trả về cả chuỗi thời gian (string) và đối tượng datetime để so sánh\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Loại bỏ phần \"GMT+7\"\n",
    "        clean_time = raw_time.replace(\"GMT+7\", \"\").strip()\n",
    "        dt = datetime.strptime(clean_time, \"%d/%m/%Y %H:%M\")\n",
    "        return dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi chuyển đổi thời gian Tuổi Trẻ: {e}\")\n",
    "        return raw_time\n",
    "def crawl_news_tuoitre(ticker='VIC', start_date='01/01/2024 00:00', end_date='01/05/2025 23:59', delay=2):\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    start_dt = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "\n",
    "    driver.get(f\"https://tuoitre.vn/tim-kiem.htm?keywords={ticker}&zoneId=11\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Giai đoạn 1: Load toàn bộ bài viết\n",
    "    while True:\n",
    "        print(\"Bắt đầu load bài viết ....\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "        try:\n",
    "            button = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.box-viewmore a\"))\n",
    "            )\n",
    "            button.click()\n",
    "            time.sleep(delay)\n",
    "            print(\"Đang load bài báo ....\")\n",
    "        except:\n",
    "            break  # không còn nút xem thêm\n",
    "            print(\"Đã load xong bài báo\")\n",
    "\n",
    "    # Giai đoạn 2: Duyệt và xử lý bài viết\n",
    "    results = []\n",
    "    articles = driver.find_elements(By.CSS_SELECTOR, \"div.box-category-item\")\n",
    "    print(f\"Tổng số bài viết load được: {len(articles)}\")\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            a_tag = article.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title = a_tag.get_attribute(\"title\")\n",
    "            url = a_tag.get_attribute(\"href\")\n",
    "            print(f\"Đang xử lý bài viết: {title}\")\n",
    "            print(f\"URL: {url}\")\n",
    "\n",
    "            # Mở tab mới để lấy thời gian và nội dung bài viết\n",
    "            driver.execute_script(\"window.open('');\")\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            driver.get(url)\n",
    "            time.sleep(delay)\n",
    "            print(f\"Truy cập bài viết: {url}\")\n",
    "\n",
    "            # Lấy thời gian chính xác\n",
    "            try:\n",
    "                time_element = driver.find_element(By.CSS_SELECTOR, 'div[data-role=\"publishdate\"]')\n",
    "                pub_time = convert_time_format_tuoitre(time_element.text.strip())\n",
    "                print(f\"Thời gian đăng: {pub_time}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi lấy thời gian: {e}\")\n",
    "                pub_time = None\n",
    "\n",
    "            if not pub_time:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                continue\n",
    "\n",
    "            # Kiểm tra điều kiện thời gian\n",
    "            pub_time_date = datetime.strptime(pub_time, \"%d/%m/%Y %H:%M\")\n",
    "            if pub_time_date > end_dt:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(f\"Bài viết quá mới không nằm trong khoản thời gian cần crawl: {pub_time}\")\n",
    "                continue\n",
    "            if pub_time_date < start_dt:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(f\"Bài viết quá cũ không nằm trong khoản thời gian cần crawl: {pub_time}\")\n",
    "                break  # các bài sau chắc chắn còn cũ hơn\n",
    "\n",
    "            # Lấy nội dung bài viết\n",
    "            try:\n",
    "                content_parts = []\n",
    "                desc = driver.find_elements(By.CSS_SELECTOR, \"h2.detail-sapo\")\n",
    "                paras = driver.find_elements(By.CSS_SELECTOR, \"div.detail-content > p\")\n",
    "                content_parts.extend([d.text.strip() for d in desc if d.text.strip()])\n",
    "                content_parts.extend([p.text.strip() for p in paras if p.text.strip()])\n",
    "                content = \"\\n\".join(content_parts)\n",
    "            except:\n",
    "                content = \"\"\n",
    "            print(f\"Nội dung bài viết: {content}\")\n",
    "\n",
    "            # Đóng tab\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "\n",
    "            results.append({\n",
    "                'ticker': ticker,\n",
    "                'time': pub_time,\n",
    "                'tittle': title,\n",
    "                'url': url,\n",
    "                'Content': clean_text(content)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi xử lý bài viết: {e}\")\n",
    "            if len(driver.window_handles) > 1:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "            continue\n",
    "\n",
    "    # driver.quit()\n",
    "    print(f\"Tổng số bài viết được lấy: {len(results)}\")\n",
    "    driver.close()\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e7cf31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news_and_official_statement_cafef_vnexpress_vietnamnet_tuoitre_for_stock(ticker, start_date, end_date, delay=2):\n",
    "    # Lấy url và title của news và official stament từ cafe f\n",
    "    start_date_cafef = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_date_cafef = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "    cafef_news_title_url = craw_title_url_stock_news_cafef(ticker, start_date_cafef, end_date_cafef)\n",
    "\n",
    "    # thu thập nội dung của các bài báo từ title và url trên cafe f.\n",
    "    df_content_news_cafef = get_content_news_cafef(cafef_news_title_url)\n",
    "\n",
    "    # Lấy official stament từ cafe f\n",
    "    destination_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx'\n",
    "    destination_official_statement_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\official_statement_pdf'\n",
    "    df_news_official_statement = get_official_statement(cafef_news_title_url, destination_file, destination_official_statement_file)\n",
    "\n",
    "    # Lấy các df của các bài báo từ cafe f từ \n",
    "    df_news_cafef = df_content_news_cafef[df_content_news_cafef['Content'].notna() & (df_content_news_cafef['Content'].str.strip() != '')].copy()\n",
    "    \n",
    "    # Lấy các bài báo từ vnexpress\n",
    "    df_vn = crawl_news_vnexpress(ticker, start_date, end_date, max_pages= 38, delay=delay)\n",
    "    # Lấy các bài báo từ vietnamnet\n",
    "    df_vietnamnet = crawl_news_vietnamnet(ticker, start_date, end_date, max_pages= 80, delay=delay)\n",
    "    # Lấy các bài báo từ tuoitre\n",
    "    df_tuoitre = crawl_news_tuoitre(ticker, start_date, end_date, delay=delay)\n",
    "    df = pd.concat([df_news_cafef, df_vn, df_vietnamnet, df_tuoitre], ignore_index=True)\n",
    "    return df, df_news_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b563899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đang lấy bài báo với thông tin như sau: \n",
      "31/07/2025 17:18\n",
      "VHM: Báo cáo tình hình quản trị 6 tháng đầu năm 2025\n",
      "https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025.chn\n",
      "Đang lấy bài báo với thông tin như sau: \n",
      "30/07/2025 15:18\n",
      "Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 tháng đầu năm 2025, cầm gần 49.000 tỷ đồng tiền mặt\n",
      "https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty-dong-sau-6-thang-dau-nam-2025-cam-gan-49000-ty-dong-tien-mat-188250730151844214.chn\n",
      "Đã tìm hết các bài báo trong khoản thời gian cần tìm\n",
      "Đã thu thập 2 tin tức từ cafe f cho cổ phiếu VHM trong khoản thời gian 2025-07-30 14:45:00 đến 2025-08-01 14:45:00\n",
      "Đang xử lý URL: https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025.chn\n",
      "Đang xử lý bài báo thứ: 0\n",
      "Đang xử lý URL: https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty-dong-sau-6-thang-dau-nam-2025-cam-gan-49000-ty-dong-tien-mat-188250730151844214.chn\n",
      "Đang xử lý bài báo thứ: 1\n",
      "Đã lấy nội dung 2 bài báo từ cafe f\n",
      "Đã đọc file excel chứa danh sách các file pdf nghị quyết, nghị định cần lấy gồm 1 file pdf.\n",
      "\n",
      "📰 Đang xử lý bài báo thứ 0 (Lần thử 1):\n",
      "https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025.chn\n",
      "📎 Tìm thấy link PDF: https://cafef1.mediacdn.vn/download/310725/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025-0.pdf\n",
      "📁 Đã tải file PDF: vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025-0.pdf\n",
      "\n",
      "✅ Đã lưu file Excel chứa 1 nghị quyết, nghị định của công ty vào : D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx\n",
      "Đã vào xử lý trang 1...\n",
      "Cổ phiếu họ Vin giảm sàn\n",
      "VIC và VHM giảm hết biên độ, còn VRE mất 3,6% là nguyên nhân chính khiến VN-Index rơi sâu trong phiên giao dịch đầu tuần.\n",
      "Đang xử lý bài viết: Cổ phiếu họ Vin giảm sàn\n",
      "URL: https://vnexpress.net/co-phieu-ho-vin-giam-san-4896548.html\n",
      "Truy cập bài viết: https://vnexpress.net/co-phieu-ho-vin-giam-san-4896548.html\n",
      "9/6/2025, 15:40\n",
      "Thời gian đăng: 09/06/2025 15:40\n",
      "Bài viết quá cũ không nằm trong khoản thời gian cần crawl: 09/06/2025 15:40\n",
      "Đang xử lý trang 1...\n",
      "Đang xử lý: Chứng khoán lên sát đỉnh: Bức tranh tươi sáng với những 'con số đẹp'\n",
      "URL: https://vietnamnet.vn/chung-khoan-len-sat-dinh-buc-tranh-tuoi-sang-voi-nhung-con-so-dep-2437214.html\n",
      "Thời gian đăng: 28/08/2025 20:57\n",
      "Bài viết quá mới không nằm trong khoản thời gian cần crawl: 28/08/2025 20:57\n",
      "Đang xử lý: Giá vàng lên 127,7 triệu đồng/lượng, chứng khoán tăng chóng mặt phiên 26/8\n",
      "URL: https://vietnamnet.vn/gia-vang-len-127-7-trieu-dong-luong-chung-khoan-tang-chong-mat-phien-26-8-2436339.html\n",
      "Thời gian đăng: 26/08/2025 17:22\n",
      "Bài viết quá mới không nằm trong khoản thời gian cần crawl: 26/08/2025 17:22\n",
      "Đang xử lý: Chứng khoán đỏ lửa: 2,6 tỷ USD sang tay, VN-Index lao dốc mạnh\n",
      "URL: https://vietnamnet.vn/chung-khoan-do-lua-2-6-ty-usd-sang-tay-vn-index-lao-doc-manh-2435003.html\n",
      "Thời gian đăng: 22/08/2025 16:24\n",
      "Bài viết quá mới không nằm trong khoản thời gian cần crawl: 22/08/2025 16:24\n",
      "Đang xử lý: Tỷ phú Nguyễn Thị Phương Thảo giàu thứ hai Việt Nam\n",
      "URL: https://vietnamnet.vn/ty-phu-vietjet-nguyen-thi-phuong-thao-giau-thu-hai-viet-nam-2434215.html\n",
      "Thời gian đăng: 20/08/2025 21:05\n",
      "Bài viết quá mới không nằm trong khoản thời gian cần crawl: 20/08/2025 21:05\n",
      "Đang xử lý: VN-Index đứt mạch 9 phiên tăng: Chứng khoán còn hút dòng tiền tỷ USD?\n",
      "URL: https://vietnamnet.vn/vn-index-dut-mach-9-phien-tang-chung-khoan-con-hut-dong-tien-ty-usd-2433126.html\n",
      "Thời gian đăng: 18/08/2025 11:13\n",
      "Bài viết quá mới không nằm trong khoản thời gian cần crawl: 18/08/2025 11:13\n",
      "Đang xử lý: Chỉ số VN-Index giảm sâu hiếm có, mất hơn 64 điểm\n",
      "URL: https://vietnamnet.vn/ban-thao-manh-gan-3-ty-usd-chung-khoan-lao-doc-2426752.html\n",
      "Thời gian đăng: 29/07/2025 17:01\n",
      "Bài viết quá cũ không nằm trong khoản thời gian cần crawl: 29/07/2025 17:01\n",
      "Bắt đầu load bài viết ....\n",
      "Tổng số bài viết load được: 13\n",
      "Đang xử lý bài viết: VN-Index vừa tăng mạnh đã quay đầu giảm do áp lực chốt lời\n",
      "URL: https://tuoitre.vn/vn-index-vua-tang-manh-da-quay-dau-giam-do-ap-luc-chot-loi-20250609170737614.htm\n",
      "Truy cập bài viết: https://tuoitre.vn/vn-index-vua-tang-manh-da-quay-dau-giam-do-ap-luc-chot-loi-20250609170737614.htm\n",
      "Thời gian đăng: 09/06/2025 17:30\n",
      "Bài viết quá cũ không nằm trong khoản thời gian cần crawl: 09/06/2025 17:30\n",
      "Tổng số bài viết được lấy: 0\n"
     ]
    }
   ],
   "source": [
    "df_news, df_official_statement = crawl_news_and_official_statement_cafef_vnexpress_vietnamnet_tuoitre_for_stock('VHM', start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8242079c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "      <th>official_statement_pdf_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VHM</td>\n",
       "      <td>31/07/2025 17:18</td>\n",
       "      <td>VHM: Báo cáo tình hình quản trị 6 tháng đầu nă...</td>\n",
       "      <td>https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...</td>\n",
       "      <td>https://cafef1.mediacdn.vn/download/310725/vhm...</td>\n",
       "      <td>vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker              time                                             tittle  \\\n",
       "0    VHM  31/07/2025 17:18  VHM: Báo cáo tình hình quản trị 6 tháng đầu nă...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  https://cafef1.mediacdn.vn/download/310725/vhm...   \n",
       "\n",
       "                         official_statement_pdf_name  \n",
       "0  vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "877dcf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VHM</td>\n",
       "      <td>30/07/2025 15:18</td>\n",
       "      <td>Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 th...</td>\n",
       "      <td>https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...</td>\n",
       "      <td>Tại thời điểm 30/06/2025, tổng tài sản VHM đạt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker              time                                             tittle  \\\n",
       "0    VHM  30/07/2025 15:18  Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 th...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...   \n",
       "\n",
       "                                             Content  \n",
       "0  Tại thời điểm 30/06/2025, tổng tài sản VHM đạt...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================Chấm điểm xúc cảm  =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e81e4e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nhatk\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60e99a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nhatk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51740200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nhatk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bda29aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.81.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nhatk\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5705d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cc0fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt OpenAI Client\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"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\",  # Thay API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116208d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================Chấm điểm xúc cảm cho news======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3ab33708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def getSize_content(content):\n",
    "    size = len(content)\n",
    "    return size\n",
    "\n",
    "def getSize_sentence(sentence):\n",
    "    size = len(sentence.split(\" \"))\n",
    "    return size\n",
    "\n",
    "def add_messages(message, prompt):\n",
    "    return message + [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "def get_prompt(title, context):\n",
    "    return \"\"\"Chủ đề: {}\n",
    "    Đoạn văn: {}\n",
    "    \"\"\".format(title,\n",
    "               context.strip()\n",
    "              ).strip()\n",
    "\n",
    "def ask(messages, title, context):\n",
    "    prompt = get_prompt(title, context)\n",
    "    new_messages = add_messages(messages, prompt)\n",
    "    answer = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "    #   model=\"gpt-3.5-turbo\",\n",
    "      messages=new_messages,\n",
    "      temperature=0\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "def get_time(start_time, end_time):\n",
    "    cost = round(end_time - start_time)\n",
    "    return \"{}:{}\".format(cost // 60, cost % 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "19f7ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentiment_result(sentiment_str):\n",
    "    \"\"\"\n",
    "    Phân tích chuỗi như '#Reputation 0.2#Financial -0.3...' thành dict\n",
    "    \"\"\"\n",
    "    pattern = r\"#([\\w\\s&\\-]+)\\s(-?\\d+\\.?\\d*)\"\n",
    "    return {match[0].strip(): float(match[1]) for match in re.findall(pattern, sentiment_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dbd61e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(df, init_messages):\n",
    "\n",
    "  # Xóa các dòng không có nội dung ở cột 'Content'\n",
    "  df = df[df['Content'].notnull() & (df['Content'].str.strip() != '')]\n",
    "  tqdm.pandas(desc=\"Đang phân tích cảm xúc\")\n",
    "\n",
    "  sentiments = []\n",
    "\n",
    "  for i, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc=\"Phân tích tin tức\"), start=1):\n",
    "    paper_title_processing = row.get('tittle', f\"Không có tiêu đề\")\n",
    "    content = row['Content']\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "    content_processed = convert_sentence(content)\n",
    "    title = 'Tin tức'\n",
    "    text = ' '.join(content_processed)\n",
    "    # In chỉ tiêu đề bài báo\n",
    "    print(f\"\\n📝 Đang chấm điểm xúc cảm cho bài báo thứ {i}/{len(df)}: Tiêu đề: {paper_title_processing}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "      response = ask(init_messages, title, text)\n",
    "      print(\"response \" + str(response))\n",
    "\n",
    "      # Trích xuất nội dung từ response\n",
    "\n",
    "      result = response.choices[0].message.content\n",
    "      print(\"result \" + str(result))\n",
    "      # sentiment_data = json.loads(result)\n",
    "      sentiment_data = parse_sentiment_result(result)\n",
    "      print(\"sentiment_data \" + str(sentiment_data))\n",
    "\n",
    "    except Exception as e:\n",
    "      print(\"Error occurred:\", e)\n",
    "      sentiment_data = {\"error\": str(e)}\n",
    "    sentiments.append(sentiment_data)\n",
    "\n",
    "    # Tránh rate limit\n",
    "    time.sleep(4)\n",
    "\n",
    "    # Gộp các cảm xúc vào các cột mới trong df\n",
    "  sentiment_df = pd.DataFrame(sentiments)\n",
    "  df = pd.concat([df.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)\n",
    "  print('Đã chấm điểm xúc cảm đa khía cạnh cho tất cả tin tức')\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "df423e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment_for_stock(ticker, df):\n",
    "    init_user_prompt = \"\"\"Chủ đề: Tin tức\n",
    "Đoạn văn: Mục tiêu đón 8 triệu lượt khách quốc tế trong năm nay sẽ khó khả thi nếu chúng ta không có những động thái quyết liệt để xóa bỏ những rào cản với du khách, trong đó đầu tiên là chính sách visa. Kết thúc năm 2022, Việt Nam đang đứng cuối bảng phục hồi du lịch quốc tế của khu vực với tỷ lệ phục hồi chỉ 18,1%. Trong khi đó, tỷ lệ này ở các nước láng giềng như Thái Lan, Singapore, Malaysia hay Campuchia đều đạt từ 26 đến 31%. Với thực tế này, nếu không có giải pháp đột phá, tháo gỡ những rào cản hiện hữu, thì mục tiêu đón 8 triệu lượt du khách quốc tế năm 2023 của nước ta khó lòng đạt được. Đồng nghĩa, du lịch Việt Nam sẽ vuột cơ hội hút nguồn doanh thu lớn từ “mỏ vàng” khách quốc tế để nhanh chóng phục hồi.Theo giới chuyên môn, một trong những rào cản lớn đầu tiên khiến du khách quốc tế không mặn mà đến Việt Nam là hạn chế trong chính sách visa.Những bất cập trong chính sách visaAnh Yan Nang Oo, du khách Myanmar, cùng bạn đi du lịch Việt Nam. Do chỉ được cấp visa 14 ngày trong khi nhóm bạn muốn khám phá nhiều điểm đến tại Việt Nam nên họ phải lên kế hoạch di chuyển gấp từ Hạ Long, Hà Nội, Huế, Đà Nẵng, Phú Quốc. “Hôm ra sân bay để check-out về nước vừa hết 14 ngày, cũng may thủ tục xuất cảnh thuận lợi, không gặp vấn đề gì”, du khách này thở phào.Nhiều khách quốc tế mong muốn chính sách visa của Việt Nam được nới lỏng, cho phép du khách kéo dài thời hạn để có nhiều thời gian khám phá và trải nghiệm. Đó cũng là mong muốn của rất nhiều du khách quốc tế. Theo thống kê, Việt Nam chỉ miễn thị thực cho công dân của 24 quốc gia. Ðây là con số quá ít ỏi khi so sánh với các nước láng giềng như Singapore miễn visa cho 162 nước, Philippines là 157 nước, Malaysia là 162 nước, Thái Lan là 64 nước\"\"\"\n",
    "    init_assistant = \"\"\"#Reputation 0.0#Company Communication 0.0#Appointment 0.0#Financial -0.3#Regulatory -0.4#Sales -0.3#M&A 0.0#Legal -0.1#Dividend Policy 0.0#Risks -0.5#Rumors 0.0#Strategy -0.4#Options 0.0#IPO 0.0#Signal -0.2#Coverage 0.0#Fundamentals -0.2#Insider Activity 0.0#Price Action 0.0#Buyside 0.0#Technical Analysis 0.0#Trade -0.1#Central Banks 0.0#Currency 0.0#Conditions -0.5#Market -0.3#Volatility -0.4#Investor Sentiment -0.6#Retail Investor Behavior -0.4#Speculation 0.0#Domestic Institutional Behavior -0.2#Foreign Institutional Behavior -0.3#Black Swan Event 0.0\"\"\"\n",
    "    init_system_prompt = f'''Bạn là trợ lý hữu ích giúp tạo các cặp 'khía cạnh' - 'cảm xúc' từ một 'đoạn bài báo thời sự' cho cổ phiếu {ticker}. Dưới đây là các yêu cầu khi trích xuất khía cạnh và cảm xúc:\n",
    "1. Các khía cạnh được trích xuất bao gồm: Reputation (Danh tiếng của công ty); Company Communication (Truyền thông của công ty); Appointment (Bổ nhiệm); Financial (Tài chính); Regulatory (Cơ quan quản lí, chính sách); Sales (Bán hàng); M&A (Merge & Acquisition): Sáp nhập; Legal (Luật, tính hợp pháp); Dividend Policy (Chính sách cổ tức); Risks (rủi ro - ví dụ operation risk, credit risk - rủi ro tín dụng, reputation risk); Rumors (tin đồn); Strategy (chiến lược); Options (quyền chọn); IPO (initial public offering): lên sàn, niêm yết; Signal (tín hiệu để mua bán cổ phiếu); Coverage (báo cáo khuyến nghị buy sell hold của analysts); Fundamentals (các chỉ số trong phân tích cơ bản như P/E, P/B, Liabilites to Asset ratio); Insider Activity (các hoạt động nội bộ của công ty); Price Action (biến động giá); Buyside (các quĩ đầu tư, công ty nhỏ) ngược với sellside IB (Investment Banks); Technical Analysis (Phân tích kĩ thuật); Trade (thương mại, xuất nhập khẩu); Central Banks (Ngân hàng Trung ương, ở VN: Ngân hàng nhà nước, SBV); Currency (tiền tệ, tỉ giá); Conditions (điều kiện); Market (thị trường); Volatility (độ biến động, rủi ro); Investor Sentiment (Tâm lý chung của nhà đầu tư) lạc quan, lo lắng, chờ đợi...; Retail Investor Behavior (Hành vi của nhà đầu tư cá nhân) FOMO, hoảng loạn, giữ tiền mặt…; Speculation (Đầu cơ, hành vi đầu tư theo tin đồn hoặc kỳ vọng ngắn hạn); Domestic Institutional Behavior (Hành vi của tổ chức trong nước: tự doanh, bảo hiểm, quỹ trong nước) mua ròng, bán ròng, giảm tỷ trọng, rút khỏi cổ phiếu, các giao dịch thỏa thuận lớn giữa các tổ chức nội bộ; Foreign Institutional Behavior (Hành vi của nhà đầu tư tổ chức nước ngoài: ETF, quỹ ngoại, nhà đầu tư nước ngoài) mua ròng, bán ròng, tin tức đưa cổ phiếu vào, ra danh mục ETF, cảnh báo rủi ro, hạ mức xếp hạng cổ phiếu từ các tổ chức nước ngoài; Black Swan Event (Sự kiện thiên nga đen: hiếm gặp, khó dự đoán, gây ảnh hưởng nghiêm trọng như khủng hoảng, chiến tranh, dịch bệnh...).\n",
    "2. Ảnh hưởng nội dung đến các khía cạnh tương ứng. Với mỗi chỉ số ảnh hưởng nên là số thực trong khoảng từ -1 (thật sự tiêu cực) đến 1 (thật sự tích cực)\n",
    "3. Đảm bảo rằng các khía cạnh mà bạn trích xuất liên quan đến vấn đề của đoạn bài báo thời sự. Và khía cạnh nào bạn phân vân hãy xem không ảnh hưởng thì đánh là 0.\n",
    "4. Chú ý là dựa vào ảnh hưởng của nội dung đến các khía cạnh. Khi cho tôi kết quả hãy theo format: có kí tự '#' trước khía cạnh, phía sau khía cạnh là con số ảnh hưởng nội dung, ví dụ về format '#khía cạnh1 1#khía cạnh2 -1#khía cạnh3 0'. Tuân thủ theo format của tôi, không làm khác.'''\n",
    "    print(init_system_prompt)\n",
    "\n",
    "    init_messages = [\n",
    "      {\"role\": \"system\", \"content\": init_system_prompt},\n",
    "      {\"role\": \"user\", \"content\": init_user_prompt},\n",
    "      {\"role\": \"assistant\", \"content\": init_assistant},\n",
    "    ]\n",
    "    try:\n",
    "      df_result = score_sentiment(df, init_messages)\n",
    "      return df_result\n",
    "    except Exception as e:\n",
    "      print(f\"Lỗi khi xử lý cổ phiếu {ticker}: {e}\")\n",
    "      return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Chấm điểm xúc cảm cho các official stament pdf ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5825124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdf2image) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45fc8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fa9d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, os, gc, re\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3848447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONVERT PDF PAGES TO IMAGES IN RAM ---\n",
    "def convert_pdf_to_images_in_memory(pdf_path, dpi=200):\n",
    "    return convert_from_path(pdf_path, dpi=dpi)\n",
    "# --- CONVERT PIL IMAGE TO BASE64 ---\n",
    "def image_to_base64_pil(pil_image):\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "# --- BUILD EXAMPLE FOR GPT FROM PDF IN MEMORY ---\n",
    "def create_image_content_from_pdf(pdf_path, max_pages=3):\n",
    "    images = convert_pdf_to_images_in_memory(pdf_path)\n",
    "    image_contents = []\n",
    "    for i, img in enumerate(images):\n",
    "        if i >= max_pages:  # chỉ lấy tối đa max_pages trang\n",
    "            break\n",
    "        base64_img = image_to_base64_pil(img)\n",
    "        image_contents.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}\n",
    "        })\n",
    "    return image_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f0986042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GỬI GPT ---\n",
    "def analyze_pdf_scan_in_memory(pdf_path, init_user_prompt):\n",
    "    init_system_prompt = f'''Bạn là trợ lý hữu ích giúp tạo các cặp 'khía cạnh' - 'cảm xúc' từ một 'văn bản chính thức', 'văn bản pháp lý', 'nghị quyết', 'báo cáo chính thức' của công ty do công ty công bố. Dưới đây là các yêu cầu khi trích xuất khía cạnh và cảm xúc:\n",
    "1. Chúng tôi đã chuyển file pdf scan thành danh sách các ảnh. Bạn cần dùng khả năng xử lý ảnh (vision) của mình để “nhìn”, “đọc” và hiểu rõ nội dung trong các ảnh PNG để chấm điểm các khía cạnh cảm xúc.\n",
    "2. Các khía cạnh được trích xuất bao gồm: Reputation (Danh tiếng của công ty); Company Communication (Truyền thông của công ty); Appointment (Bổ nhiệm); Financial (Tài chính); Regulatory (Cơ quan quản lí, chính sách); Sales (Bán hàng); M&A (Merge & Acquisition): Sáp nhập; Legal (Luật, tính hợp pháp); Dividend Policy (Chính sách cổ tức); Risks (rủi ro - ví dụ operation risk, credit risk - rủi ro tín dụng, reputation risk); Rumors (tin đồn); Strategy (chiến lược); Options (quyền chọn); IPO (initial public offering): lên sàn, niêm yết; Signal (tín hiệu để mua bán cổ phiếu); Coverage (báo cáo khuyến nghị buy sell hold của analysts); Fundamentals (các chỉ số trong phân tích cơ bản như P/E, P/B, Liabilites to Asset ratio); Insider Activity (các hoạt động nội bộ của công ty); Price Action (biến động giá); Buyside (các quĩ đầu tư, công ty nhỏ) ngược với sellside IB (Investment Banks); Technical Analysis (Phân tích kĩ thuật); Trade (thương mại, xuất nhập khẩu); Central Banks (Ngân hàng Trung ương, ở VN: Ngân hàng nhà nước, SBV); Currency (tiền tệ, tỉ giá); Conditions (điều kiện); Market (thị trường); Volatility (độ biến động, rủi ro); Investor Sentiment (Tâm lý chung của nhà đầu tư) lạc quan, lo lắng, chờ đợi...; Retail Investor Behavior (Hành vi của nhà đầu tư cá nhân) FOMO, hoảng loạn, giữ tiền mặt…; Speculation (Đầu cơ, hành vi đầu tư theo tin đồn hoặc kỳ vọng ngắn hạn); Domestic Institutional Behavior (Hành vi của tổ chức trong nước: tự doanh, bảo hiểm, quỹ trong nước) mua ròng, bán ròng, giảm tỷ trọng, rút khỏi cổ phiếu, các giao dịch thỏa thuận lớn giữa các tổ chức nội bộ; Foreign Institutional Behavior (Hành vi của nhà đầu tư tổ chức nước ngoài: ETF, quỹ ngoại, nhà đầu tư nước ngoài) mua ròng, bán ròng, tin tức đưa cổ phiếu vào, ra danh mục ETF, cảnh báo rủi ro, hạ mức xếp hạng cổ phiếu từ các tổ chức nước ngoài; Black Swan Event (Sự kiện thiên nga đen: hiếm gặp, khó dự đoán, gây ảnh hưởng nghiêm trọng như khủng hoảng, chiến tranh, dịch bệnh...).\n",
    "3. Ảnh hưởng nội dung đến các khía cạnh tương ứng. Với mỗi chỉ số ảnh hưởng nên là số thực trong khoảng từ -1 (thật sự tiêu cực) đến 1 (thật sự tích cực)\n",
    "4. Đảm bảo rằng các khía cạnh mà bạn trích xuất liên quan đến vấn đề của văn bản trên. Và khía cạnh nào bạn phân vân hãy xem không ảnh hưởng thì đánh là 0.\n",
    "5. Chú ý là dựa vào ảnh hưởng của nội dung đến các khía cạnh. Khi cho tôi kết quả hãy theo format: có kí tự '#' trước khía cạnh, phía sau khía cạnh là con số ảnh hưởng nội dung, ví dụ về format '#khía cạnh1 1#khía cạnh2 -1#khía cạnh3 0'. Tuân thủ theo format của tôi, không làm khác.'''\n",
    "    query = \"Dựa vào các hình ảnh sau, hãy đánh giá ảnh hưởng của nội dung trong ảnh đến từng khía cạnh theo yêu cầu đã mô tả.\"\n",
    "    \n",
    "    init_assistant = \"\"\"#Reputation 0.1#Company Communication 0.3#Appointment 0.0#Financial 0.5#Regulatory 0.0#Sales 0.0#M&A 0.0#Legal 0.0#Dividend Policy 0.0#Risks -0.1#Rumors 0.0#Strategy 0.0#Options 0.0#IPO 0.0#Signal 0.1#Coverage 0.0#Fundamentals 0.4#Insider Activity 0.0#Price Action 0.0#Buyside 0.0#Technical Analysis 0.0#Trade 0.0#Central Banks 0.0#Currency 0.0#Conditions 0.1#Market 0.0#Volatility 0.1#Investor Sentiment 0.2#Retail Investor Behavior 0.0#Speculation 0.0#Domestic Institutional Behavior 0.0#Foreign Institutional Behavior 0.0#Black Swan Event 0.0\"\"\"\n",
    "    image_contents = create_image_content_from_pdf(pdf_path)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": init_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}] + init_user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": init_assistant},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}] + image_contents},\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPT lỗi: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        del image_contents\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7797a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYZE TỪNG FILE PDF TRONG EXCEL ---\n",
    "def analyze_official_statements_for_stock(folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt):\n",
    "    df = pd.read_excel(folder_excel_input)\n",
    "\n",
    "    sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "    for col in sentiment_columns:\n",
    "        df[col] = None\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            pdf_name = row.get(\"official_statement_pdf_name\", \"\")\n",
    "            if not isinstance(pdf_name, str) or not pdf_name.endswith(\".pdf\"):\n",
    "                continue\n",
    "            pdf_path = os.path.join(folder_pdf_input, pdf_name)\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"❌ Mất file: {pdf_name}\")\n",
    "                continue\n",
    "            \n",
    "            init_user_prompt = create_image_content_from_pdf(pdf_path_init_user_prompt)\n",
    "            sentiment_str = analyze_pdf_scan_in_memory(pdf_path, init_user_prompt)\n",
    "            if sentiment_str is None:\n",
    "                continue\n",
    "\n",
    "            sentiment_data = parse_sentiment_result(sentiment_str)\n",
    "            for col in sentiment_columns:\n",
    "                if col in sentiment_data:\n",
    "                    df.at[i, col] = sentiment_data[col]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Lỗi ở {pdf_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Đã chấm điểm xúc cảm cho các nghị quyết, nghị định của công ty\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "161fb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score_for_all(ticker, df_news, folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt):\n",
    "    print(\"Đang chấm điểm xúc cảm cho các bài báo\")\n",
    "    df_news_sentiemt_scored = score_sentiment_for_stock(ticker, df_news)\n",
    "    print(\"Đang chấm điểm các tuyên bố chính thức\")\n",
    "    df_official_statement_scored = analyze_official_statements_for_stock(folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt)\n",
    "    print(\"Đang kết hợp các kết quả chấm điểm\")\n",
    "    df = pd.concat([df_news_sentiemt_scored, df_official_statement_scored], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "79642611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang chấm điểm xúc cảm cho các bài báo\n",
      "Bạn là trợ lý hữu ích giúp tạo các cặp 'khía cạnh' - 'cảm xúc' từ một 'đoạn bài báo thời sự' cho cổ phiếu VHM. Dưới đây là các yêu cầu khi trích xuất khía cạnh và cảm xúc:\n",
      "1. Các khía cạnh được trích xuất bao gồm: Reputation (Danh tiếng của công ty); Company Communication (Truyền thông của công ty); Appointment (Bổ nhiệm); Financial (Tài chính); Regulatory (Cơ quan quản lí, chính sách); Sales (Bán hàng); M&A (Merge & Acquisition): Sáp nhập; Legal (Luật, tính hợp pháp); Dividend Policy (Chính sách cổ tức); Risks (rủi ro - ví dụ operation risk, credit risk - rủi ro tín dụng, reputation risk); Rumors (tin đồn); Strategy (chiến lược); Options (quyền chọn); IPO (initial public offering): lên sàn, niêm yết; Signal (tín hiệu để mua bán cổ phiếu); Coverage (báo cáo khuyến nghị buy sell hold của analysts); Fundamentals (các chỉ số trong phân tích cơ bản như P/E, P/B, Liabilites to Asset ratio); Insider Activity (các hoạt động nội bộ của công ty); Price Action (biến động giá); Buyside (các quĩ đầu tư, công ty nhỏ) ngược với sellside IB (Investment Banks); Technical Analysis (Phân tích kĩ thuật); Trade (thương mại, xuất nhập khẩu); Central Banks (Ngân hàng Trung ương, ở VN: Ngân hàng nhà nước, SBV); Currency (tiền tệ, tỉ giá); Conditions (điều kiện); Market (thị trường); Volatility (độ biến động, rủi ro); Investor Sentiment (Tâm lý chung của nhà đầu tư) lạc quan, lo lắng, chờ đợi...; Retail Investor Behavior (Hành vi của nhà đầu tư cá nhân) FOMO, hoảng loạn, giữ tiền mặt…; Speculation (Đầu cơ, hành vi đầu tư theo tin đồn hoặc kỳ vọng ngắn hạn); Domestic Institutional Behavior (Hành vi của tổ chức trong nước: tự doanh, bảo hiểm, quỹ trong nước) mua ròng, bán ròng, giảm tỷ trọng, rút khỏi cổ phiếu, các giao dịch thỏa thuận lớn giữa các tổ chức nội bộ; Foreign Institutional Behavior (Hành vi của nhà đầu tư tổ chức nước ngoài: ETF, quỹ ngoại, nhà đầu tư nước ngoài) mua ròng, bán ròng, tin tức đưa cổ phiếu vào, ra danh mục ETF, cảnh báo rủi ro, hạ mức xếp hạng cổ phiếu từ các tổ chức nước ngoài; Black Swan Event (Sự kiện thiên nga đen: hiếm gặp, khó dự đoán, gây ảnh hưởng nghiêm trọng như khủng hoảng, chiến tranh, dịch bệnh...).\n",
      "2. Ảnh hưởng nội dung đến các khía cạnh tương ứng. Với mỗi chỉ số ảnh hưởng nên là số thực trong khoảng từ -1 (thật sự tiêu cực) đến 1 (thật sự tích cực)\n",
      "3. Đảm bảo rằng các khía cạnh mà bạn trích xuất liên quan đến vấn đề của đoạn bài báo thời sự. Và khía cạnh nào bạn phân vân hãy xem không ảnh hưởng thì đánh là 0.\n",
      "4. Chú ý là dựa vào ảnh hưởng của nội dung đến các khía cạnh. Khi cho tôi kết quả hãy theo format: có kí tự '#' trước khía cạnh, phía sau khía cạnh là con số ảnh hưởng nội dung, ví dụ về format '#khía cạnh1 1#khía cạnh2 -1#khía cạnh3 0'. Tuân thủ theo format của tôi, không làm khác.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phân tích tin tức:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Đang chấm điểm xúc cảm cho bài báo thứ 1/1: Tiêu đề: Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 tháng đầu năm 2025, cầm gần 49.000 tỷ đồng tiền mặt\n",
      "response ChatCompletion(id='chatcmpl-CCLlxeRfwHvT42K5qjCQSzRmgn9W8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='#Reputation 0.5#Company Communication 0.3#Appointment 0.0#Financial 0.7#Regulatory 0.0#Sales 0.6#M&A 0.0#Legal 0.0#Dividend Policy 0.0#Risks 0.0#Rumors 0.0#Strategy 0.6#Options 0.0#IPO 0.0#Signal 0.5#Coverage 0.0#Fundamentals 0.6#Insider Activity 0.0#Price Action 0.4#Buyside 0.0#Technical Analysis 0.0#Trade 0.0#Central Banks 0.0#Currency 0.0#Conditions 0.0#Market 0.5#Volatility 0.0#Investor Sentiment 0.6#Retail Investor Behavior 0.0#Speculation 0.0#Domestic Institutional Behavior 0.0#Foreign Institutional Behavior 0.0#Black Swan Event 0.0', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757059665, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_cbf1785567', usage=CompletionUsage(completion_tokens=225, prompt_tokens=2696, total_tokens=2921, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "result #Reputation 0.5#Company Communication 0.3#Appointment 0.0#Financial 0.7#Regulatory 0.0#Sales 0.6#M&A 0.0#Legal 0.0#Dividend Policy 0.0#Risks 0.0#Rumors 0.0#Strategy 0.6#Options 0.0#IPO 0.0#Signal 0.5#Coverage 0.0#Fundamentals 0.6#Insider Activity 0.0#Price Action 0.4#Buyside 0.0#Technical Analysis 0.0#Trade 0.0#Central Banks 0.0#Currency 0.0#Conditions 0.0#Market 0.5#Volatility 0.0#Investor Sentiment 0.6#Retail Investor Behavior 0.0#Speculation 0.0#Domestic Institutional Behavior 0.0#Foreign Institutional Behavior 0.0#Black Swan Event 0.0\n",
      "sentiment_data {'Reputation': 0.5, 'Company Communication': 0.3, 'Appointment': 0.0, 'Financial': 0.7, 'Regulatory': 0.0, 'Sales': 0.6, 'M&A': 0.0, 'Legal': 0.0, 'Dividend Policy': 0.0, 'Risks': 0.0, 'Rumors': 0.0, 'Strategy': 0.6, 'Options': 0.0, 'IPO': 0.0, 'Signal': 0.5, 'Coverage': 0.0, 'Fundamentals': 0.6, 'Insider Activity': 0.0, 'Price Action': 0.4, 'Buyside': 0.0, 'Technical Analysis': 0.0, 'Trade': 0.0, 'Central Banks': 0.0, 'Currency': 0.0, 'Conditions': 0.0, 'Market': 0.5, 'Volatility': 0.0, 'Investor Sentiment': 0.6, 'Retail Investor Behavior': 0.0, 'Speculation': 0.0, 'Domestic Institutional Behavior': 0.0, 'Foreign Institutional Behavior': 0.0, 'Black Swan Event': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phân tích tin tức: 100%|██████████| 1/1 [00:10<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã chấm điểm xúc cảm đa khía cạnh cho tất cả tin tức\n",
      "Đang chấm điểm các tuyên bố chính thức\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã chấm điểm xúc cảm cho các nghị quyết, nghị định của công ty\n",
      "Đang kết hợp các kết quả chấm điểm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>Company Communication</th>\n",
       "      <th>Appointment</th>\n",
       "      <th>Financial</th>\n",
       "      <th>Regulatory</th>\n",
       "      <th>...</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Market</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Investor Sentiment</th>\n",
       "      <th>Retail Investor Behavior</th>\n",
       "      <th>Speculation</th>\n",
       "      <th>Domestic Institutional Behavior</th>\n",
       "      <th>Foreign Institutional Behavior</th>\n",
       "      <th>Black Swan Event</th>\n",
       "      <th>official_statement_pdf_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VHM</td>\n",
       "      <td>30/07/2025 15:18</td>\n",
       "      <td>Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 th...</td>\n",
       "      <td>https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...</td>\n",
       "      <td>Tại thời điểm 30/06/2025, tổng tài sản VHM đạt...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VHM</td>\n",
       "      <td>31/07/2025 17:18</td>\n",
       "      <td>VHM: Báo cáo tình hình quản trị 6 tháng đầu nă...</td>\n",
       "      <td>https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...</td>\n",
       "      <td>https://cafef1.mediacdn.vn/download/310725/vhm...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker              time                                             tittle  \\\n",
       "0    VHM  30/07/2025 15:18  Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 th...   \n",
       "1    VHM  31/07/2025 17:18  VHM: Báo cáo tình hình quản trị 6 tháng đầu nă...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...   \n",
       "1  https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...   \n",
       "\n",
       "                                             Content Reputation  \\\n",
       "0  Tại thời điểm 30/06/2025, tổng tài sản VHM đạt...        0.5   \n",
       "1  https://cafef1.mediacdn.vn/download/310725/vhm...        0.2   \n",
       "\n",
       "  Company Communication Appointment Financial Regulatory  ... Conditions  \\\n",
       "0                   0.3         0.0       0.7        0.0  ...        0.0   \n",
       "1                   0.2         0.3       0.0        0.1  ...        0.0   \n",
       "\n",
       "  Market Volatility Investor Sentiment Retail Investor Behavior Speculation  \\\n",
       "0    0.5        0.0                0.6                      0.0         0.0   \n",
       "1    0.0        0.0                0.0                      0.0         0.0   \n",
       "\n",
       "  Domestic Institutional Behavior Foreign Institutional Behavior  \\\n",
       "0                             0.0                            0.0   \n",
       "1                             0.0                            0.0   \n",
       "\n",
       "  Black Swan Event                        official_statement_pdf_name  \n",
       "0              0.0                                                NaN  \n",
       "1              0.0  vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_excel_input = rf\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx\"\n",
    "folder_pdf_input = rf\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\official_statement_pdf\"\n",
    "pdf_path_init_user_prompt = r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\SA\\data\\v2\\sentiment_score_data\\official_statement\\example_pdf_score\\acb-giai-trinh-chenh-lech-lnst-quy-1-2023-so-voi-cung-ky-nam-truoc-0.pdf\"\n",
    "df_news_official_statement_scored = sentiment_score_for_all('VHM',df_news,folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt)\n",
    "df_news_official_statement_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "efdc361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_norm(sentiment_df):\n",
    "    # Xác định các cột khía cạnh cảm xúc (giả sử các cột này có tên như dưới)\n",
    "    sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "\n",
    "    # Đảm bảo các cột này đều tồn tại trong DataFrame\n",
    "    existing_columns = [col for col in sentiment_columns if col in sentiment_df.columns]\n",
    "\n",
    "    # Thay NaN bằng 0 nếu chưa làm\n",
    "    sentiment_df[existing_columns] = sentiment_df[existing_columns].fillna(0)\n",
    "\n",
    "    # Tính Euclidean norm cho từng hàng\n",
    "    sentiment_df['euclidean norm'] = np.linalg.norm(sentiment_df[existing_columns].values, axis=1)\n",
    "\n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c14bde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(name: str) -> str:\n",
    "    return name.strip().lower().replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c1a1b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_for_ta_fa(ta_fa_df, sentiment_df):\n",
    "    ta_fa_df = ta_fa_df.reset_index(drop=True)\n",
    "    # Xử lý thiếu và chuẩn hóa\n",
    "    sentiment_df.fillna(0, inplace=True)\n",
    "    sentiment_df = compute_sentiment_norm(sentiment_df)\n",
    "    # sentiment_df.to_excel(r\"D:\\sentiment_df.xlsx\", index=False)\n",
    "\n",
    "    # Định dạng thời gian\n",
    "    sentiment_df['time'] = pd.to_datetime(sentiment_df['time'], dayfirst=True, errors='coerce')\n",
    "    ta_fa_df['time'] = pd.to_datetime(ta_fa_df['time'], dayfirst=True)\n",
    "\n",
    "    # Danh sách cột cảm xúc\n",
    "    sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "\n",
    "    # Thêm các cột cảm xúc vào `ta_fa_df`, mặc định là 0\n",
    "    for col in sentiment_columns:\n",
    "        ta_fa_df[col] = 0.0\n",
    "\n",
    "    for i in range(len(ta_fa_df)):\n",
    "        current_date = ta_fa_df.at[i, 'time']\n",
    "        # next_trading_date = ta_fa_df.at[i + 1, 'time']\n",
    "        try:\n",
    "            next_trading_date = ta_fa_df.at[i + 1, 'time']\n",
    "        except:\n",
    "            next_trading_date = current_date + timedelta(days=1)\n",
    "        from_time = datetime.combine(current_date, datetime.min.time()) + timedelta(hours=14, minutes=45)\n",
    "        to_time = datetime.combine(next_trading_date, datetime.min.time()) + timedelta(hours=14, minutes=45)\n",
    "\n",
    "        mask = (sentiment_df['time'] >= from_time) & (sentiment_df['time'] < to_time)\n",
    "        news_in_range = sentiment_df[mask]\n",
    "\n",
    "        print(f\"\\n📅 Ngày đang xét: {current_date.date()}\")\n",
    "        print(f\"📅 Ngày giao dịch tiếp theo: {next_trading_date.date()}\")\n",
    "        print(f\"📰 Các tin tức từ {from_time} đến {to_time} sẽ ảnh hưởng đến nhãn tại ngày {current_date.date()}:\")\n",
    "\n",
    "        if news_in_range.empty:\n",
    "            print(\" - Không có tin tức ảnh hưởng.\")\n",
    "            print(\"📊 Các khía cạnh cảm xúc (mặc định 0):\")\n",
    "            continue\n",
    "\n",
    "        max_norm_idx = news_in_range['euclidean norm'].idxmax()\n",
    "        max_news = news_in_range.loc[max_norm_idx]\n",
    "\n",
    "        for idx, row in news_in_range.iterrows():\n",
    "            title = row.get('tittle', '[Không có tiêu đề]')\n",
    "            time_str = row['time']\n",
    "            print(f\" - {time_str}: {title}\")\n",
    "            for col in sentiment_columns:\n",
    "                    print(f\"   - {col}: {row[col]}\")\n",
    "            if idx == max_norm_idx:\n",
    "                print(\"  🔍 Đây là tin có euclidean norm cao nhất.\")\n",
    "                print(\"  📊 Các khía cạnh cảm xúc:\")\n",
    "                for col in sentiment_columns:\n",
    "                    print(f\"   - {col}: {row[col]}\")\n",
    "\n",
    "        # Nếu muốn tính tổng hợp nhiều tin trong ngày\n",
    "        ta_fa_df.at[i, 'num_sa_news'] = len(news_in_range)\n",
    "        ta_fa_df.at[i, 'mean_sentiment_norm'] = news_in_range['euclidean norm'].mean()\n",
    "        ta_fa_df.at[i, 'max_sentiment_norm'] = news_in_range['euclidean norm'].max()\n",
    "        ta_fa_df.at[i, 'std_sentiment_score'] = news_in_range[sentiment_columns].std().mean()\n",
    "\n",
    "        # Tính tổng các cảm xúc tích cực / tiêu cực\n",
    "        pos_sum = news_in_range[sentiment_columns].applymap(lambda x: x if x > 0 else 0).sum().sum()\n",
    "        neg_sum = news_in_range[sentiment_columns].applymap(lambda x: x if x < 0 else 0).sum().sum()\n",
    "\n",
    "        ta_fa_df.at[i, 'sum_positive_sa'] = pos_sum\n",
    "        ta_fa_df.at[i, 'sum_negative_sa'] = neg_sum\n",
    "\n",
    "        # ➕ Tính khoảng cách đến 14:45 hôm đó (âm nếu tin xảy ra trước giờ đóng cửa)\n",
    "        news_time = max_news['time']\n",
    "        cutoff_time = datetime.combine(current_date, datetime.min.time()) + timedelta(hours=14, minutes=45)\n",
    "        time_distance_minutes = (cutoff_time - news_time).total_seconds() / 60.0\n",
    "        ta_fa_df.at[i, 'time_distance_from_sa_to_close_minutes'] = time_distance_minutes\n",
    "\n",
    "        # Gán các giá trị cảm xúc vào ta_fa_df tại dòng i\n",
    "        for col in sentiment_columns:\n",
    "            ta_fa_df.at[i, col] = max_news[col]\n",
    "    \n",
    "    # Tạo các cột hiệu ứng chậm từ cảm xúc hôm trước\n",
    "\n",
    "    for col in sentiment_columns:\n",
    "        ta_fa_df[f\"{to_snake_case(col)}_p1d\"] = ta_fa_df[col].shift(1)\n",
    "\n",
    "    meta_cols = ['num_sa_news', 'mean_sentiment_norm', 'max_sentiment_norm', 'std_sentiment_score', 'sum_positive_sa', 'sum_negative_sa', 'time_distance_from_sa_to_close_minutes']\n",
    "    for col in meta_cols:\n",
    "        ta_fa_df[f\"{col}_p1d\"] = ta_fa_df[col].shift(1)\n",
    "    ta_fa_df.fillna(0, inplace=True)\n",
    "\n",
    "    return ta_fa_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1fb4c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Ngày đang xét: 2025-07-30\n",
      "📅 Ngày giao dịch tiếp theo: 2025-07-31\n",
      "📰 Các tin tức từ 2025-07-30 14:45:00 đến 2025-07-31 14:45:00 sẽ ảnh hưởng đến nhãn tại ngày 2025-07-30:\n",
      " - 2025-07-30 15:18:00: Vinhomes (VHM) báo lãi 11.000 tỷ đồng sau 6 tháng đầu năm 2025, cầm gần 49.000 tỷ đồng tiền mặt\n",
      "   - Reputation: 0.5\n",
      "   - Company Communication: 0.3\n",
      "   - Appointment: 0.0\n",
      "   - Financial: 0.7\n",
      "   - Regulatory: 0.0\n",
      "   - Sales: 0.6\n",
      "   - M&A: 0.0\n",
      "   - Legal: 0.0\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.6\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.5\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.6\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.4\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.5\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.6\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n",
      "  🔍 Đây là tin có euclidean norm cao nhất.\n",
      "  📊 Các khía cạnh cảm xúc:\n",
      "   - Reputation: 0.5\n",
      "   - Company Communication: 0.3\n",
      "   - Appointment: 0.0\n",
      "   - Financial: 0.7\n",
      "   - Regulatory: 0.0\n",
      "   - Sales: 0.6\n",
      "   - M&A: 0.0\n",
      "   - Legal: 0.0\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.6\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.5\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.6\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.4\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.5\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.6\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n",
      "\n",
      "📅 Ngày đang xét: 2025-07-31\n",
      "📅 Ngày giao dịch tiếp theo: 2025-08-01\n",
      "📰 Các tin tức từ 2025-07-31 14:45:00 đến 2025-08-01 14:45:00 sẽ ảnh hưởng đến nhãn tại ngày 2025-07-31:\n",
      " - 2025-07-31 17:18:00: VHM: Báo cáo tình hình quản trị 6 tháng đầu năm 2025\n",
      "   - Reputation: 0.2\n",
      "   - Company Communication: 0.2\n",
      "   - Appointment: 0.3\n",
      "   - Financial: 0.0\n",
      "   - Regulatory: 0.1\n",
      "   - Sales: 0.0\n",
      "   - M&A: 0.2\n",
      "   - Legal: 0.1\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.1\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.0\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.0\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.0\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.0\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.0\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n",
      "  🔍 Đây là tin có euclidean norm cao nhất.\n",
      "  📊 Các khía cạnh cảm xúc:\n",
      "   - Reputation: 0.2\n",
      "   - Company Communication: 0.2\n",
      "   - Appointment: 0.3\n",
      "   - Financial: 0.0\n",
      "   - Regulatory: 0.1\n",
      "   - Sales: 0.0\n",
      "   - M&A: 0.2\n",
      "   - Legal: 0.1\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.1\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.0\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.0\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.0\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.0\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.0\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>p/b_previous_quarter</th>\n",
       "      <th>P/B_Same_Period_Last_Year</th>\n",
       "      <th>P/B_d_1</th>\n",
       "      <th>P/B_d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>domestic_institutional_behavior_p1d</th>\n",
       "      <th>foreign_institutional_behavior_p1d</th>\n",
       "      <th>black_swan_event_p1d</th>\n",
       "      <th>num_sa_news_p1d</th>\n",
       "      <th>mean_sentiment_norm_p1d</th>\n",
       "      <th>max_sentiment_norm_p1d</th>\n",
       "      <th>std_sentiment_score_p1d</th>\n",
       "      <th>sum_positive_sa_p1d</th>\n",
       "      <th>sum_negative_sa_p1d</th>\n",
       "      <th>time_distance_from_sa_to_close_minutes_p1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.5</td>\n",
       "      <td>5198000</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>89.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>88.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7811500</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.711724</td>\n",
       "      <td>1.711724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 396 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  open  high   low  close   volume  p/b_previous_quarter  \\\n",
       "0 2025-07-30  92.1  92.5  89.0   91.5  5198000              1.971749   \n",
       "1 2025-07-31  89.7  91.5  88.2   90.0  7811500              1.971749   \n",
       "\n",
       "   P/B_Same_Period_Last_Year   P/B_d_1   P/B_d_2  ...  \\\n",
       "0                   1.027068  1.971749  1.776398  ...   \n",
       "1                   1.027068  1.971749  1.776398  ...   \n",
       "\n",
       "   domestic_institutional_behavior_p1d  foreign_institutional_behavior_p1d  \\\n",
       "0                                  0.0                                 0.0   \n",
       "1                                  0.0                                 0.0   \n",
       "\n",
       "   black_swan_event_p1d  num_sa_news_p1d  mean_sentiment_norm_p1d  \\\n",
       "0                   0.0              0.0                 0.000000   \n",
       "1                   0.0              1.0                 1.711724   \n",
       "\n",
       "   max_sentiment_norm_p1d  std_sentiment_score_p1d  sum_positive_sa_p1d  \\\n",
       "0                0.000000                      0.0                  0.0   \n",
       "1                1.711724                      0.0                  5.3   \n",
       "\n",
       "   sum_negative_sa_p1d  time_distance_from_sa_to_close_minutes_p1d  \n",
       "0                  0.0                                         0.0  \n",
       "1                  0.0                                       -33.0  \n",
       "\n",
       "[2 rows x 396 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta_fa_sa_df = apply_sentiment_for_ta_fa(df_ta_fa, df_news_official_statement_scored)\n",
    "ta_fa_sa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6d9acb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_fa_sa_df.to_excel(r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\ta_fa_sa_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d934ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "01aac4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã load xong 35865 dòng dữ liệu từ 30 cổ phiếu.\n",
      "✅ Đã load xong 7380 dòng dữ liệu từ 30 cổ phiếu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:51:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6493224932249323\n"
     ]
    }
   ],
   "source": [
    "# traning model\n",
    "def dataframe_to_x_y(df, feature, target):\n",
    "  X_train, y_train = [], []\n",
    "  X_val, y_val = [], []\n",
    "  n = len(df)\n",
    "  split_index = int(n * 0.8)\n",
    "  X_train = df[feature][:split_index]\n",
    "  y_train = df[target][:split_index]\n",
    "  X_val = df[feature][split_index:]\n",
    "  y_val = df[target][split_index:]\n",
    "  return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "\n",
    "sentiment_p1d_columns = [\n",
    "        'reputation_p1d', 'company_communication_p1d', 'appointment_p1d', 'financial_p1d', 'regulatory_p1d',\n",
    "        'sales_p1d', 'm&a_p1d', 'legal_p1d', 'dividend_policy_p1d', 'risks_p1d', 'rumors_p1d', 'strategy_p1d',\n",
    "        'options_p1d', 'ipo_p1d', 'signal_p1d', 'coverage_p1d', 'fundamentals_p1d', 'insider_activity_p1d',\n",
    "        'price_action_p1d', 'buyside_p1d', 'technical_analysis_p1d', 'trade_p1d', 'central_banks_p1d',\n",
    "        'currency_p1d', 'conditions_p1d', 'market_p1d', 'volatility_p1d', 'investor_sentiment_p1d', 'retail_investor_behavior_p1d',\n",
    "        'speculation_p1d', 'domestic_institutional_behavior_p1d', 'foreign_institutional_behavior_p1d', 'black_swan_event_p1d',\n",
    "    ]\n",
    "meta_cols = ['num_sa_news', 'mean_sentiment_norm', 'max_sentiment_norm', 'std_sentiment_score', 'sum_positive_sa', 'sum_negative_sa', 'time_distance_from_sa_to_close_minutes']\n",
    "meta_p1d_cols = ['num_sa_news_p1d', 'mean_sentiment_norm_p1d', 'max_sentiment_norm_p1d', 'std_sentiment_score_p1d', 'sum_positive_sa_p1d', 'sum_negative_sa_p1d', 'time_distance_from_sa_to_close_minutes_p1d']\n",
    "\n",
    "sentiment_feature_selected =  sentiment_columns + sentiment_p1d_columns + meta_cols + meta_p1d_cols\n",
    "ta_features = ['volume_ma','volume_to_volume_ma_ratio','ema_12','ema_26','sma_20','sma_50','roc_5','roc_1','roc_9','%K','%R','cci','obv','macd','signal_line','macd_histogram','rsi','rsi_base_ma','rsi_rsi_base_ma_ratio','bb_bbm','bb_bbh','bb_bbl','bb_bbp','bb_bbh_bb_bbl_ratio','hl_ratio', 'co_ratio', 'price_range', 'sma_ratio_20_50', 'ema_ratio_12_26', 'bb_width', 'bb_position', 'rsi_overbought', 'rsi_oversold', 'rsi_neutral', 'macd_bullish', 'momentum_5', 'momentum_10','log_return','volatility_5d','volatility_10d','volatility_20d','volatility_30d','mean_log_return_5d','mean_log_return_10d','mean_log_return_20d','mean_log_return_30d','sharpe_like_5d','sharpe_like_10d','sharpe_like_20d','sharpe_like_30d','up_streak','pos_log_return_ratio_20d','z_score_5d','z_score_10d','z_score_20d','z_score_30d','annual_return','daily_return','sharpe_ratio',\n",
    "               'rsi_vn30','rsi_base_ma_vn30','rsi_rsi_base_ma_ratio_vn30','volume_ma_vn30','volume_to_volume_ma_ratio_vn30','bb_bbm_vn30','bb_bbh_vn30','bb_bbl_vn30','bb_bbp_vn30','bb_bbh_bb_bbl_ratio_vn30','roc_1_vn30', 'roc_5_vn30', 'roc_9_vn30','%K_vn30','%R_vn30','cci_vn30','obv_vn30','ema_12_vn30','ema_26_vn30','sma_20_vn30','sma_50_vn30', 'hl_ratio_vn30', 'co_ratio_vn30', 'price_range_vn30', 'sma_ratio_20_50_vn30', 'ema_ratio_12_26_vn30', 'bb_width_vn30', 'bb_position_vn30', 'rsi_overbought_vn30', 'rsi_oversold_vn30', 'rsi_neutral_vn30', 'momentum_5_vn30', 'momentum_10_vn30', 'log_return_vn30','volatility_5d_vn30','volatility_10d_vn30','volatility_20d_vn30','volatility_30d_vn30','mean_log_return_5d_vn30','mean_log_return_10d_vn30','mean_log_return_20d_vn30','mean_log_return_30d_vn30','sharpe_like_5d_vn30','sharpe_like_10d_vn30','sharpe_like_20d_vn30','sharpe_like_30d_vn30','up_streak_vn30','pos_log_return_ratio_20d_vn30','z_score_5d_vn30','z_score_10d_vn30','z_score_20d_vn30','z_score_30d_vn30','annual_return_vn30','daily_return_vn30','sharpe_ratio_vn30',\n",
    "               'rsi_vni','rsi_base_ma_vni','rsi_rsi_base_ma_ratio_vni','volume_ma_vni','volume_to_volume_ma_ratio_vni','bb_bbm_vni','bb_bbh_vni','bb_bbl_vni','bb_bbp_vni','bb_bbh_bb_bbl_ratio_vni','roc_1_vni', 'roc_5_vni', 'roc_9_vni','%K_vni','%R_vni','cci_vni','obv_vni','ema_12_vni','ema_26_vni','sma_20_vni','sma_50_vni', 'hl_ratio_vni', 'co_ratio_vni', 'price_range_vni', 'sma_ratio_20_50_vni', 'ema_ratio_12_26_vni', 'bb_width_vni', 'bb_position_vni', 'rsi_overbought_vni', 'rsi_oversold_vni', 'rsi_neutral_vni', 'momentum_5_vni', 'momentum_10_vni','log_return_vni','volatility_5d_vni','volatility_10d_vni','volatility_20d_vni','volatility_30d_vni','mean_log_return_5d_vni','mean_log_return_10d_vni','mean_log_return_20d_vni','mean_log_return_30d_vni','sharpe_like_5d_vni','sharpe_like_10d_vni','sharpe_like_20d_vni','sharpe_like_30d_vni','up_streak_vni','pos_log_return_ratio_20d_vni','z_score_5d_vni','z_score_10d_vni','z_score_20d_vni','z_score_30d_vni','annual_return_vni','daily_return_vni','sharpe_ratio_vni']\n",
    "fa_features = ['p/b_previous_quarter', 'p/b_change_rate','p/b_change_rate_flag','p/e_previous_quarter','p/e_change_rate','p/e_change_rate_flag','p/s_previous_quarter','p/s_change_rate','p/s_change_rate_flag','p/cash_flow_previous_quarter','p/cash_flow_change_rate','p/cash_flow_change_rate_flag','eps_previous_quarter','eps_change_rate', 'eps_change_rate_flag','bvps_previous_quarter','bvps_change_rate', 'bvps_change_rate_flag','roe_previous_quarter','roe_change_rate','roe_change_rate_flag','roa_previous_quarter','roa_change_rate','roa_change_rate_flag','coefficient_p/b','coefficient_p/e','coefficient_p/s','coefficient_p/cash_flow','coefficient_eps','coefficient_bvps','coefficient_roe','coefficient_roa','distance_to_nearest_quarter']\n",
    "ta_fa_feature_selected = ta_features + fa_features\n",
    "features = ta_fa_feature_selected + sentiment_feature_selected\n",
    "target = 'target'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def load_data_for_analysis(tickers, folder_path):\n",
    "\n",
    "    df_all = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        file_path = os.path.join(folder_path, f\"{ticker}.xlsx\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            df[\"ticker\"] = ticker  # thêm cột ticker\n",
    "            df_all.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ Không tìm thấy: {file_path}\")\n",
    "\n",
    "    # Gộp lại thành 1 DataFrame\n",
    "    df_global = pd.concat(df_all, ignore_index=True)\n",
    "    print(f\"✅ Đã load xong {len(df_global)} dòng dữ liệu từ {len(df_all)} cổ phiếu.\")\n",
    "    return df_global\n",
    "\n",
    "tickers = ['ACB', 'BCM', 'BID','BVH','CTG','FPT','GAS','GVR','HDB','HPG',\n",
    "                    'LPB','MBB','MSN','MWG','PLX','SAB','SHB','SSB','SSI','STB',\n",
    "                    'TCB','TPB','VCB','VHM','VIB','VIC','VJC','VNM','VPB','VRE']\n",
    "folder_path_train = r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA\\TA_FA_SA\\data\\v6\\stock_trend\\train_data\" \n",
    "folder_path_test = r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA\\TA_FA_SA\\data\\v6\\stock_trend\\test_data\" \n",
    "df_global_train = load_data_for_analysis(tickers, folder_path_train)\n",
    "df_global_test = load_data_for_analysis(tickers, folder_path_test)\n",
    "\n",
    "X_train_with_sa = df_global_train[features]  # features = ta + fa + sa\n",
    "X_test_with_sa = df_global_test[features]\n",
    "y_train = df_global_train[target]\n",
    "y_test = df_global_test[target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_with_sa_scaled = scaler.fit_transform(X_train_with_sa)   # Fit scaler trên train, scale train luôn\n",
    "X_test_with_sa_scaled = scaler.transform(X_test_with_sa)\n",
    "xgb_model = XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=42, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='mlogloss',\n",
    "        n_estimators = 1089,\n",
    "        max_depth = 8, \n",
    "        learning_rate = 0.010501883895575981, \n",
    "        subsample = 0.9005472962597326, \n",
    "        colsample_bytree = 0.22675172202304014\n",
    "    )\n",
    "xgb_model.fit(X_train_with_sa_scaled, y_train)\n",
    "y_pred = xgb_model.predict(X_test_with_sa_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4187f924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\thacsi\\\\TAILIEULUANVAN\\\\code\\\\PredictStock_TA_FA_SA - Copy\\\\TA_FA_SA\\\\code\\\\v5\\\\stock_trend\\\\model_scaler\\\\scaler.pkl']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Lưu model\n",
    "joblib.dump(xgb_model, r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\xgb_model.pkl\")\n",
    "\n",
    "# Lưu scaler\n",
    "joblib.dump(scaler, r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca4468c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán giá cổ phiếu: 2\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "loaded_model = joblib.load(r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\xgb_model.pkl\")\n",
    "\n",
    "# Load scaler\n",
    "loaded_scaler = joblib.load(r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\scaler.pkl\")\n",
    "\n",
    "# Dự đoán dữ liệu mới\n",
    "X_new = ta_fa_sa_df.tail(1)[features]\n",
    "X_new_scaled = loaded_scaler.transform(X_new)  # scale trước\n",
    "y_new_pred = loaded_model.predict(X_new_scaled)\n",
    "print(\"Dự đoán giá cổ phiếu:\", y_new_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533f065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
