{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff47cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vnstock in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.2.9.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U vnstock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb8fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ta in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ta) (1.21.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ta) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->ta) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->ta) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3f982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nhatk\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.32.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.5.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b3fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-self-attention in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.51.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-self-attention) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40e2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-tuner) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nhatk\\appdata\\roaming\\python\\python37\\site-packages (from keras-tuner) (24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-tuner) (2.31.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->keras-tuner) (2022.5.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe155f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (2022.5.18.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from attrs>=20.1.0->trio~=0.17->selenium) (6.7.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->attrs>=20.1.0->trio~=0.17->selenium) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b9a113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ‘‹ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i Vnstock!\n",
       "\n",
       "Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng package phÃ¢n tÃ­ch chá»©ng khoÃ¡n #1 táº¡i Viá»‡t Nam\n",
       "\n",
       "* TÃ i liá»‡u: [Sá»• tay hÆ°á»›ng dáº«n](https://vnstocks.com/docs/category/s%E1%BB%95-tay-h%C6%B0%E1%BB%9Bng-d%E1%BA%ABn)\n",
       "* Cá»™ng Ä‘á»“ng: [NhÃ³m Facebook](https://www.facebook.com/groups/vnstock.official)\n",
       "\n",
       "KhÃ¡m phÃ¡ cÃ¡c tÃ­nh nÄƒng má»›i nháº¥t vÃ  tham gia cá»™ng Ä‘á»“ng Ä‘á»ƒ nháº­n há»— trá»£.\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "PhiÃªn báº£n Vnai 2.1.9 Ä‘Ã£ cÃ³ máº·t, vui lÃ²ng cáº­p nháº­t vá»›i cÃ¢u lá»‡nh : `pip install vnai --upgrade`.\n",
       "Lá»‹ch sá»­ phiÃªn báº£n: https://pypi.org/project/vnai/#history\n",
       "PhiÃªn báº£n hiá»‡n táº¡i 2.0.4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ta.momentum import RSIIndicator\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, RNN, concatenate\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from vnstock import *\n",
    "from ta.volatility import BollingerBands\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, Flatten, BatchNormalization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from ta.momentum import StochasticOscillator\n",
    "from ta.volume import OnBalanceVolumeIndicator\n",
    "from ta.trend import CCIIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ta.volume import ChaikinMoneyFlowIndicator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras import regularizers\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2aea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "548b71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f21814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_previous_and_last_year_quarter(input_date):\n",
    "    # Láº¥y thÃ¡ng vÃ  nÄƒm tá»« input_date\n",
    "    month = input_date.month\n",
    "    year = input_date.year\n",
    "\n",
    "    # XÃ¡c Ä‘á»‹nh quÃ½ hiá»‡n táº¡i dá»±a vÃ o thÃ¡ng\n",
    "    if 1 <= month <= 3:\n",
    "        current_quarter = 1\n",
    "    elif 4 <= month <= 6:\n",
    "        current_quarter = 2\n",
    "    elif 7 <= month <= 9:\n",
    "        current_quarter = 3\n",
    "    else:\n",
    "        current_quarter = 4\n",
    "\n",
    "    # TÃ¬m quÃ½ trÆ°á»›c Ä‘Ã³\n",
    "    previous_quarter = current_quarter - 1\n",
    "    previous_year = year\n",
    "\n",
    "    # Náº¿u quÃ½ trÆ°á»›c lÃ  0, thÃ¬ sáº½ chuyá»ƒn vá» quÃ½ 4 cá»§a nÄƒm trÆ°á»›c\n",
    "    if previous_quarter == 0:\n",
    "        previous_quarter = 4\n",
    "        previous_year -= 1\n",
    "\n",
    "    # XÃ¡c Ä‘á»‹nh quÃ½ cÃ¹ng ká»³ nÄƒm trÆ°á»›c\n",
    "    last_year = previous_year - 1\n",
    "\n",
    "    # Format káº¿t quáº£\n",
    "    previous_quarter_str = f'Q{previous_quarter}-{previous_year}'\n",
    "    last_year_quarter_str = f'Q{previous_quarter}-{last_year}'\n",
    "\n",
    "    # return [previous_quarter_str, last_year_quarter_str]\n",
    "    return previous_quarter, previous_year, last_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16fd5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2023 2022\n"
     ]
    }
   ],
   "source": [
    "# VÃ­ dá»¥ sá»­ dá»¥ng\n",
    "previous_quarter, previous_year, last_year = get_previous_and_last_year_quarter(datetime.date(2024, 1, 10)) #This was changed from datetime.date to datetime.date\n",
    "print(previous_quarter, previous_year, last_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc0afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_quarters(input_date, n=8):\n",
    "    \"\"\"\n",
    "    Tráº£ vá» danh sÃ¡ch [(quÃ½, nÄƒm)] cá»§a n quÃ½ Gáº¦N NHáº¤T, khÃ´ng bao gá»“m quÃ½ hiá»‡n táº¡i.\n",
    "    \"\"\"\n",
    "    quarters = []\n",
    "    current_date = pd.to_datetime(input_date)\n",
    "\n",
    "    # Báº¯t Ä‘áº§u tá»« quÃ½ TRÆ¯á»šC cá»§a ngÃ y input\n",
    "    current_date -= relativedelta(months=3)\n",
    "\n",
    "    for _ in range(n):\n",
    "        quarter = (current_date.month - 1) // 3 + 1\n",
    "        year = current_date.year\n",
    "        quarters.append((quarter, year))\n",
    "        current_date -= relativedelta(months=3)\n",
    "\n",
    "    return quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "044dd64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 2024), (2, 2024), (1, 2024), (4, 2023), (3, 2023), (2, 2023), (1, 2023), (4, 2022)]\n"
     ]
    }
   ],
   "source": [
    "# VÃ­ dá»¥ sá»­ dá»¥ng\n",
    "quarter = get_last_n_quarters(datetime.date(2024, 10, 10), n=8)\n",
    "print(quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab35f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_quarter_end_date(input_date):\n",
    "    \"\"\"\n",
    "    Tráº£ vá» ngÃ y káº¿t thÃºc cá»§a quÃ½ gáº§n nháº¥t TRÆ¯á»šC input_date\n",
    "    \"\"\"\n",
    "    input_date = pd.to_datetime(input_date)\n",
    "    year = input_date.year\n",
    "    month = input_date.month\n",
    "\n",
    "    if month <= 3:\n",
    "        return pd.Timestamp(year=year-1, month=12, day=31)\n",
    "    elif month <= 6:\n",
    "        return pd.Timestamp(year=year, month=3, day=31)\n",
    "    elif month <= 9:\n",
    "        return pd.Timestamp(year=year, month=6, day=30)\n",
    "    else:\n",
    "        return pd.Timestamp(year=year, month=9, day=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8823b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "latest_quarter_end_date = get_latest_quarter_end_date(datetime.date(2024, 1, 10))\n",
    "print(latest_quarter_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed1be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HÃ m thu tháº­p dá»¯ liá»‡u phÃ¢n tÃ­ch cá»Ÿ báº£n vÃ  phÃ¢n tÃ­ch ká»¹ thuáº­t theo mÃ£ cá»• phiáº¿u vÃ  ngÃ y giao dá»‹ch.\n",
    "def crawl_stock_data_FA_TA(symbol, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Crawl dá»¯ liá»‡u lá»‹ch sá»­ giÃ¡ cá»• phiáº¿u cho danh sÃ¡ch mÃ£, lÆ°u má»—i mÃ£ vÃ o 1 file Excel riÃªng.\n",
    "\n",
    "    Parameters:\n",
    "        symbols (list): Danh sÃ¡ch mÃ£ cá»• phiáº¿u (vÃ­ dá»¥: ['VCB', 'FPT', 'MWG'])\n",
    "        start_date (str): NgÃ y báº¯t Ä‘áº§u (vÃ­ dá»¥: '2020-01-01')\n",
    "        end_date (str): NgÃ y káº¿t thÃºc (vÃ­ dá»¥: '2025-01-01')\n",
    "        save_folder (str): TÃªn thÆ° má»¥c Ä‘á»ƒ lÆ°u file Excel (máº·c Ä‘á»‹nh lÃ  'stock_data')\n",
    "    \"\"\"\n",
    "\n",
    "    # Táº¡o thÆ° má»¥c lÆ°u file náº¿u chÆ°a tá»“n táº¡i\n",
    "    # os.makedirs(save_folder, exist_ok=True)\n",
    "    indicators = {\n",
    "    'P/B': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'P/B'),\n",
    "    'P/E': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'P/E'),\n",
    "    'P/S': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'P/S'),\n",
    "    'P/Cash Flow': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'P/Cash Flow'),\n",
    "    'EPS (VND)': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'EPS (VND)'),\n",
    "    'BVPS (VND)': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'BVPS (VND)'),\n",
    "    'EV/EBITDA': ('Chá»‰ tiÃªu Ä‘á»‹nh giÃ¡', 'EV/EBITDA'),\n",
    "    'ROE (%)': ('Chá»‰ tiÃªu kháº£ nÄƒng sinh lá»£i', 'ROE (%)'),\n",
    "    'ROIC (%)': ('Chá»‰ tiÃªu kháº£ nÄƒng sinh lá»£i', 'ROIC (%)'),\n",
    "    'ROA (%)': ('Chá»‰ tiÃªu kháº£ nÄƒng sinh lá»£i', 'ROA (%)'),\n",
    "}\n",
    "\n",
    "    # for symbol in symbols:\n",
    "    try:\n",
    "            print(f\"Äang láº¥y dá»¯ liá»‡u cho: {symbol}...\")\n",
    "            stock = Vnstock().stock(symbol=symbol)\n",
    "            df = stock.quote.history(start=start_date, end=end_date)\n",
    "            finance_data = stock.finance.ratio(period='quarter', lang='vi', dropna=True)\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                input_date = row['time']\n",
    "                prev_quarter, prev_year, year_before = get_previous_and_last_year_quarter(input_date)\n",
    "\n",
    "                for name, multi_idx in indicators.items():\n",
    "                    col_prev = f\"{name}_Previous_Quarter\"\n",
    "                    col_last_year = f\"{name}_Same_Period_Last_Year\"\n",
    "\n",
    "                    # GiÃ¡ trá»‹ quÃ½ trÆ°á»›c\n",
    "                    try:\n",
    "                        value_prev = finance_data.loc[\n",
    "                            (finance_data[('Meta', 'NÄƒm')].astype(str) == str(prev_year)) &\n",
    "                            (finance_data[('Meta', 'Ká»³')].astype(str) == str(prev_quarter)),\n",
    "                            multi_idx\n",
    "                        ].values\n",
    "                        df.at[index, col_prev] = value_prev[0] if len(value_prev) > 0 else None\n",
    "                    except:\n",
    "                        df.at[index, col_prev] = None\n",
    "\n",
    "                    # GiÃ¡ trá»‹ cÃ¹ng ká»³ nÄƒm trÆ°á»›c\n",
    "                    try:\n",
    "                        value_last_year = finance_data.loc[\n",
    "                            (finance_data[('Meta', 'NÄƒm')].astype(str) == str(year_before)) &\n",
    "                            (finance_data[('Meta', 'Ká»³')].astype(str) == str(prev_quarter)),\n",
    "                            multi_idx\n",
    "                        ].values\n",
    "                        df.at[index, col_last_year] = value_last_year[0] if len(value_last_year) > 0 else None\n",
    "                    except:\n",
    "                        df.at[index, col_last_year] = None\n",
    "\n",
    "                    # ThÃªm 8 quÃ½ gáº§n nháº¥t\n",
    "                    recent_quarters = get_last_n_quarters(input_date, n=8)\n",
    "                    for i, (q, y) in enumerate(recent_quarters):\n",
    "                        col_recent = f\"{name}_d_{i+1}\"\n",
    "                        try:\n",
    "                            value = finance_data.loc[\n",
    "                                (finance_data[('Meta', 'NÄƒm')].astype(str) == str(y)) &\n",
    "                                (finance_data[('Meta', 'Ká»³')].astype(str) == str(q)),\n",
    "                                multi_idx\n",
    "                            ].values\n",
    "                            df.at[index, col_recent] = value[0] if len(value) > 0 else 0\n",
    "                        except:\n",
    "                            df.at[index, col_recent] = 0\n",
    "                    # TÃ­nh khoáº£ng cÃ¡ch tá»« input_date Ä‘áº¿n cuá»‘i quÃ½ gáº§n nháº¥t\n",
    "                    try:\n",
    "                        end_of_last_quarter = get_latest_quarter_end_date(input_date)\n",
    "                        distance = (pd.to_datetime(input_date) - end_of_last_quarter).days\n",
    "                        df.at[index, 'distance_to_nearest_quarter'] = distance\n",
    "                    except:\n",
    "                        df.at[index, 'distance_to_nearest_quarter'] = None\n",
    "\n",
    "            df['ticket'] = symbol  # ðŸ‘‰ ThÃªm cá»™t 'ticket'\n",
    "            # file_path = os.path.join(save_folder, f\"{symbol}.xlsx\")\n",
    "            # df.to_excel(file_path, index=False)\n",
    "            print(f\"âœ… ÄÃ£ thu tháº­p dá»¯ liá»‡u PhÃ¢n tÃ­ch cÆ¡ báº£n vÃ  phÃ¢n tÃ­ch ká»¹ thuáº­t cho {symbol} ngÃ y {input_date}\")\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i khi xá»­ lÃ½ mÃ£ {symbol}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd6445e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(symbol, start_date, end_date, drop_na=True):\n",
    "    data = crawl_stock_data_FA_TA(symbol, start_date, end_date)\n",
    "    if data.empty:\n",
    "        return None\n",
    "\n",
    "    df = data\n",
    "\n",
    "    columns_to_drop = ['EV/EBITDA_Previous_Quarter','EV/EBITDA_Same_Period_Last_Year','ROIC (%)_Previous_Quarter','ROIC (%)_Same_Period_Last_Year']  # thay báº±ng danh sÃ¡ch cá»™t báº¡n muá»‘n xÃ³a\n",
    "    # doi ten o day\n",
    "    \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "     # TÃ­nh toÃ¡n khá»‘i lÆ°á»£ng trung bÃ¬nh\n",
    "    volume_ma_period = 20  # VÃ­ dá»¥: khá»‘i lÆ°á»£ng trung bÃ¬nh 20 ngÃ y\n",
    "    df['volume_ma'] = df['volume'].rolling(window=volume_ma_period).mean()\n",
    "\n",
    "\n",
    "    # TÃ­nh tá»· lá»‡ khá»‘i lÆ°á»£ng hiá»‡n táº¡i / khá»‘i lÆ°á»£ng trung bÃ¬nh\n",
    "    df['volume_to_volume_ma_ratio'] = df['volume'] / df['volume_ma']\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n EMA ngáº¯n háº¡n (12 ngÃ y)\n",
    "    df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n EMA dÃ i háº¡n (26 ngÃ y)\n",
    "    df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n SMA trung háº¡n (20 ngÃ y)\n",
    "    df['sma_20'] = df['close'].rolling(window=20).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n SMA dÃ i háº¡n (50 ngÃ y)\n",
    "    df['sma_50'] = df['close'].rolling(window=50).mean()\n",
    "\n",
    "     # TÃ­nh toÃ¡n ROC\n",
    "\n",
    "    df['roc_1'] = ((df['close'] - df['close'].shift(1)) / df['close'].shift(1)) * 100\n",
    "    df['roc_5'] = ((df['close'] - df['close'].shift(5)) / df['close'].shift(5)) * 100\n",
    "    df['roc_9'] = ((df['close'] - df['close'].shift(9)) / df['close'].shift(9)) * 100\n",
    "\n",
    "    # TÃ­nh toÃ¡n %K (Stochastic Oscillator)\n",
    "    stoch_period = 14\n",
    "    df['%K'] = ((df['close'] - df['low'].rolling(window=stoch_period).min()) /\n",
    "                (df['high'].rolling(window=stoch_period).max() - df['low'].rolling(window=stoch_period).min())) * 100\n",
    "\n",
    "    # TÃ­nh toÃ¡n %R (Williams %R)\n",
    "    df['%R'] = ((df['high'].rolling(window=stoch_period).max() - df['close']) /\n",
    "                (df['high'].rolling(window=stoch_period).max() - df['low'].rolling(window=stoch_period).min())) * -100\n",
    "\n",
    "    # TÃ­nh toÃ¡n CCI (Commodity Channel Index)\n",
    "    cci_period = 20\n",
    "    df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3\n",
    "    df['cci'] = CCIIndicator(high=df['high'], low=df['low'], close=df['close'], window=cci_period).cci()\n",
    "\n",
    "    # TÃ­nh toÃ¡n OBV (On Balance Volume)\n",
    "    df['obv'] = OnBalanceVolumeIndicator(close=df['close'], volume=df['volume']).on_balance_volume()\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n MACD\n",
    "    df['macd'] = df['ema_12'] - df['ema_26']\n",
    "    # TÃ­nh toÃ¡n Signal line (EMA 9 ngÃ y cá»§a MACD)\n",
    "    df['signal_line'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    # TÃ­nh toÃ¡n MACD histogram\n",
    "    df['macd_histogram'] = df['macd'] - df['signal_line']\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n RSI\n",
    "    rsi_period = 14\n",
    "    df['rsi'] = RSIIndicator(df['close'], window=rsi_period).rsi()\n",
    "    ma_period = 9\n",
    "    df['rsi_base_ma'] = df['rsi'].rolling(window=ma_period).mean()\n",
    "    df['rsi_rsi_base_ma_ratio'] = df['rsi'] / df['rsi_base_ma']\n",
    "\n",
    "\n",
    "    # Bollinger Bands\n",
    "    indicator_bb = BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "    df['bb_bbm'] = indicator_bb.bollinger_mavg()\n",
    "    df['bb_bbh'] = indicator_bb.bollinger_hband()\n",
    "    df['bb_bbl'] = indicator_bb.bollinger_lband()\n",
    "    df['bb_bbp'] = indicator_bb.bollinger_pband()\n",
    "\n",
    "    df['bb_bbh_bb_bbl_ratio'] = df['bb_bbh'] / df['bb_bbl']\n",
    "\n",
    "    df['hl_ratio'] = df['high'] / (df['low'] + 1e-8)\n",
    "    df['co_ratio'] = df['close'] / (df['open'] + 1e-8)\n",
    "    df['price_range'] = (df['high'] - df['low']) / (df['close'] + 1e-8)\n",
    "\n",
    "    if 'sma_20' in df.columns and 'sma_50' in df.columns:\n",
    "        df['sma_ratio_20_50'] = df['sma_20'] / (df['sma_50'] + 1e-8)\n",
    "    if 'ema_12' in df.columns and 'ema_26' in df.columns:\n",
    "        df['ema_ratio_12_26'] = df['ema_12'] / (df['ema_26'] + 1e-8)\n",
    "\n",
    "    if all(col in df.columns for col in ['bb_bbh', 'bb_bbl', 'bb_bbm']):\n",
    "        df['bb_width'] = (df['bb_bbh'] - df['bb_bbl']) / (df['bb_bbm'] + 1e-8)\n",
    "        df['bb_position'] = (df['close'] - df['bb_bbl']) / (df['bb_bbh'] - df['bb_bbl'] + 1e-8)\n",
    "\n",
    "    if 'rsi' in df.columns:\n",
    "        df['rsi_overbought'] = (df['rsi'] > 70).astype(int)\n",
    "        df['rsi_oversold'] = (df['rsi'] < 30).astype(int)\n",
    "        df['rsi_neutral'] = ((df['rsi'] >= 30) & (df['rsi'] <= 70)).astype(int)\n",
    "\n",
    "    if all(col in df.columns for col in ['macd', 'signal_line']):\n",
    "        df['macd_signal_diff'] = df['macd'] - df['signal_line']\n",
    "        df['macd_bullish'] = (df['macd'] > df['signal_line']).astype(int)\n",
    "\n",
    "    for period in [5, 10]:\n",
    "        if len(df) > period:\n",
    "            df[f'momentum_{period}'] = df['close'].pct_change(period)\n",
    "\n",
    "    # =============================================================\n",
    "\n",
    "    # =======================================================================\n",
    "\n",
    "    # 1. Daily log return\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "    # 2. Rolling volatility (biáº¿n Ä‘á»™ng trÆ°á»£t)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        df[f'volatility_{window}d'] = df['log_return'].rolling(window=window).std()\n",
    "\n",
    "    # 3. Rolling mean log return\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        df[f'mean_log_return_{window}d'] = df['log_return'].rolling(window=window).mean()\n",
    "\n",
    "    # 4. Sharpe-like ratio (mean return / volatility)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col = f'mean_log_return_{window}d'\n",
    "        vol_col = f'volatility_{window}d'\n",
    "        df[f'sharpe_like_{window}d'] = df[mean_col] / (df[vol_col] + 1e-9)\n",
    "\n",
    "    # 5. Directional Feature - chuá»—i ngÃ y tÄƒng liÃªn tiáº¿p\n",
    "    df['up_streak'] = (df['log_return'] > 0).astype(int)\n",
    "    df['up_streak'] = df['up_streak'] * (df['up_streak'].groupby((df['up_streak'] != df['up_streak'].shift()).cumsum()).cumcount() + 1)\n",
    "    df['up_streak'] = df['up_streak'].where(df['log_return'] > 0, 0)\n",
    "\n",
    "    # 6. Tá»· lá»‡ ngÃ y log return dÆ°Æ¡ng trong 20 ngÃ y gáº§n nháº¥t\n",
    "    df['pos_log_return_ratio_20d'] = df['log_return'].rolling(window=20).apply(lambda x: np.mean(x > 0), raw=True)\n",
    "\n",
    "    # 7. Z-score cá»§a log return hiá»‡n táº¡i\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col = f'mean_log_return_{window}d'\n",
    "        vol_col = f'volatility_{window}d'\n",
    "        df[f'z_score_{window}d'] = (df['log_return'] - df[mean_col]) / (df[vol_col] + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "    # TÃ­nh Annual Return (lá»£i nhuáº­n hÃ ng nÄƒm)\n",
    "    df['annual_return'] = df['close'] / df['close'].shift(252) - 1  # 252 ngÃ y giao dá»‹ch trong nÄƒm\n",
    "\n",
    "    # TÃ­nh Volatility (Äá»™ biáº¿n Ä‘á»™ng) hÃ ng nÄƒm (Standard Deviation of Returns)\n",
    "    df['daily_return'] = df['close'].pct_change()  # Tá»· suáº¥t lá»£i nhuáº­n hÃ ng ngÃ y\n",
    "    df['annual_volatility'] = df['daily_return'].rolling(window=252).std() * (252 ** 0.5)  # Äá»™ lá»‡ch chuáº©n hÃ ng nÄƒm (tÃ­nh tá»« tá»· suáº¥t lá»£i nhuáº­n hÃ ng ngÃ y)\n",
    "\n",
    "    risk_free_rate = 0.03  # tÆ°Æ¡ng Ä‘Æ°Æ¡ng 3%/nÄƒm\n",
    "    # TÃ­nh Sharpe ratio: (Rp - Rf) / Volatility\n",
    "    df['sharpe_ratio'] = (df['annual_return'] - risk_free_rate) / df['annual_volatility']\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n chá»‰ sá»‘ cÆ¡ báº£n ==========================================================\n",
    "    df['P/B_change_rate_flag'] = np.where(df['P/B_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/B_change_rate'] = np.where(\n",
    "        df['P/B_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/B_Previous_Quarter'] - df['P/B_Same_Period_Last_Year']) / df['P/B_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # P/E\n",
    "    df['P/E_change_rate_flag'] = np.where(df['P/E_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/E_change_rate'] = np.where(\n",
    "        df['P/E_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/E_Previous_Quarter'] - df['P/E_Same_Period_Last_Year']) / df['P/E_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # P/S\n",
    "    df['P/S_change_rate_flag'] = np.where(df['P/S_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/S_change_rate'] = np.where(\n",
    "        df['P/S_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/S_Previous_Quarter'] - df['P/S_Same_Period_Last_Year']) / df['P/S_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # P/Cash Flow\n",
    "    df['P/Cash Flow_change_rate_flag'] = np.where(df['P/Cash Flow_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['P/Cash Flow_change_rate'] = np.where(\n",
    "        df['P/Cash Flow_Same_Period_Last_Year'] != 0,\n",
    "        (df['P/Cash Flow_Previous_Quarter'] - df['P/Cash Flow_Same_Period_Last_Year']) / df['P/Cash Flow_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # EPS (VND)\n",
    "    df['EPS (VND)_change_rate_flag'] = np.where(df['EPS (VND)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['EPS (VND)_change_rate'] = np.where(\n",
    "        df['EPS (VND)_Same_Period_Last_Year'] != 0,\n",
    "        (df['EPS (VND)_Previous_Quarter'] - df['EPS (VND)_Same_Period_Last_Year']) / df['EPS (VND)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # BVPS (VND)\n",
    "    df['BVPS (VND)_change_rate_flag'] = np.where(df['BVPS (VND)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['BVPS (VND)_change_rate'] = np.where(\n",
    "        df['BVPS (VND)_Same_Period_Last_Year'] != 0,\n",
    "        (df['BVPS (VND)_Previous_Quarter'] - df['BVPS (VND)_Same_Period_Last_Year']) / df['BVPS (VND)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # ROE (%)\n",
    "    df['ROE (%)_change_rate_flag'] = np.where(df['ROE (%)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['ROE (%)_change_rate'] = np.where(\n",
    "        df['ROE (%)_Same_Period_Last_Year'] != 0,\n",
    "        (df['ROE (%)_Previous_Quarter'] - df['ROE (%)_Same_Period_Last_Year']) / df['ROE (%)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # ROA (%)\n",
    "    df['ROA (%)_change_rate_flag'] = np.where(df['ROA (%)_Same_Period_Last_Year'] == 0, 1, 0)\n",
    "    df['ROA (%)_change_rate'] = np.where(\n",
    "        df['ROA (%)_Same_Period_Last_Year'] != 0,\n",
    "        (df['ROA (%)_Previous_Quarter'] - df['ROA (%)_Same_Period_Last_Year']) / df['ROA (%)_Same_Period_Last_Year'],\n",
    "        999\n",
    "    )\n",
    "\n",
    "    # ==================================================================================================\n",
    "\n",
    "\n",
    "    # Xá»© lÃ½ vá»›i VNINDEX =============================================================================\n",
    "    data_time = data['time']\n",
    "    stock = Vnstock().stock(symbol=\"VNINDEX\")\n",
    "    data_vni = stock.quote.history(start=start_date, end=end_date)\n",
    "    data_vni.columns = ['time','open_vni', 'high_vni', 'low_vni', 'close_vni', 'volume_vni']\n",
    "\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n RSI\n",
    "    rsi_period = 14\n",
    "    data_vni['rsi_vni'] = RSIIndicator(data_vni['close_vni'], window=rsi_period).rsi()\n",
    "    # TÃ­nh toÃ¡n RSI-base-MA\n",
    "    ma_period = 9\n",
    "    data_vni['rsi_base_ma_vni'] = data_vni['rsi_vni'].rolling(window=ma_period).mean()\n",
    "    data_vni['rsi_rsi_base_ma_ratio_vni'] = data_vni['rsi_vni'] / data_vni['rsi_base_ma_vni']\n",
    "\n",
    "    # TÃ­nh toÃ¡n khá»‘i lÆ°á»£ng trung bÃ¬nh\n",
    "    volume_ma_period = 20  # VÃ­ dá»¥: khá»‘i lÆ°á»£ng trung bÃ¬nh 20 ngÃ y\n",
    "    data_vni['volume_ma_vni'] = data_vni['volume_vni'].rolling(window=volume_ma_period).mean()\n",
    "    # TÃ­nh tá»· lá»‡ khá»‘i lÆ°á»£ng hiá»‡n táº¡i / khá»‘i lÆ°á»£ng trung bÃ¬nh\n",
    "    data_vni['volume_to_volume_ma_ratio_vni'] = data_vni['volume_vni'] / data_vni['volume_ma_vni']\n",
    "\n",
    "    # Bollinger Bands\n",
    "    indicator_bb = BollingerBands(close=data_vni['close_vni'], window=20, window_dev=2)\n",
    "    data_vni['bb_bbm_vni'] = indicator_bb.bollinger_mavg()\n",
    "    data_vni['bb_bbh_vni'] = indicator_bb.bollinger_hband()\n",
    "    data_vni['bb_bbl_vni'] = indicator_bb.bollinger_lband()\n",
    "    data_vni['bb_bbp_vni'] = indicator_bb.bollinger_pband()\n",
    "\n",
    "    data_vni['bb_bbh_bb_bbl_ratio_vni'] = data_vni['bb_bbh_vni'] / data_vni['bb_bbl_vni']\n",
    "\n",
    "    \n",
    "\n",
    "    # TÃ­nh toÃ¡n ROC\n",
    "    roc_period = 9  # Chu ká»³ 9 ngÃ y, báº¡n cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y Ã½\n",
    "    # data_vni['roc_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(roc_period)) / data_vni['close_vni'].shift(roc_period)) * 100\n",
    "    data_vni['roc_1_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(1)) / data_vni['close_vni'].shift(1)) * 100\n",
    "    data_vni['roc_5_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(5)) / data_vni['close_vni'].shift(5)) * 100\n",
    "    data_vni['roc_9_vni'] = ((data_vni['close_vni'] - data_vni['close_vni'].shift(9)) / data_vni['close_vni'].shift(9)) * 100\n",
    "    # TÃ­nh toÃ¡n %K (Stochastic Oscillator)\n",
    "    stoch_period = 14\n",
    "    data_vni['%K_vni'] = ((data_vni['close_vni'] - data_vni['low_vni'].rolling(window=stoch_period).min()) /\n",
    "                (data_vni['high_vni'].rolling(window=stoch_period).max() - data_vni['low_vni'].rolling(window=stoch_period).min())) * 100\n",
    "\n",
    "    # TÃ­nh toÃ¡n %R (Williams %R)\n",
    "    data_vni['%R_vni'] = ((data_vni['high_vni'].rolling(window=stoch_period).max() - data_vni['close_vni']) /\n",
    "                (data_vni['high_vni'].rolling(window=stoch_period).max() - data_vni['low_vni'].rolling(window=stoch_period).min())) * -100\n",
    "\n",
    "    # TÃ­nh toÃ¡n CCI (Commodity Channel Index)\n",
    "    cci_period = 20\n",
    "    data_vni['typical_price_vni'] = (data_vni['high_vni'] + data_vni['low_vni'] + data_vni['close_vni']) / 3\n",
    "    data_vni['cci_vni'] = CCIIndicator(high=data_vni['high_vni'], low=data_vni['low_vni'], close=data_vni['close_vni'], window=cci_period).cci()\n",
    "\n",
    "    # TÃ­nh toÃ¡n OBV (On Balance Volume)\n",
    "    data_vni['obv_vni'] = OnBalanceVolumeIndicator(close=data_vni['close_vni'], volume=data_vni['volume_vni']).on_balance_volume()\n",
    "\n",
    "    # TÃ­nh toÃ¡n EMA ngáº¯n háº¡n (12 ngÃ y)\n",
    "    data_vni['ema_12_vni'] = data_vni['close_vni'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n EMA dÃ i háº¡n (26 ngÃ y)\n",
    "    data_vni['ema_26_vni'] = data_vni['close_vni'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n SMA trung háº¡n (20 ngÃ y)\n",
    "    data_vni['sma_20_vni'] = data_vni['close_vni'].rolling(window=20).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n SMA dÃ i háº¡n (50 ngÃ y)\n",
    "    data_vni['sma_50_vni'] = data_vni['close_vni'].rolling(window=50).mean()\n",
    "\n",
    "    data_vni['hl_ratio_vni'] = data_vni['high_vni'] / (data_vni['low_vni'] + 1e-8)\n",
    "    data_vni['co_ratio_vni'] = data_vni['close_vni'] / (data_vni['open_vni'] + 1e-8)\n",
    "    data_vni['price_range_vni'] = (data_vni['high_vni'] - data_vni['low_vni']) / (data_vni['close_vni'] + 1e-8)\n",
    "\n",
    "    if 'sma_20_vni' in data_vni.columns and 'sma_50_vni' in data_vni.columns:\n",
    "        data_vni['sma_ratio_20_50_vni'] = data_vni['sma_20_vni'] / (data_vni['sma_50_vni'] + 1e-8)\n",
    "    if 'ema_12_vni' in data_vni.columns and 'ema_26_vni' in data_vni.columns:\n",
    "        data_vni['ema_ratio_12_26_vni'] = data_vni['ema_12_vni'] / (data_vni['ema_26_vni'] + 1e-8)\n",
    "\n",
    "    if all(col in data_vni.columns for col in ['bb_bbh_vni', 'bb_bbl_vni', 'bb_bbm_vni']):\n",
    "        data_vni['bb_width_vni'] = (data_vni['bb_bbh_vni'] - data_vni['bb_bbl_vni']) / (data_vni['bb_bbm_vni'] + 1e-8)\n",
    "        data_vni['bb_position_vni'] = (data_vni['close_vni'] - data_vni['bb_bbl_vni']) / (data_vni['bb_bbh_vni'] - data_vni['bb_bbl_vni'] + 1e-8)\n",
    "\n",
    "    if 'rsi_vni' in data_vni.columns:\n",
    "        data_vni['rsi_overbought_vni'] = (data_vni['rsi_vni'] > 70).astype(int)\n",
    "        data_vni['rsi_oversold_vni'] = (data_vni['rsi_vni'] < 30).astype(int)\n",
    "        data_vni['rsi_neutral_vni'] = ((data_vni['rsi_vni'] >= 30) & (data_vni['rsi_vni'] <= 70)).astype(int)\n",
    "\n",
    "    if all(col in data_vni.columns for col in ['macd_vni', 'signal_line_vni']):\n",
    "        data_vni['macd_signal_diff_vni'] = data_vni['macd_vni'] - data_vni['signal_line_vni']\n",
    "        data_vni['macd_bullish_vni'] = (data_vni['macd_vni'] > data_vni['signal_line_vni']).astype(int)\n",
    "\n",
    "    for period in [5, 10]:\n",
    "        if len(data_vni) > period:\n",
    "            df[f'momentum_{period}_vni'] = data_vni['close_vni'].pct_change(period)\n",
    "\n",
    "    \n",
    "    # 1. Daily log return\n",
    "    data_vni['log_return_vni'] = np.log(data_vni['close_vni'] / data_vni['close_vni'].shift(1))\n",
    "\n",
    "    # 2. Rolling volatility (biáº¿n Ä‘á»™ng trÆ°á»£t)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vni[f'volatility_{window}d_vni'] = data_vni['log_return_vni'].rolling(window=window).std()\n",
    "\n",
    "    # 3. Rolling mean log return\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vni[f'mean_log_return_{window}d_vni'] = data_vni['log_return_vni'].rolling(window=window).mean()\n",
    "\n",
    "    # 4. Sharpe-like ratio (mean return / volatility)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vni = f'mean_log_return_{window}d_vni'\n",
    "        vol_col_vni = f'volatility_{window}d_vni'\n",
    "        data_vni[f'sharpe_like_{window}d_vni'] = data_vni[mean_col_vni] / (data_vni[vol_col_vni] + 1e-9)\n",
    "\n",
    "    # 5. Directional Feature - chuá»—i ngÃ y tÄƒng liÃªn tiáº¿p\n",
    "    data_vni['up_streak_vni'] = (data_vni['log_return_vni'] > 0).astype(int)\n",
    "    data_vni['up_streak_vni'] = data_vni['up_streak_vni'] * (data_vni['up_streak_vni'].groupby((data_vni['up_streak_vni'] != data_vni['up_streak_vni'].shift()).cumsum()).cumcount() + 1)\n",
    "    data_vni['up_streak_vni'] = data_vni['up_streak_vni'].where(data_vni['log_return_vni'] > 0, 0)\n",
    "\n",
    "    # 6. Tá»· lá»‡ ngÃ y log return dÆ°Æ¡ng trong 20 ngÃ y gáº§n nháº¥t\n",
    "    data_vni['pos_log_return_ratio_20d_vni'] = data_vni['log_return_vni'].rolling(window=20).apply(lambda x: np.mean(x > 0), raw=True)\n",
    "\n",
    "    # 7. Z-score cá»§a log return hiá»‡n táº¡i\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vni = f'mean_log_return_{window}d_vni'\n",
    "        vol_col_vni = f'volatility_{window}d_vni'\n",
    "        data_vni[f'z_score_{window}d_vni'] = (data_vni['log_return_vni'] - data_vni[mean_col_vni]) / (data_vni[vol_col_vni] + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "    # TÃ­nh Annual Return (lá»£i nhuáº­n hÃ ng nÄƒm)\n",
    "    data_vni['annual_return_vni'] = data_vni['close_vni'] / data_vni['close_vni'].shift(252) - 1  # 252 ngÃ y giao dá»‹ch trong nÄƒm\n",
    "\n",
    "    # TÃ­nh Volatility (Äá»™ biáº¿n Ä‘á»™ng) hÃ ng nÄƒm (Standard Deviation of Returns)\n",
    "    data_vni['daily_return_vni'] = data_vni['close_vni'].pct_change()  # Tá»· suáº¥t lá»£i nhuáº­n hÃ ng ngÃ y\n",
    "    data_vni['annual_volatility_vni'] = data_vni['daily_return_vni'].rolling(window=252).std() * (252 ** 0.5)  # Äá»™ lá»‡ch chuáº©n hÃ ng nÄƒm (tÃ­nh tá»« tá»· suáº¥t lá»£i nhuáº­n hÃ ng ngÃ y)\n",
    "\n",
    "    risk_free_rate = 0.03  # tÆ°Æ¡ng Ä‘Æ°Æ¡ng 3%/nÄƒm\n",
    "    # TÃ­nh Sharpe ratio: (Rp - Rf) / Volatility\n",
    "    data_vni['sharpe_ratio_vni'] = (data_vni['annual_return_vni'] - risk_free_rate) / data_vni['annual_volatility_vni']\n",
    "\n",
    "\n",
    "    data_vni['time'] = pd.to_datetime(data_vni['time']).dt.date\n",
    "\n",
    "\n",
    "    # Xá»© lÃ½ vá»›i VN30 =============================================================================\n",
    "    stock = Vnstock().stock(symbol=\"VN30\")\n",
    "    data_vn30 = stock.quote.history(start=start_date, end=end_date)\n",
    "    data_vn30.columns = ['time','open_vn30', 'high_vn30', 'low_vn30', 'close_vn30', 'volume_vn30']\n",
    "\n",
    "\n",
    "    # TÃ­nh toÃ¡n RSI\n",
    "    rsi_period = 14\n",
    "    data_vn30['rsi_vn30'] = RSIIndicator(data_vn30['close_vn30'], window=rsi_period).rsi()\n",
    "    # TÃ­nh toÃ¡n RSI-base-MA\n",
    "    ma_period = 9\n",
    "    data_vn30['rsi_base_ma_vn30'] = data_vn30['rsi_vn30'].rolling(window=ma_period).mean()\n",
    "    data_vn30['rsi_rsi_base_ma_ratio_vn30'] = data_vn30['rsi_vn30'] / data_vn30['rsi_base_ma_vn30']\n",
    "\n",
    "    # TÃ­nh toÃ¡n khá»‘i lÆ°á»£ng trung bÃ¬nh\n",
    "    volume_ma_period = 20  # VÃ­ dá»¥: khá»‘i lÆ°á»£ng trung bÃ¬nh 20 ngÃ y\n",
    "    data_vn30['volume_ma_vn30'] = data_vn30['volume_vn30'].rolling(window=volume_ma_period).mean()\n",
    "    # TÃ­nh tá»· lá»‡ khá»‘i lÆ°á»£ng hiá»‡n táº¡i / khá»‘i lÆ°á»£ng trung bÃ¬nh\n",
    "    data_vn30['volume_to_volume_ma_ratio_vn30'] = data_vn30['volume_vn30'] / data_vn30['volume_ma_vn30']\n",
    "\n",
    "    # Bollinger Bands\n",
    "    indicator_bb = BollingerBands(close=data_vn30['close_vn30'], window=20, window_dev=2)\n",
    "    data_vn30['bb_bbm_vn30'] = indicator_bb.bollinger_mavg()\n",
    "    data_vn30['bb_bbh_vn30'] = indicator_bb.bollinger_hband()\n",
    "    data_vn30['bb_bbl_vn30'] = indicator_bb.bollinger_lband()\n",
    "    data_vn30['bb_bbp_vn30'] = indicator_bb.bollinger_pband()\n",
    "\n",
    "    data_vn30['bb_bbh_bb_bbl_ratio_vn30'] = data_vn30['bb_bbh_vn30'] / data_vn30['bb_bbl_vn30']\n",
    "\n",
    "    # TÃ­nh toÃ¡n ROC\n",
    "    roc_period = 9  # Chu ká»³ 9 ngÃ y, báº¡n cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y Ã½\n",
    "    # data_vn30['roc_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(roc_period)) / data_vn30['close_vn30'].shift(roc_period)) * 100\n",
    "    data_vn30['roc_1_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(1)) / data_vn30['close_vn30'].shift(1)) * 100\n",
    "    data_vn30['roc_5_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(5)) / data_vn30['close_vn30'].shift(5)) * 100\n",
    "    data_vn30['roc_9_vn30'] = ((data_vn30['close_vn30'] - data_vn30['close_vn30'].shift(9)) / data_vn30['close_vn30'].shift(9)) * 100\n",
    "\n",
    "    # TÃ­nh toÃ¡n %K (Stochastic Oscillator)\n",
    "    stoch_period = 14\n",
    "    data_vn30['%K_vn30'] = ((data_vn30['close_vn30'] - data_vn30['low_vn30'].rolling(window=stoch_period).min()) /\n",
    "                (data_vn30['high_vn30'].rolling(window=stoch_period).max() - data_vn30['low_vn30'].rolling(window=stoch_period).min())) * 100\n",
    "\n",
    "    # TÃ­nh toÃ¡n %R (Williams %R)\n",
    "    data_vn30['%R_vn30'] = ((data_vn30['high_vn30'].rolling(window=stoch_period).max() - data_vn30['close_vn30']) /\n",
    "                (data_vn30['high_vn30'].rolling(window=stoch_period).max() - data_vn30['low_vn30'].rolling(window=stoch_period).min())) * -100\n",
    "\n",
    "    # TÃ­nh toÃ¡n CCI (Commodity Channel Index)\n",
    "    cci_period = 20\n",
    "    data_vn30['typical_price_vn30'] = (data_vn30['high_vn30'] + data_vn30['low_vn30'] + data_vn30['close_vn30']) / 3\n",
    "    data_vn30['cci_vn30'] = CCIIndicator(high=data_vn30['high_vn30'], low=data_vn30['low_vn30'], close=data_vn30['close_vn30'], window=cci_period).cci()\n",
    "\n",
    "    # TÃ­nh toÃ¡n OBV (On Balance Volume)\n",
    "    data_vn30['obv_vn30'] = OnBalanceVolumeIndicator(close=data_vn30['close_vn30'], volume=data_vn30['volume_vn30']).on_balance_volume()\n",
    "\n",
    "    # TÃ­nh toÃ¡n EMA ngáº¯n háº¡n (12 ngÃ y)\n",
    "    data_vn30['ema_12_vn30'] = data_vn30['close_vn30'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n EMA dÃ i háº¡n (26 ngÃ y)\n",
    "    data_vn30['ema_26_vn30'] = data_vn30['close_vn30'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n SMA trung háº¡n (20 ngÃ y)\n",
    "    data_vn30['sma_20_vn30'] = data_vn30['close_vn30'].rolling(window=20).mean()\n",
    "\n",
    "    # TÃ­nh toÃ¡n SMA dÃ i háº¡n (50 ngÃ y)\n",
    "    data_vn30['sma_50_vn30'] = data_vn30['close_vn30'].rolling(window=50).mean()\n",
    "\n",
    "    data_vn30['hl_ratio_vn30'] = data_vn30['high_vn30'] / (data_vn30['low_vn30'] + 1e-8)\n",
    "    data_vn30['co_ratio_vn30'] = data_vn30['close_vn30'] / (data_vn30['open_vn30'] + 1e-8)\n",
    "    data_vn30['price_range_vn30'] = (data_vn30['high_vn30'] - data_vn30['low_vn30']) / (data_vn30['close_vn30'] + 1e-8)\n",
    "\n",
    "    if 'sma_20_vn30' in data_vn30.columns and 'sma_50_vn30' in data_vn30.columns:\n",
    "        data_vn30['sma_ratio_20_50_vn30'] = data_vn30['sma_20_vn30'] / (data_vn30['sma_50_vn30'] + 1e-8)\n",
    "    if 'ema_12_vn30' in data_vn30.columns and 'ema_26_vn30' in data_vn30.columns:\n",
    "        data_vn30['ema_ratio_12_26_vn30'] = data_vn30['ema_12_vn30'] / (data_vn30['ema_26_vn30'] + 1e-8)\n",
    "\n",
    "    if all(col in data_vn30.columns for col in ['bb_bbh_vn30', 'bb_bbl_vn30', 'bb_bbm_vn30']):\n",
    "        data_vn30['bb_width_vn30'] = (data_vn30['bb_bbh_vn30'] - data_vn30['bb_bbl_vn30']) / (data_vn30['bb_bbm_vn30'] + 1e-8)\n",
    "        data_vn30['bb_position_vn30'] = (data_vn30['close_vn30'] - data_vn30['bb_bbl_vn30']) / (data_vn30['bb_bbh_vn30'] - data_vn30['bb_bbl_vn30'] + 1e-8)\n",
    "\n",
    "    if 'rsi_vn30' in data_vn30.columns:\n",
    "        data_vn30['rsi_overbought_vn30'] = (data_vn30['rsi_vn30'] > 70).astype(int)\n",
    "        data_vn30['rsi_oversold_vn30'] = (data_vn30['rsi_vn30'] < 30).astype(int)\n",
    "        data_vn30['rsi_neutral_vn30'] = ((data_vn30['rsi_vn30'] >= 30) & (data_vn30['rsi_vn30'] <= 70)).astype(int)\n",
    "\n",
    "    if all(col in data_vn30.columns for col in ['macd_vn30', 'signal_line_vn30']):\n",
    "        data_vn30['macd_signal_diff_vn30'] = data_vn30['macd_vn30'] - data_vn30['signal_line_vn30']\n",
    "        data_vn30['macd_bullish_vn30'] = (data_vn30['macd_vn30'] > data_vn30['signal_line_vn30']).astype(int)\n",
    "\n",
    "    for period in [5, 10]:\n",
    "        if len(data_vn30) > period:\n",
    "            df[f'momentum_{period}_vn30'] = data_vn30['close_vn30'].pct_change(period)\n",
    "\n",
    "\n",
    "    # 1. Daily log return\n",
    "    data_vn30['log_return_vn30'] = np.log(data_vn30['close_vn30'] / data_vn30['close_vn30'].shift(1))\n",
    "\n",
    "    # 2. Rolling volatility (biáº¿n Ä‘á»™ng trÆ°á»£t)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vn30[f'volatility_{window}d_vn30'] = data_vn30['log_return_vn30'].rolling(window=window).std()\n",
    "\n",
    "    # 3. Rolling mean log return\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        data_vn30[f'mean_log_return_{window}d_vn30'] = data_vn30['log_return_vn30'].rolling(window=window).mean()\n",
    "\n",
    "    # 4. Sharpe-like ratio (mean return / volatility)\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vn30 = f'mean_log_return_{window}d_vn30'\n",
    "        vol_col_vn30 = f'volatility_{window}d_vn30'\n",
    "        data_vn30[f'sharpe_like_{window}d_vn30'] = data_vn30[mean_col_vn30] / (data_vn30[vol_col_vn30] + 1e-9)\n",
    "\n",
    "    # 5. Directional Feature - chuá»—i ngÃ y tÄƒng liÃªn tiáº¿p\n",
    "    data_vn30['up_streak_vn30'] = (data_vn30['log_return_vn30'] > 0).astype(int)\n",
    "    data_vn30['up_streak_vn30'] = data_vn30['up_streak_vn30'] * (data_vn30['up_streak_vn30'].groupby((data_vn30['up_streak_vn30'] != data_vn30['up_streak_vn30'].shift()).cumsum()).cumcount() + 1)\n",
    "    data_vn30['up_streak_vn30'] = data_vn30['up_streak_vn30'].where(data_vn30['log_return_vn30'] > 0, 0)\n",
    "\n",
    "    # 6. Tá»· lá»‡ ngÃ y log return dÆ°Æ¡ng trong 20 ngÃ y gáº§n nháº¥t\n",
    "    data_vn30['pos_log_return_ratio_20d_vn30'] = data_vn30['log_return_vn30'].rolling(window=20).apply(lambda x: np.mean(x > 0), raw=True)\n",
    "\n",
    "    # 7. Z-score cá»§a log return hiá»‡n táº¡i\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        mean_col_vn30 = f'mean_log_return_{window}d_vn30'\n",
    "        vol_col_vn30 = f'volatility_{window}d_vn30'\n",
    "        data_vn30[f'z_score_{window}d_vn30'] = (data_vn30['log_return_vn30'] - data_vn30[mean_col_vn30]) / (data_vn30[vol_col_vn30] + 1e-9)\n",
    "\n",
    "\n",
    "    # TÃ­nh Annual Return (lá»£i nhuáº­n hÃ ng nÄƒm)\n",
    "    data_vn30['annual_return_vn30'] = data_vn30['close_vn30'] / data_vn30['close_vn30'].shift(252) - 1  # 252 ngÃ y giao dá»‹ch trong nÄƒm\n",
    "\n",
    "    # TÃ­nh Volatility (Äá»™ biáº¿n Ä‘á»™ng) hÃ ng nÄƒm (Standard Deviation of Returns)\n",
    "    data_vn30['daily_return_vn30'] = data_vn30['close_vn30'].pct_change()  # Tá»· suáº¥t lá»£i nhuáº­n hÃ ng ngÃ y\n",
    "    data_vn30['annual_volatility_vn30'] = data_vn30['daily_return_vn30'].rolling(window=252).std() * (252 ** 0.5)  # Äá»™ lá»‡ch chuáº©n hÃ ng nÄƒm (tÃ­nh tá»« tá»· suáº¥t lá»£i nhuáº­n hÃ ng ngÃ y)\n",
    "\n",
    "    risk_free_rate = 0.03  # tÆ°Æ¡ng Ä‘Æ°Æ¡ng 3%/nÄƒm\n",
    "    # TÃ­nh Sharpe ratio: (Rp - Rf) / Volatility\n",
    "    data_vn30['sharpe_ratio_vn30'] = (data_vn30['annual_return_vn30'] - risk_free_rate) / data_vn30['annual_volatility_vn30']\n",
    "\n",
    "\n",
    "    data_vn30['time'] = pd.to_datetime(data_vn30['time']).dt.date\n",
    "\n",
    "\n",
    "\n",
    "    # merge\n",
    "    df['time'] = pd.to_datetime(df['time']).dt.date\n",
    "    df = pd.merge(df, data_vni, on='time')\n",
    "    df = pd.merge(df, data_vn30, on='time')\n",
    "\n",
    "\n",
    "     # TÃ­nh tá»· lá»‡ thay Ä‘á»•i giÃ¡\n",
    "    threshold=0.01\n",
    "    df['change'] = (df['close'].shift(-1) - df['close']) / df['close']\n",
    "\n",
    "    # GÃ¡n nhÃ£n\n",
    "    df['target'] = 1  # Hold máº·c Ä‘á»‹nh\n",
    "    df.loc[df['change'] > threshold, 'target'] = 2   # Buy\n",
    "    df.loc[df['change'] < -threshold, 'target'] = 0  # Sell\n",
    "\n",
    "    if drop_na:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "672d3c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äang láº¥y dá»¯ liá»‡u cho: ACB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 11:36:47 - vnstock.common.data.data_explorer - INFO - KhÃ´ng pháº£i lÃ  mÃ£ chá»©ng khoÃ¡n, thÃ´ng tin cÃ´ng ty vÃ  tÃ i chÃ­nh khÃ´ng kháº£ dá»¥ng.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ thu tháº­p dá»¯ liá»‡u PhÃ¢n tÃ­ch cÆ¡ báº£n vÃ  phÃ¢n tÃ­ch ká»¹ thuáº­t cho ACB ngÃ y 2025-09-05 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 11:36:47 - vnstock.common.data.data_explorer - INFO - KhÃ´ng pháº£i lÃ  mÃ£ chá»©ng khoÃ¡n, thÃ´ng tin cÃ´ng ty vÃ  tÃ i chÃ­nh khÃ´ng kháº£ dá»¥ng.\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>P/B_Previous_Quarter</th>\n",
       "      <th>P/B_Same_Period_Last_Year</th>\n",
       "      <th>P/B_d_1</th>\n",
       "      <th>P/B_d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>z_score_5d_vn30</th>\n",
       "      <th>z_score_10d_vn30</th>\n",
       "      <th>z_score_20d_vn30</th>\n",
       "      <th>z_score_30d_vn30</th>\n",
       "      <th>annual_return_vn30</th>\n",
       "      <th>daily_return_vn30</th>\n",
       "      <th>annual_volatility_vn30</th>\n",
       "      <th>sharpe_ratio_vn30</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>27.60</td>\n",
       "      <td>27.75</td>\n",
       "      <td>27.40</td>\n",
       "      <td>27.65</td>\n",
       "      <td>11279900</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.549748</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.339106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.455021</td>\n",
       "      <td>0.449498</td>\n",
       "      <td>0.490590</td>\n",
       "      <td>0.517766</td>\n",
       "      <td>0.423146</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.191727</td>\n",
       "      <td>2.050548</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>27.95</td>\n",
       "      <td>28.05</td>\n",
       "      <td>27.75</td>\n",
       "      <td>27.75</td>\n",
       "      <td>8852600</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.549748</td>\n",
       "      <td>1.628569</td>\n",
       "      <td>1.339106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113257</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>-0.052652</td>\n",
       "      <td>-0.037642</td>\n",
       "      <td>0.425283</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.191740</td>\n",
       "      <td>2.061557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time   open   high    low  close    volume  P/B_Previous_Quarter  \\\n",
       "498  2025-09-04  27.60  27.75  27.40  27.65  11279900              1.628569   \n",
       "499  2025-09-05  27.95  28.05  27.75  27.75   8852600              1.628569   \n",
       "\n",
       "     P/B_Same_Period_Last_Year   P/B_d_1   P/B_d_2  ...  z_score_5d_vn30  \\\n",
       "498                   1.549748  1.628569  1.339106  ...         1.455021   \n",
       "499                   1.549748  1.628569  1.339106  ...        -0.113257   \n",
       "\n",
       "     z_score_10d_vn30  z_score_20d_vn30  z_score_30d_vn30  annual_return_vn30  \\\n",
       "498          0.449498          0.490590          0.517766            0.423146   \n",
       "499          0.025803         -0.052652         -0.037642            0.425283   \n",
       "\n",
       "     daily_return_vn30  annual_volatility_vn30  sharpe_ratio_vn30    change  \\\n",
       "498           0.012906                0.191727           2.050548  0.003617   \n",
       "499           0.003870                0.191740           2.061557       NaN   \n",
       "\n",
       "     target  \n",
       "498       1  \n",
       "499       1  \n",
       "\n",
       "[2 rows x 308 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta_fa_result = load_and_process_data('ACB', start_date='2023-09-05', end_date='2025-09-05', drop_na=False)\n",
    "df_ta_fa = df_ta_fa_result.tail(2)\n",
    "df_ta_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0b2a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tÃ­nh trend cho cÃ¡c feature cÆ¡ báº£n trong 8 quÃ½ gáº§n nháº¥t\n",
    "# thÃªm vÃ o df má»›i cÃ¡c cá»™t trend tÆ°Æ¡ng á»©ng vá»›i cÃ¡c feature cÆ¡ báº£n\n",
    "def add_slope_features(df, feature_base_names, n=8):\n",
    "    \"\"\"\n",
    "    Vá»›i má»—i dÃ²ng trong df, tÃ­nh há»‡ sá»‘ gá»‘c (slope) cá»§a Ä‘Æ°á»ng há»“i quy tuyáº¿n tÃ­nh theo thá»i gian\n",
    "    cho tá»«ng feature trong danh sÃ¡ch feature_base_names dá»±a trÃªn cÃ¡c giÃ¡ trá»‹ 8 quÃ½ gáº§n nháº¥t.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame chá»©a cÃ¡c cá»™t dáº¡ng 'feature_d_1' Ä‘áº¿n 'feature_d_n'.\n",
    "        feature_base_names (list): Danh sÃ¡ch tÃªn feature cÆ¡ báº£n (string).\n",
    "        n (int): Sá»‘ quÃ½ gáº§n nháº¥t Ä‘á»ƒ tÃ­nh trend (máº·c Ä‘á»‹nh 8).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame Ä‘Ã£ thÃªm cÃ¡c cá»™t há»‡ sá»‘ gá»‘c tÆ°Æ¡ng á»©ng.\n",
    "    \"\"\"\n",
    "    for feature in feature_base_names:\n",
    "        col_name = f\"coefficient_{feature}\"\n",
    "        if col_name not in df.columns:\n",
    "            df[col_name] = np.nan\n",
    "\n",
    "    # Duyá»‡t tá»«ng hÃ ng\n",
    "    for index, row in df.iterrows():\n",
    "        for feature in feature_base_names:\n",
    "            y_values = []\n",
    "            for i in range(n, 0, -1):  # d_8, ..., d_1\n",
    "                val = row.get(f\"{feature}_d_{i}\", np.nan)\n",
    "                y_values.append(val)\n",
    "\n",
    "            y_values = np.array(y_values, dtype=float)\n",
    "\n",
    "            # Cáº§n Ã­t nháº¥t 2 Ä‘iá»ƒm há»£p lá»‡ Ä‘á»ƒ fit mÃ´ hÃ¬nh\n",
    "            if np.isnan(y_values).sum() > (n - 2):\n",
    "                coef = np.nan\n",
    "            else:\n",
    "                X = np.arange(n).reshape(-1, 1)\n",
    "                mask = ~np.isnan(y_values)\n",
    "                model = LinearRegression()\n",
    "                model.fit(X[mask], y_values[mask])\n",
    "                coef = model.coef_[0]\n",
    "\n",
    "            df.at[index, f\"coefficient_{feature}\"] = coef\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bac44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_load_and_process_data(symbol, start_date, end_date, drop_na=True):\n",
    "  df = load_and_process_data(symbol, start_date, end_date, drop_na = False)\n",
    "  df = df.tail(2)\n",
    "  feature_base_names = ['P/B','P/E','P/S','P/Cash Flow','EPS (VND)','BVPS (VND)','ROE (%)','ROA (%)']\n",
    "  df = add_slope_features(df, feature_base_names)\n",
    "\n",
    "  df.rename(columns={\n",
    "            'P/B_Previous_Quarter': 'p/b_previous_quarter',\n",
    "            'P/B_change_rate': 'p/b_change_rate',\n",
    "            'P/B_change_rate_flag': 'p/b_change_rate_flag',\n",
    "            'P/E_Previous_Quarter': 'p/e_previous_quarter',\n",
    "            'P/E_change_rate': 'p/e_change_rate',\n",
    "            'P/E_change_rate_flag':'p/e_change_rate_flag',\n",
    "            'P/S_Previous_Quarter': 'p/s_previous_quarter',\n",
    "            'P/S_change_rate': 'p/s_change_rate',\n",
    "            'P/S_change_rate_flag': 'p/s_change_rate_flag',\n",
    "            'P/Cash Flow_Previous_Quarter': 'p/cash_flow_previous_quarter',\n",
    "            'P/Cash Flow_change_rate': 'p/cash_flow_change_rate',\n",
    "            'P/Cash Flow_change_rate_flag' : 'p/cash_flow_change_rate_flag',\n",
    "            'EPS (VND)_Previous_Quarter': 'eps_previous_quarter',\n",
    "            'EPS (VND)_change_rate': 'eps_change_rate',\n",
    "            'EPS (VND)_change_rate_flag': 'eps_change_rate_flag',\n",
    "            'BVPS (VND)_Previous_Quarter': 'bvps_previous_quarter',\n",
    "            'BVPS (VND)_change_rate' : 'bvps_change_rate',\n",
    "            'BVPS (VND)_change_rate_flag': 'bvps_change_rate_flag',\n",
    "            'ROE (%)_Previous_Quarter': 'roe_previous_quarter',\n",
    "            'ROE (%)_change_rate': 'roe_change_rate',\n",
    "            'ROE (%)_change_rate_flag': 'roe_change_rate_flag',\n",
    "            'ROA (%)_Previous_Quarter': 'roa_previous_quarter',\n",
    "            'ROA (%)_change_rate': 'roa_change_rate',\n",
    "            'ROA (%)_change_rate_flag': 'roa_change_rate_flag',\n",
    "            'coefficient_P/B': 'coefficient_p/b',\n",
    "            'coefficient_P/E': 'coefficient_p/e',\n",
    "            'coefficient_P/S': 'coefficient_p/s',\n",
    "            'coefficient_P/Cash Flow': 'coefficient_p/cash_flow',\n",
    "            'coefficient_EPS (VND)': 'coefficient_eps',\n",
    "            'coefficient_BVPS (VND)': 'coefficient_bvps',\n",
    "            'coefficient_ROE (%)': 'coefficient_roe',\n",
    "            'coefficient_ROA (%)': 'coefficient_roa'\n",
    "\n",
    "\n",
    "        }, inplace=True)  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3819aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äang láº¥y dá»¯ liá»‡u cho: VHM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 14:57:58 - vnstock.common.data.data_explorer - INFO - KhÃ´ng pháº£i lÃ  mÃ£ chá»©ng khoÃ¡n, thÃ´ng tin cÃ´ng ty vÃ  tÃ i chÃ­nh khÃ´ng kháº£ dá»¥ng.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ thu tháº­p dá»¯ liá»‡u PhÃ¢n tÃ­ch cÆ¡ báº£n vÃ  phÃ¢n tÃ­ch ká»¹ thuáº­t cho VHM ngÃ y 2025-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2025-09-05 14:57:58 - vnstock.common.data.data_explorer - INFO - KhÃ´ng pháº£i lÃ  mÃ£ chá»©ng khoÃ¡n, thÃ´ng tin cÃ´ng ty vÃ  tÃ i chÃ­nh khÃ´ng kháº£ dá»¥ng.\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>p/b_previous_quarter</th>\n",
       "      <th>P/B_Same_Period_Last_Year</th>\n",
       "      <th>P/B_d_1</th>\n",
       "      <th>P/B_d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "      <th>coefficient_p/b</th>\n",
       "      <th>coefficient_p/e</th>\n",
       "      <th>coefficient_p/s</th>\n",
       "      <th>coefficient_p/cash_flow</th>\n",
       "      <th>coefficient_eps</th>\n",
       "      <th>coefficient_bvps</th>\n",
       "      <th>coefficient_roe</th>\n",
       "      <th>coefficient_roa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.5</td>\n",
       "      <td>5198000</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016393</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127646</td>\n",
       "      <td>1.19543</td>\n",
       "      <td>0.320346</td>\n",
       "      <td>-9.421164</td>\n",
       "      <td>66.826894</td>\n",
       "      <td>1771.340893</td>\n",
       "      <td>-0.00999</td>\n",
       "      <td>-0.005391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>89.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>88.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7811500</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127646</td>\n",
       "      <td>1.19543</td>\n",
       "      <td>0.320346</td>\n",
       "      <td>-9.421164</td>\n",
       "      <td>66.826894</td>\n",
       "      <td>1771.340893</td>\n",
       "      <td>-0.00999</td>\n",
       "      <td>-0.005391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time  open  high   low  close   volume  p/b_previous_quarter  \\\n",
       "498  2025-07-30  92.1  92.5  89.0   91.5  5198000              1.971749   \n",
       "499  2025-07-31  89.7  91.5  88.2   90.0  7811500              1.971749   \n",
       "\n",
       "     P/B_Same_Period_Last_Year   P/B_d_1   P/B_d_2  ...    change  target  \\\n",
       "498                   1.027068  1.971749  1.776398  ... -0.016393       0   \n",
       "499                   1.027068  1.971749  1.776398  ...       NaN       1   \n",
       "\n",
       "     coefficient_p/b  coefficient_p/e  coefficient_p/s  \\\n",
       "498         0.127646          1.19543         0.320346   \n",
       "499         0.127646          1.19543         0.320346   \n",
       "\n",
       "     coefficient_p/cash_flow  coefficient_eps  coefficient_bvps  \\\n",
       "498                -9.421164        66.826894       1771.340893   \n",
       "499                -9.421164        66.826894       1771.340893   \n",
       "\n",
       "     coefficient_roe  coefficient_roa  \n",
       "498         -0.00999        -0.005391  \n",
       "499         -0.00999        -0.005391  \n",
       "\n",
       "[2 rows x 316 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta_fa = fn_load_and_process_data('VHM', start_date='2023-07-31', end_date='2025-07-31')\n",
    "df_ta_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu tháº­p tin tá»©c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a3140277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NgÃ y dá»± Ä‘oÃ¡n lÃ  31/07/2025, ngÃ y giao dá»‹ch trÆ°á»›c Ä‘Ã³ lÃ  30/07/2025 - > cáº§n láº¥y cÃ¡c tin tá»©c trong khoáº£n 14:45 ngÃ y 30/07/2025 Ä‘áº¿n 14:45 ngÃ y 01/08/2025)\n",
    "start_date = '30/07/2025 14:45'\n",
    "end_date = '01/08/2025 14:45'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a07166be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from openpyxl.utils import escape\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64eaff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.binary_location = \"/usr/bin/google-chrome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e0a7f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid escape sequence '\\c'\n",
      "invalid escape sequence '\\c'\n",
      "invalid escape sequence '\\c'\n"
     ]
    }
   ],
   "source": [
    "chrome_driver_path = \"D:\\chromedriver-win64-140\\chromedriver-win64\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ae824c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True  # khÃ´ng hiá»ƒn thá»‹ trÃ¬nh duyá»‡t\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--blink-settings=imagesEnabled=false\")  # táº¯t hÃ¬nh áº£nh\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/115.0 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e10f6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khá»Ÿi táº¡o driver\n",
    "driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0e58dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test thá»­ má»Ÿ trang Cafef\n",
    "driver.get(\"https://cafef.vn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb74dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu tháº­p title vÃ  url cá»§a cÃ¡c bÃ i bÃ¡o tá»« cafe f trong khoáº£n thá»i gian nháº¥t Ä‘á»‹nh\n",
    "def craw_title_url_stock_news_cafef(ticker, start_date, end_date):\n",
    "  # Khá»Ÿi táº¡o driver\n",
    "  driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "  df = pd.DataFrame(data = None, columns = ['ticker','time', 'tittle', 'url'])\n",
    "  driver.get(\"https://cafef.vn/du-lieu/tin-doanh-nghiep/\"+str(ticker)+\"/event.chn\")\n",
    "  print(driver.title)\n",
    "  flag = True\n",
    "  while (flag):\n",
    "    li_elements = driver.find_elements(By.XPATH, \"//div[@id='divEvents']/ul/li\")\n",
    "    for idx, li in enumerate(li_elements, start= 1):\n",
    "      span = li.find_element(By.TAG_NAME, \"span\")\n",
    "      curent_date_of_new = span.text\n",
    "      curent_date_of_new_convert = datetime.strptime(curent_date_of_new, \"%d/%m/%Y %H:%M\")\n",
    "      if (curent_date_of_new_convert < start_date):\n",
    "        print(\"ÄÃ£ tÃ¬m háº¿t cÃ¡c bÃ i bÃ¡o trong khoáº£n thá»i gian cáº§n tÃ¬m\")\n",
    "        flag = False\n",
    "        break\n",
    "      if curent_date_of_new_convert >= start_date and curent_date_of_new_convert <= end_date:\n",
    "        time = curent_date_of_new\n",
    "        url = li.find_element(By.TAG_NAME, \"a\").get_attribute('href')\n",
    "        title = li.find_element(By.TAG_NAME, \"a\").text\n",
    "        print('Äang láº¥y bÃ i bÃ¡o vá»›i thÃ´ng tin nhÆ° sau: ')\n",
    "        print(time)\n",
    "        print(title)\n",
    "        print(url)\n",
    "        new_row = {'ticker': ticker, 'time':time, 'tittle': title, 'url':url}\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "      if (idx == len(li_elements)):\n",
    "        # print(\"ÄÃ¢y lÃ  pháº§n tá»­ cuá»‘i cÃ¹ng\")\n",
    "        # Click vÃ o nÃºt xem tiáº¿p\n",
    "        try:\n",
    "          btn_next_element = driver.find_element(By.XPATH, \"//span[@id='spanNext']/a\")\n",
    "          driver.execute_script(\"arguments[0].click();\", btn_next_element)\n",
    "          sleep(random.uniform(1, 2))\n",
    "          # print(\"Cuyá»ƒn qua trang tiáº¿p theo\")\n",
    "        except Exception as e:\n",
    "          print(f\"KhÃ´ng thá»ƒ click nÃºt tiáº¿p: {e}\")\n",
    "          flag = False\n",
    "          break\n",
    "  # 5. ÄÃ³ng browser\n",
    "  driver.close()\n",
    "  print(f\"ÄÃ£ thu tháº­p {len(df)} tin tá»©c tá»« cafe f cho cá»• phiáº¿u {ticker} trong khoáº£n thá»i gian {start_date} Ä‘áº¿n {end_date}\")\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d5ff97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÄÃ£ tÃ¬m háº¿t cÃ¡c bÃ i bÃ¡o trong khoáº£n thá»i gian cáº§n tÃ¬m\n",
      "ÄÃ£ thu tháº­p 0 tin tá»©c tá»« cafe f cho cá»• phiáº¿u ACB trong khoáº£n thá»i gian 2025-09-04 14:45:00 Ä‘áº¿n 2025-09-06 14:45:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, time, tittle, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VÃ­ dá»¥ thu tháº­p title vÃ  url cá»§a cÃ¡c bÃ i bÃ¡o tá»« cafe f trong khoáº£n thá»i gian nháº¥t Ä‘á»‹nh\n",
    "start_date_cafef = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "end_date_cafef = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "cafef_news_title_url = craw_title_url_stock_news_cafef(\"ACB\", start_date_cafef, end_date_cafef)\n",
    "cafef_news_title_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6a845f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu tháº­p ná»™i dung cá»§a cÃ¡c bÃ i bÃ¡o tá»« title vÃ  url trÃªn cafe f.\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return ILLEGAL_CHARACTERS_RE.sub(\"\", text)\n",
    "    return text\n",
    "def get_content_news_cafef(cafef_news_title_url):\n",
    "  # Khá»Ÿi táº¡o driver\n",
    "  driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "  df = cafef_news_title_url\n",
    "  df['Content'] = \"\"\n",
    "  for i in range(len(df)):\n",
    "    text = \"\"\n",
    "    url = df.url[i]\n",
    "    try:\n",
    "        #Má»Ÿ vÃ  thu tháº­p dá»¯ liá»‡u ná»™i dung bÃ i bÃ¡o thÃ´ng qua URL cá»§a bÃ i bÃ¡o\n",
    "        driver.get(url)\n",
    "        print(\"Äang xá»­ lÃ½ URL: \" + str(url))\n",
    "        print(\"Äang xá»­ lÃ½ bÃ i bÃ¡o thá»©: \" + str(i))\n",
    "        try:\n",
    "            title_news = driver.find_element(\"xpath\",\"//div[@class='left_cate totalcontentdetail']/h1[@class='title']\")\n",
    "        except:\n",
    "            title_news = None\n",
    "            # browser.quit()\n",
    "        if title_news is not None:\n",
    "          try:\n",
    "            # description = browser.find_element(\"xpath\", \"/html/body/form/div[2]/div[4]/div[5]/div/div[1]/div[\" + str(j) + \"]/h2\")\n",
    "            description = driver.find_element(\"xpath\",\"//div[@class='w640 fr clear']/h2[@class='sapo'][1]\")\n",
    "\n",
    "          except:\n",
    "              continue\n",
    "\n",
    "          text = text + description.text + \" \"\n",
    "\n",
    "          for k in range(1,30):\n",
    "            try:\n",
    "              article = driver.find_element(\"xpath\", \"//div[@class='detail-content afcbc-body']/p[\"+str(k)+\"]\")\n",
    "              text = text + article.text\n",
    "            except:\n",
    "              break\n",
    "          cleaned_text = clean_text(text)\n",
    "          df.loc[i, 'Content'] = df.loc[i, 'Content'] + cleaned_text\n",
    "    except Exception as e:\n",
    "      print(f\"Lá»—i khi xá»­ lÃ½ URL {url}: {e}\")\n",
    "      continue\n",
    "  print(f\"ÄÃ£ láº¥y ná»™i dung {len(df)} bÃ i bÃ¡o tá»« cafe f\")\n",
    "  driver.close()\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6c65066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ láº¥y ná»™i dung 0 bÃ i bÃ¡o tá»« cafe f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, time, tittle, url, Content]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VÃ­ dá»¥ thu tháº­p ná»™i dung cá»§a cÃ¡c bÃ i bÃ¡o tá»« title vÃ  url trÃªn cafe f.\n",
    "df_content_news_cafef = get_content_news_cafef(cafef_news_title_url)\n",
    "df_content_news_cafef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "56366c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get official statement pdf link and download pdf file\n",
    "def get_official_statement(df_content_news_cafef, destination_file, destination_official_statement_folder):\n",
    "    def init_driver():\n",
    "        # options = webdriver.ChromeOptions()\n",
    "        # options.add_argument('--headless')  # TÃ¹y chá»n náº¿u báº¡n khÃ´ng cáº§n giao diá»‡n\n",
    "        # return webdriver.Chrome(options=options)\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = True  # khÃ´ng hiá»ƒn thá»‹ trÃ¬nh duyá»‡t\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--blink-settings=imagesEnabled=false\")  # táº¯t hÃ¬nh áº£nh\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/115.0 Safari/537.36\")\n",
    "        return webdriver.Chrome(options=options)\n",
    "\n",
    "    driver = init_driver()  # Khá»Ÿi táº¡o driver ban Ä‘áº§u\n",
    "\n",
    "    # df = pd.read_excel(original_file)\n",
    "    df = df_content_news_cafef\n",
    "    df_news_official_statement = df[df['Content'].fillna('').str.strip() == ''].copy()\n",
    "    df_news_official_statement['official_statement_pdf_name'] = \"\"\n",
    "    print(f\"ÄÃ£ Ä‘á»c file excel chá»©a danh sÃ¡ch cÃ¡c file pdf nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cáº§n láº¥y gá»“m {df_news_official_statement.shape[0]} file pdf.\")\n",
    "\n",
    "    os.makedirs(destination_official_statement_folder, exist_ok=True)\n",
    "    max_retries = 5\n",
    "\n",
    "    for i, row in df_news_official_statement.iterrows():\n",
    "        url = row['url']\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(random.uniform(1, 5))\n",
    "                print(f\"\\nðŸ“° Äang xá»­ lÃ½ bÃ i bÃ¡o thá»© {i} (Láº§n thá»­ {attempt+1}):\\n{url}\")\n",
    "                driver.get(url)\n",
    "                \n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                pdf_element = wait.until(EC.presence_of_element_located((\n",
    "                    By.XPATH,\n",
    "                    \"//div[@id='newscontent']//a[contains(@href, '.pdf')]\"\n",
    "                )))\n",
    "                pdf_url = pdf_element.get_attribute('href')\n",
    "                full_pdf_url = urljoin(url, pdf_url)\n",
    "\n",
    "                df_news_official_statement.at[i, 'Content'] = full_pdf_url\n",
    "                print(f\"ðŸ“Ž TÃ¬m tháº¥y link PDF: {full_pdf_url}\")\n",
    "\n",
    "                pdf_filename = os.path.basename(full_pdf_url)\n",
    "                pdf_path = os.path.join(destination_official_statement_folder, pdf_filename)\n",
    "\n",
    "                response = requests.get(full_pdf_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    with open(pdf_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"ðŸ“ ÄÃ£ táº£i file PDF: {pdf_filename}\")\n",
    "                    df_news_official_statement.at[i, 'official_statement_pdf_name'] = pdf_filename\n",
    "                else:\n",
    "                    print(f\"âš ï¸ KhÃ´ng thá»ƒ táº£i PDF: {full_pdf_url} (mÃ£ tráº¡ng thÃ¡i {response.status_code})\")\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                err_msg = str(e)\n",
    "                print(f\"âŒ Lá»—i láº§n {attempt+1} khi xá»­ lÃ½ URL {url}: {err_msg}\")\n",
    "\n",
    "                # Náº¿u lá»—i liÃªn quan Ä‘áº¿n máº¥t káº¿t ná»‘i WebDriver\n",
    "                if (\n",
    "                    \"Max retries exceeded\" in err_msg or \n",
    "                    \"Failed to establish a new connection\" in err_msg or \n",
    "                    isinstance(e, WebDriverException)\n",
    "                ):\n",
    "                    print(\"ðŸ”Œ PhÃ¡t hiá»‡n lá»—i káº¿t ná»‘i vá»›i driver. Äang khá»Ÿi Ä‘á»™ng láº¡i...\")\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass  # Äáº£m báº£o khÃ´ng crash náº¿u driver Ä‘Ã£ bá»‹ Ä‘Ã³ng\n",
    "\n",
    "                    time.sleep(2)\n",
    "                    driver = init_driver()\n",
    "                    continue\n",
    "\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(\"ðŸ” Äang thá»­ láº¡i sau 2 giÃ¢y...\")\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    print(\"ðŸš« Bá» qua URL nÃ y sau nhiá»u láº§n thá»­.\")\n",
    "                    break\n",
    "\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with pd.ExcelWriter(destination_file, engine='openpyxl', mode='w') as writer:\n",
    "        df_news_official_statement.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u file Excel chá»©a {len(df_news_official_statement)} nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cá»§a cÃ´ng ty vÃ o : {destination_file}\")\n",
    "    return df_news_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f8a3c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ Ä‘á»c file excel chá»©a danh sÃ¡ch cÃ¡c file pdf nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cáº§n láº¥y gá»“m 0 file pdf.\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u file Excel chá»©a 0 nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cá»§a cÃ´ng ty vÃ o : D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "      <th>official_statement_pdf_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, time, tittle, url, Content, official_statement_pdf_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vÃ­ dá»¥ láº¥y file pdf official statement\n",
    "destination_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx'\n",
    "destination_official_statement_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\official_statement_pdf'\n",
    "df_news_official_statement = get_official_statement(df_content_news_cafef, destination_file, destination_official_statement_file)\n",
    "df_news_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================crawl news tuoitre - vietnamnet - vnexpress=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e6633028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl news vnexpress\n",
    "def convert_time_format(raw_time):\n",
    "    try:\n",
    "        # Bá» pháº§n ngÃ y trong tuáº§n (VD: \"Thá»© ba,\") vÃ  pháº§n (GMT+7)\n",
    "        clean_time = re.sub(r'^.*?,\\s*', '', raw_time)\n",
    "        clean_time = re.sub(r'\\s*\\(.*?\\)', '', clean_time)      # Bá» \"(GMT+7)\"\n",
    "        clean_time = clean_time.strip()\n",
    "        print(clean_time)\n",
    "\n",
    "        # Chuyá»ƒn Ä‘á»•i thÃ nh datetime\n",
    "        dt = datetime.strptime(clean_time, \"%d/%m/%Y, %H:%M\")\n",
    "\n",
    "        return dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i chuyá»ƒn Ä‘á»•i thá»i gian: {e}\")\n",
    "        return raw_time\n",
    "def crawl_news_vnexpress(ticker='VIC', start_date='01/01/2024 00:00', end_date='01/05/2025 23:59', max_pages=5, delay=2):\n",
    "    \"\"\"\n",
    "    Crawl tin tá»©c tá»« VnExpress theo tá»« khÃ³a (ticker) sá»­ dá»¥ng Selenium vÃ  tráº£ vá» DataFrame.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Tá»« khÃ³a tÃ¬m kiáº¿m, vÃ­ dá»¥ 'VIC'.\n",
    "        max_pages (int): Sá»‘ trang káº¿t quáº£ tÃ¬m kiáº¿m muá»‘n thu tháº­p.\n",
    "        delay (int): Thá»i gian chá» giá»¯a cÃ¡c trang (giÃ¢y).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame chá»©a cÃ¡c cá»™t: title, ticker, time, url, Content.\n",
    "    \"\"\"\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    base_url = f\"https://timkiem.vnexpress.net/?search_q={ticker}&cate_code=kinhdoanh&media_type=all&latest=on&fromdate=&todate=&date_format=all\"\n",
    "    start_dt = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        driver.get(f\"{base_url}&page={page}\")\n",
    "        print(f\"ÄÃ£ vÃ o xá»­ lÃ½ trang {page}...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"article.item-news[data-url]\")\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o trÃªn trang {page}.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                print(article.text.strip())\n",
    "                title_tag = article.find_elements(By.CSS_SELECTOR, \"h3.title-news a\")[0]\n",
    "                title = title_tag.text.strip()\n",
    "                url = title_tag.get_attribute(\"href\")\n",
    "                print(f\"Äang xá»­ lÃ½ bÃ i viáº¿t: {title}\")\n",
    "                print(f\"URL: {url}\")\n",
    "\n",
    "\n",
    "\n",
    "                # Truy cáº­p tá»«ng bÃ i viáº¿t Ä‘á»ƒ láº¥y ná»™i dung\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(url)\n",
    "                print(f\"Truy cáº­p bÃ i viáº¿t: {url}\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "                # Láº¥y thá»i gian Ä‘Äƒng\n",
    "                try:\n",
    "                    time_tag = driver.find_element(By.CSS_SELECTOR, \"span.date\")\n",
    "                    time_text = time_tag.text.strip()\n",
    "                    time_text = convert_time_format(time_text)\n",
    "                except Exception as e:\n",
    "                    time_text = \"\"\n",
    "                    print(f\"Lá»—i láº¥y thá»i gian: {e}\")\n",
    "                print(f\"Thá»i gian Ä‘Äƒng: {time_text}\")\n",
    "\n",
    "                # Kiá»ƒm tra Ä‘iá»u kiá»‡n thá»i gian\n",
    "                pub_time_date = datetime.strptime(time_text, \"%d/%m/%Y %H:%M\")\n",
    "                if pub_time_date > end_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: {time_text}\")\n",
    "                    continue\n",
    "                if pub_time_date < start_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"BÃ i viáº¿t quÃ¡ cÅ© khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: {time_text}\")\n",
    "                    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "                # Láº¥y ná»™i dung bÃ i viáº¿t (cáº£ mÃ´ táº£ náº¿u cÃ³)\n",
    "                try:\n",
    "                    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"article.fck_detail p\")\n",
    "                    description = driver.find_elements(By.CSS_SELECTOR, \"p.description\")\n",
    "                    content_parts = []\n",
    "\n",
    "                    if description:\n",
    "                        content_parts.extend([desc.text.strip() for desc in description if desc.text.strip()])\n",
    "                    content_parts.extend([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "\n",
    "                    content = \"\\n\".join(content_parts)\n",
    "\n",
    "                except:\n",
    "                    content = \"\"\n",
    "\n",
    "                print(f\"Ná»™i dung bÃ i viáº¿t: {content}\")\n",
    "\n",
    "\n",
    "                # ÄÃ³ng tab bÃ i viáº¿t vÃ  quay láº¡i\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(\"ÄÃ£ Ä‘Ã³ng tab bÃ i viáº¿t\")\n",
    "\n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'time': time_text,\n",
    "                    'tittle': title,\n",
    "                    'url': url,\n",
    "                    'Content': clean_text(content)\n",
    "                })\n",
    "                print(\"ÄÃ£ thÃªm vÃ o káº¿t quáº£\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Lá»—i khi xá»­ lÃ½ bÃ i viáº¿t: {e}\")\n",
    "                continue\n",
    "\n",
    "    # driver.quit()\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"ÄÃ£ thu tháº­p tá»•ng cá»™ng {len(df)} bÃ i viáº¿t tá»« VnExpress.\")\n",
    "    driver.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fbc38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl news vietnamnet\n",
    "def convert_time_format_vietnamnet(raw_time):\n",
    "    \"\"\"\n",
    "    Chuyá»ƒn Ä‘á»‹nh dáº¡ng 'Thá»© Ba, 13/05/2025 - 05:35' => '13/05/2025 05:35'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Loáº¡i bá» \"Thá»© ...,\" vÃ  tÃ¡ch theo dáº¥u \"-\"\n",
    "        clean_time = re.sub(r'^.*?,\\s*', '', raw_time)  # \"Thá»© Ba, \" -> \"\"\n",
    "        date_part, time_part = clean_time.split(\" - \")\n",
    "        dt = datetime.strptime(f\"{date_part.strip()} {time_part.strip()}\", \"%d/%m/%Y %H:%M\")\n",
    "        return dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i chuyá»ƒn Ä‘á»•i thá»i gian: {e}\")\n",
    "        return raw_time\n",
    "def crawl_news_vietnamnet(ticker='VIC', start_date='01/01/2024 00:00', end_date='01/05/2025 23:59', max_pages=3, delay=2):\n",
    "    start_dt = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "    results = []\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    for page in range(0, max_pages):\n",
    "        print(f\"Äang xá»­ lÃ½ trang {page+1}...\")\n",
    "        search_url = f\"https://vietnamnet.vn/tim-kiem-p{page}?bydaterang=all&cate=000003&newstype=all&od=2&q={ticker}\"\n",
    "        driver.get(search_url)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"div.horizontalPost\")\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o trÃªn trang {page}.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                a_tag = article.find_elements(By.CSS_SELECTOR, \"div.horizontalPost__avt a\")[0]\n",
    "                title = a_tag.get_attribute(\"title\")\n",
    "                url = a_tag.get_attribute(\"href\")\n",
    "                print(f\"Äang xá»­ lÃ½: {title}\")\n",
    "                print(f\"URL: {url}\")\n",
    "\n",
    "                # Má»Ÿ bÃ i viáº¿t Ä‘á»ƒ láº¥y ná»™i dung vÃ  thá»i gian\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(url)\n",
    "                time.sleep(delay)\n",
    "\n",
    "                # Thá»i gian\n",
    "                try:\n",
    "                    time_element = driver.find_element(By.CSS_SELECTOR, \"div.bread-crumb-detail__time\")\n",
    "                    time_text = convert_time_format_vietnamnet(time_element.text.strip())\n",
    "                except:\n",
    "                    time_text = \"\"\n",
    "                print(f\"Thá»i gian Ä‘Äƒng: {time_text}\")\n",
    "\n",
    "                # Kiá»ƒm tra Ä‘iá»u kiá»‡n thá»i gian\n",
    "                pub_time_date = datetime.strptime(time_text, \"%d/%m/%Y %H:%M\")\n",
    "                if pub_time_date > end_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: {time_text}\")\n",
    "                    continue\n",
    "                if pub_time_date < start_dt:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                    print(f\"BÃ i viáº¿t quÃ¡ cÅ© khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: {time_text}\")\n",
    "                    return pd.DataFrame(results)\n",
    "\n",
    "                # Ná»™i dung\n",
    "                try:\n",
    "                    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"div.maincontent p\")\n",
    "                    description = driver.find_elements(By.CSS_SELECTOR, \"h2.content-detail-sapo\")\n",
    "                    content_parts = []\n",
    "\n",
    "                    if description:\n",
    "                        content_parts.extend([desc.text.strip() for desc in description if desc.text.strip()])\n",
    "                    content_parts.extend([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "\n",
    "                    content = \"\\n\".join(content_parts)\n",
    "                except:\n",
    "                    content = \"\"\n",
    "                print(f\"Ná»™i dung bÃ i viáº¿t: {content}\")\n",
    "                # ÄÃ³ng tab\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(\"ÄÃ£ Ä‘Ã³ng tab bÃ i viáº¿t\")\n",
    "\n",
    "\n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'time': time_text,\n",
    "                    'tittle': title,\n",
    "                    'url': url,\n",
    "                    'Content': clean_text(content)\n",
    "                })\n",
    "                print(\"ÄÃ£ thÃªm vÃ o káº¿t quáº£\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Lá»—i xá»­ lÃ½ bÃ i viáº¿t: {e}\")\n",
    "                continue\n",
    "\n",
    "    # driver.quit()\n",
    "    driver.close()\n",
    "    print(f\"ÄÃ£ thu tháº­p tá»•ng cá»™ng {len(results)} bÃ i viáº¿t tá»« Vietnamnet.\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "056d3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_format_tuoitre(raw_time):\n",
    "    \"\"\"\n",
    "    Chuyá»ƒn Ä‘á»‹nh dáº¡ng '30/12/2024 15:30 GMT+7' => '30/12/2024 15:30'\n",
    "    Tráº£ vá» cáº£ chuá»—i thá»i gian (string) vÃ  Ä‘á»‘i tÆ°á»£ng datetime Ä‘á»ƒ so sÃ¡nh\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Loáº¡i bá» pháº§n \"GMT+7\"\n",
    "        clean_time = raw_time.replace(\"GMT+7\", \"\").strip()\n",
    "        dt = datetime.strptime(clean_time, \"%d/%m/%Y %H:%M\")\n",
    "        return dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i chuyá»ƒn Ä‘á»•i thá»i gian Tuá»•i Tráº»: {e}\")\n",
    "        return raw_time\n",
    "def crawl_news_tuoitre(ticker='VIC', start_date='01/01/2024 00:00', end_date='01/05/2025 23:59', delay=2):\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    start_dt = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "\n",
    "    driver.get(f\"https://tuoitre.vn/tim-kiem.htm?keywords={ticker}&zoneId=11\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Giai Ä‘oáº¡n 1: Load toÃ n bá»™ bÃ i viáº¿t\n",
    "    while True:\n",
    "        print(\"Báº¯t Ä‘áº§u load bÃ i viáº¿t ....\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "        try:\n",
    "            button = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.box-viewmore a\"))\n",
    "            )\n",
    "            button.click()\n",
    "            time.sleep(delay)\n",
    "            print(\"Äang load bÃ i bÃ¡o ....\")\n",
    "        except:\n",
    "            break  # khÃ´ng cÃ²n nÃºt xem thÃªm\n",
    "            print(\"ÄÃ£ load xong bÃ i bÃ¡o\")\n",
    "\n",
    "    # Giai Ä‘oáº¡n 2: Duyá»‡t vÃ  xá»­ lÃ½ bÃ i viáº¿t\n",
    "    results = []\n",
    "    articles = driver.find_elements(By.CSS_SELECTOR, \"div.box-category-item\")\n",
    "    print(f\"Tá»•ng sá»‘ bÃ i viáº¿t load Ä‘Æ°á»£c: {len(articles)}\")\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            a_tag = article.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title = a_tag.get_attribute(\"title\")\n",
    "            url = a_tag.get_attribute(\"href\")\n",
    "            print(f\"Äang xá»­ lÃ½ bÃ i viáº¿t: {title}\")\n",
    "            print(f\"URL: {url}\")\n",
    "\n",
    "            # Má»Ÿ tab má»›i Ä‘á»ƒ láº¥y thá»i gian vÃ  ná»™i dung bÃ i viáº¿t\n",
    "            driver.execute_script(\"window.open('');\")\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            driver.get(url)\n",
    "            time.sleep(delay)\n",
    "            print(f\"Truy cáº­p bÃ i viáº¿t: {url}\")\n",
    "\n",
    "            # Láº¥y thá»i gian chÃ­nh xÃ¡c\n",
    "            try:\n",
    "                time_element = driver.find_element(By.CSS_SELECTOR, 'div[data-role=\"publishdate\"]')\n",
    "                pub_time = convert_time_format_tuoitre(time_element.text.strip())\n",
    "                print(f\"Thá»i gian Ä‘Äƒng: {pub_time}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lá»—i láº¥y thá»i gian: {e}\")\n",
    "                pub_time = None\n",
    "\n",
    "            if not pub_time:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                continue\n",
    "\n",
    "            # Kiá»ƒm tra Ä‘iá»u kiá»‡n thá»i gian\n",
    "            pub_time_date = datetime.strptime(pub_time, \"%d/%m/%Y %H:%M\")\n",
    "            if pub_time_date > end_dt:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(f\"BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: {pub_time}\")\n",
    "                continue\n",
    "            if pub_time_date < start_dt:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                print(f\"BÃ i viáº¿t quÃ¡ cÅ© khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: {pub_time}\")\n",
    "                break  # cÃ¡c bÃ i sau cháº¯c cháº¯n cÃ²n cÅ© hÆ¡n\n",
    "\n",
    "            # Láº¥y ná»™i dung bÃ i viáº¿t\n",
    "            try:\n",
    "                content_parts = []\n",
    "                desc = driver.find_elements(By.CSS_SELECTOR, \"h2.detail-sapo\")\n",
    "                paras = driver.find_elements(By.CSS_SELECTOR, \"div.detail-content > p\")\n",
    "                content_parts.extend([d.text.strip() for d in desc if d.text.strip()])\n",
    "                content_parts.extend([p.text.strip() for p in paras if p.text.strip()])\n",
    "                content = \"\\n\".join(content_parts)\n",
    "            except:\n",
    "                content = \"\"\n",
    "            print(f\"Ná»™i dung bÃ i viáº¿t: {content}\")\n",
    "\n",
    "            # ÄÃ³ng tab\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "\n",
    "            results.append({\n",
    "                'ticker': ticker,\n",
    "                'time': pub_time,\n",
    "                'tittle': title,\n",
    "                'url': url,\n",
    "                'Content': clean_text(content)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Lá»—i khi xá»­ lÃ½ bÃ i viáº¿t: {e}\")\n",
    "            if len(driver.window_handles) > 1:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "            continue\n",
    "\n",
    "    # driver.quit()\n",
    "    print(f\"Tá»•ng sá»‘ bÃ i viáº¿t Ä‘Æ°á»£c láº¥y: {len(results)}\")\n",
    "    driver.close()\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e7cf31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news_and_official_statement_cafef_vnexpress_vietnamnet_tuoitre_for_stock(ticker, start_date, end_date, delay=2):\n",
    "    # Láº¥y url vÃ  title cá»§a news vÃ  official stament tá»« cafe f\n",
    "    start_date_cafef = datetime.strptime(start_date, \"%d/%m/%Y %H:%M\")\n",
    "    end_date_cafef = datetime.strptime(end_date, \"%d/%m/%Y %H:%M\")\n",
    "    cafef_news_title_url = craw_title_url_stock_news_cafef(ticker, start_date_cafef, end_date_cafef)\n",
    "\n",
    "    # thu tháº­p ná»™i dung cá»§a cÃ¡c bÃ i bÃ¡o tá»« title vÃ  url trÃªn cafe f.\n",
    "    df_content_news_cafef = get_content_news_cafef(cafef_news_title_url)\n",
    "\n",
    "    # Láº¥y official stament tá»« cafe f\n",
    "    destination_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx'\n",
    "    destination_official_statement_file = r'D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\official_statement_pdf'\n",
    "    df_news_official_statement = get_official_statement(cafef_news_title_url, destination_file, destination_official_statement_file)\n",
    "\n",
    "    # Láº¥y cÃ¡c df cá»§a cÃ¡c bÃ i bÃ¡o tá»« cafe f tá»« \n",
    "    df_news_cafef = df_content_news_cafef[df_content_news_cafef['Content'].notna() & (df_content_news_cafef['Content'].str.strip() != '')].copy()\n",
    "    \n",
    "    # Láº¥y cÃ¡c bÃ i bÃ¡o tá»« vnexpress\n",
    "    df_vn = crawl_news_vnexpress(ticker, start_date, end_date, max_pages= 38, delay=delay)\n",
    "    # Láº¥y cÃ¡c bÃ i bÃ¡o tá»« vietnamnet\n",
    "    df_vietnamnet = crawl_news_vietnamnet(ticker, start_date, end_date, max_pages= 80, delay=delay)\n",
    "    # Láº¥y cÃ¡c bÃ i bÃ¡o tá»« tuoitre\n",
    "    df_tuoitre = crawl_news_tuoitre(ticker, start_date, end_date, delay=delay)\n",
    "    df = pd.concat([df_news_cafef, df_vn, df_vietnamnet, df_tuoitre], ignore_index=True)\n",
    "    return df, df_news_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b563899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Äang láº¥y bÃ i bÃ¡o vá»›i thÃ´ng tin nhÆ° sau: \n",
      "31/07/2025 17:18\n",
      "VHM: BÃ¡o cÃ¡o tÃ¬nh hÃ¬nh quáº£n trá»‹ 6 thÃ¡ng Ä‘áº§u nÄƒm 2025\n",
      "https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025.chn\n",
      "Äang láº¥y bÃ i bÃ¡o vá»›i thÃ´ng tin nhÆ° sau: \n",
      "30/07/2025 15:18\n",
      "Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 thÃ¡ng Ä‘áº§u nÄƒm 2025, cáº§m gáº§n 49.000 tá»· Ä‘á»“ng tiá»n máº·t\n",
      "https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty-dong-sau-6-thang-dau-nam-2025-cam-gan-49000-ty-dong-tien-mat-188250730151844214.chn\n",
      "ÄÃ£ tÃ¬m háº¿t cÃ¡c bÃ i bÃ¡o trong khoáº£n thá»i gian cáº§n tÃ¬m\n",
      "ÄÃ£ thu tháº­p 2 tin tá»©c tá»« cafe f cho cá»• phiáº¿u VHM trong khoáº£n thá»i gian 2025-07-30 14:45:00 Ä‘áº¿n 2025-08-01 14:45:00\n",
      "Äang xá»­ lÃ½ URL: https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025.chn\n",
      "Äang xá»­ lÃ½ bÃ i bÃ¡o thá»©: 0\n",
      "Äang xá»­ lÃ½ URL: https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty-dong-sau-6-thang-dau-nam-2025-cam-gan-49000-ty-dong-tien-mat-188250730151844214.chn\n",
      "Äang xá»­ lÃ½ bÃ i bÃ¡o thá»©: 1\n",
      "ÄÃ£ láº¥y ná»™i dung 2 bÃ i bÃ¡o tá»« cafe f\n",
      "ÄÃ£ Ä‘á»c file excel chá»©a danh sÃ¡ch cÃ¡c file pdf nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cáº§n láº¥y gá»“m 1 file pdf.\n",
      "\n",
      "ðŸ“° Äang xá»­ lÃ½ bÃ i bÃ¡o thá»© 0 (Láº§n thá»­ 1):\n",
      "https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025.chn\n",
      "ðŸ“Ž TÃ¬m tháº¥y link PDF: https://cafef1.mediacdn.vn/download/310725/vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025-0.pdf\n",
      "ðŸ“ ÄÃ£ táº£i file PDF: vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam-2025-0.pdf\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u file Excel chá»©a 1 nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cá»§a cÃ´ng ty vÃ o : D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx\n",
      "ÄÃ£ vÃ o xá»­ lÃ½ trang 1...\n",
      "Cá»• phiáº¿u há» Vin giáº£m sÃ n\n",
      "VIC vÃ  VHM giáº£m háº¿t biÃªn Ä‘á»™, cÃ²n VRE máº¥t 3,6% lÃ  nguyÃªn nhÃ¢n chÃ­nh khiáº¿n VN-Index rÆ¡i sÃ¢u trong phiÃªn giao dá»‹ch Ä‘áº§u tuáº§n.\n",
      "Äang xá»­ lÃ½ bÃ i viáº¿t: Cá»• phiáº¿u há» Vin giáº£m sÃ n\n",
      "URL: https://vnexpress.net/co-phieu-ho-vin-giam-san-4896548.html\n",
      "Truy cáº­p bÃ i viáº¿t: https://vnexpress.net/co-phieu-ho-vin-giam-san-4896548.html\n",
      "9/6/2025, 15:40\n",
      "Thá»i gian Ä‘Äƒng: 09/06/2025 15:40\n",
      "BÃ i viáº¿t quÃ¡ cÅ© khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 09/06/2025 15:40\n",
      "Äang xá»­ lÃ½ trang 1...\n",
      "Äang xá»­ lÃ½: Chá»©ng khoÃ¡n lÃªn sÃ¡t Ä‘á»‰nh: Bá»©c tranh tÆ°Æ¡i sÃ¡ng vá»›i nhá»¯ng 'con sá»‘ Ä‘áº¹p'\n",
      "URL: https://vietnamnet.vn/chung-khoan-len-sat-dinh-buc-tranh-tuoi-sang-voi-nhung-con-so-dep-2437214.html\n",
      "Thá»i gian Ä‘Äƒng: 28/08/2025 20:57\n",
      "BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 28/08/2025 20:57\n",
      "Äang xá»­ lÃ½: GiÃ¡ vÃ ng lÃªn 127,7 triá»‡u Ä‘á»“ng/lÆ°á»£ng, chá»©ng khoÃ¡n tÄƒng chÃ³ng máº·t phiÃªn 26/8\n",
      "URL: https://vietnamnet.vn/gia-vang-len-127-7-trieu-dong-luong-chung-khoan-tang-chong-mat-phien-26-8-2436339.html\n",
      "Thá»i gian Ä‘Äƒng: 26/08/2025 17:22\n",
      "BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 26/08/2025 17:22\n",
      "Äang xá»­ lÃ½: Chá»©ng khoÃ¡n Ä‘á» lá»­a: 2,6 tá»· USD sang tay, VN-Index lao dá»‘c máº¡nh\n",
      "URL: https://vietnamnet.vn/chung-khoan-do-lua-2-6-ty-usd-sang-tay-vn-index-lao-doc-manh-2435003.html\n",
      "Thá»i gian Ä‘Äƒng: 22/08/2025 16:24\n",
      "BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 22/08/2025 16:24\n",
      "Äang xá»­ lÃ½: Tá»· phÃº Nguyá»…n Thá»‹ PhÆ°Æ¡ng Tháº£o giÃ u thá»© hai Viá»‡t Nam\n",
      "URL: https://vietnamnet.vn/ty-phu-vietjet-nguyen-thi-phuong-thao-giau-thu-hai-viet-nam-2434215.html\n",
      "Thá»i gian Ä‘Äƒng: 20/08/2025 21:05\n",
      "BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 20/08/2025 21:05\n",
      "Äang xá»­ lÃ½: VN-Index Ä‘á»©t máº¡ch 9 phiÃªn tÄƒng: Chá»©ng khoÃ¡n cÃ²n hÃºt dÃ²ng tiá»n tá»· USD?\n",
      "URL: https://vietnamnet.vn/vn-index-dut-mach-9-phien-tang-chung-khoan-con-hut-dong-tien-ty-usd-2433126.html\n",
      "Thá»i gian Ä‘Äƒng: 18/08/2025 11:13\n",
      "BÃ i viáº¿t quÃ¡ má»›i khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 18/08/2025 11:13\n",
      "Äang xá»­ lÃ½: Chá»‰ sá»‘ VN-Index giáº£m sÃ¢u hiáº¿m cÃ³, máº¥t hÆ¡n 64 Ä‘iá»ƒm\n",
      "URL: https://vietnamnet.vn/ban-thao-manh-gan-3-ty-usd-chung-khoan-lao-doc-2426752.html\n",
      "Thá»i gian Ä‘Äƒng: 29/07/2025 17:01\n",
      "BÃ i viáº¿t quÃ¡ cÅ© khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 29/07/2025 17:01\n",
      "Báº¯t Ä‘áº§u load bÃ i viáº¿t ....\n",
      "Tá»•ng sá»‘ bÃ i viáº¿t load Ä‘Æ°á»£c: 13\n",
      "Äang xá»­ lÃ½ bÃ i viáº¿t: VN-Index vá»«a tÄƒng máº¡nh Ä‘Ã£ quay Ä‘áº§u giáº£m do Ã¡p lá»±c chá»‘t lá»i\n",
      "URL: https://tuoitre.vn/vn-index-vua-tang-manh-da-quay-dau-giam-do-ap-luc-chot-loi-20250609170737614.htm\n",
      "Truy cáº­p bÃ i viáº¿t: https://tuoitre.vn/vn-index-vua-tang-manh-da-quay-dau-giam-do-ap-luc-chot-loi-20250609170737614.htm\n",
      "Thá»i gian Ä‘Äƒng: 09/06/2025 17:30\n",
      "BÃ i viáº¿t quÃ¡ cÅ© khÃ´ng náº±m trong khoáº£n thá»i gian cáº§n crawl: 09/06/2025 17:30\n",
      "Tá»•ng sá»‘ bÃ i viáº¿t Ä‘Æ°á»£c láº¥y: 0\n"
     ]
    }
   ],
   "source": [
    "df_news, df_official_statement = crawl_news_and_official_statement_cafef_vnexpress_vietnamnet_tuoitre_for_stock('VHM', start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8242079c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "      <th>official_statement_pdf_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VHM</td>\n",
       "      <td>31/07/2025 17:18</td>\n",
       "      <td>VHM: BÃ¡o cÃ¡o tÃ¬nh hÃ¬nh quáº£n trá»‹ 6 thÃ¡ng Ä‘áº§u nÄƒ...</td>\n",
       "      <td>https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...</td>\n",
       "      <td>https://cafef1.mediacdn.vn/download/310725/vhm...</td>\n",
       "      <td>vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker              time                                             tittle  \\\n",
       "0    VHM  31/07/2025 17:18  VHM: BÃ¡o cÃ¡o tÃ¬nh hÃ¬nh quáº£n trá»‹ 6 thÃ¡ng Ä‘áº§u nÄƒ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  https://cafef1.mediacdn.vn/download/310725/vhm...   \n",
       "\n",
       "                         official_statement_pdf_name  \n",
       "0  vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_official_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "877dcf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VHM</td>\n",
       "      <td>30/07/2025 15:18</td>\n",
       "      <td>Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 th...</td>\n",
       "      <td>https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...</td>\n",
       "      <td>Táº¡i thá»i Ä‘iá»ƒm 30/06/2025, tá»•ng tÃ i sáº£n VHM Ä‘áº¡t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker              time                                             tittle  \\\n",
       "0    VHM  30/07/2025 15:18  Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 th...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...   \n",
       "\n",
       "                                             Content  \n",
       "0  Táº¡i thá»i Ä‘iá»ƒm 30/06/2025, tá»•ng tÃ i sáº£n VHM Ä‘áº¡t...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================Cháº¥m Ä‘iá»ƒm xÃºc cáº£m  =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e81e4e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nhatk\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60e99a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nhatk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51740200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nhatk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bda29aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.81.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nhatk\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5705d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cc0fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t OpenAI Client\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"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\",  # Thay API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116208d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================Cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho news======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3ab33708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def getSize_content(content):\n",
    "    size = len(content)\n",
    "    return size\n",
    "\n",
    "def getSize_sentence(sentence):\n",
    "    size = len(sentence.split(\" \"))\n",
    "    return size\n",
    "\n",
    "def add_messages(message, prompt):\n",
    "    return message + [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "def get_prompt(title, context):\n",
    "    return \"\"\"Chá»§ Ä‘á»: {}\n",
    "    Äoáº¡n vÄƒn: {}\n",
    "    \"\"\".format(title,\n",
    "               context.strip()\n",
    "              ).strip()\n",
    "\n",
    "def ask(messages, title, context):\n",
    "    prompt = get_prompt(title, context)\n",
    "    new_messages = add_messages(messages, prompt)\n",
    "    answer = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "    #   model=\"gpt-3.5-turbo\",\n",
    "      messages=new_messages,\n",
    "      temperature=0\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "def get_time(start_time, end_time):\n",
    "    cost = round(end_time - start_time)\n",
    "    return \"{}:{}\".format(cost // 60, cost % 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "19f7ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentiment_result(sentiment_str):\n",
    "    \"\"\"\n",
    "    PhÃ¢n tÃ­ch chuá»—i nhÆ° '#Reputation 0.2#Financial -0.3...' thÃ nh dict\n",
    "    \"\"\"\n",
    "    pattern = r\"#([\\w\\s&\\-]+)\\s(-?\\d+\\.?\\d*)\"\n",
    "    return {match[0].strip(): float(match[1]) for match in re.findall(pattern, sentiment_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dbd61e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(df, init_messages):\n",
    "\n",
    "  # XÃ³a cÃ¡c dÃ²ng khÃ´ng cÃ³ ná»™i dung á»Ÿ cá»™t 'Content'\n",
    "  df = df[df['Content'].notnull() & (df['Content'].str.strip() != '')]\n",
    "  tqdm.pandas(desc=\"Äang phÃ¢n tÃ­ch cáº£m xÃºc\")\n",
    "\n",
    "  sentiments = []\n",
    "\n",
    "  for i, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc=\"PhÃ¢n tÃ­ch tin tá»©c\"), start=1):\n",
    "    paper_title_processing = row.get('tittle', f\"KhÃ´ng cÃ³ tiÃªu Ä‘á»\")\n",
    "    content = row['Content']\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "    content_processed = convert_sentence(content)\n",
    "    title = 'Tin tá»©c'\n",
    "    text = ' '.join(content_processed)\n",
    "    # In chá»‰ tiÃªu Ä‘á» bÃ i bÃ¡o\n",
    "    print(f\"\\nðŸ“ Äang cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho bÃ i bÃ¡o thá»© {i}/{len(df)}: TiÃªu Ä‘á»: {paper_title_processing}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "      response = ask(init_messages, title, text)\n",
    "      print(\"response \" + str(response))\n",
    "\n",
    "      # TrÃ­ch xuáº¥t ná»™i dung tá»« response\n",
    "\n",
    "      result = response.choices[0].message.content\n",
    "      print(\"result \" + str(result))\n",
    "      # sentiment_data = json.loads(result)\n",
    "      sentiment_data = parse_sentiment_result(result)\n",
    "      print(\"sentiment_data \" + str(sentiment_data))\n",
    "\n",
    "    except Exception as e:\n",
    "      print(\"Error occurred:\", e)\n",
    "      sentiment_data = {\"error\": str(e)}\n",
    "    sentiments.append(sentiment_data)\n",
    "\n",
    "    # TrÃ¡nh rate limit\n",
    "    time.sleep(4)\n",
    "\n",
    "    # Gá»™p cÃ¡c cáº£m xÃºc vÃ o cÃ¡c cá»™t má»›i trong df\n",
    "  sentiment_df = pd.DataFrame(sentiments)\n",
    "  df = pd.concat([df.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)\n",
    "  print('ÄÃ£ cháº¥m Ä‘iá»ƒm xÃºc cáº£m Ä‘a khÃ­a cáº¡nh cho táº¥t cáº£ tin tá»©c')\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "df423e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment_for_stock(ticker, df):\n",
    "    init_user_prompt = \"\"\"Chá»§ Ä‘á»: Tin tá»©c\n",
    "Äoáº¡n vÄƒn: Má»¥c tiÃªu Ä‘Ã³n 8 triá»‡u lÆ°á»£t khÃ¡ch quá»‘c táº¿ trong nÄƒm nay sáº½ khÃ³ kháº£ thi náº¿u chÃºng ta khÃ´ng cÃ³ nhá»¯ng Ä‘á»™ng thÃ¡i quyáº¿t liá»‡t Ä‘á»ƒ xÃ³a bá» nhá»¯ng rÃ o cáº£n vá»›i du khÃ¡ch, trong Ä‘Ã³ Ä‘áº§u tiÃªn lÃ  chÃ­nh sÃ¡ch visa. Káº¿t thÃºc nÄƒm 2022, Viá»‡t Nam Ä‘ang Ä‘á»©ng cuá»‘i báº£ng phá»¥c há»“i du lá»‹ch quá»‘c táº¿ cá»§a khu vá»±c vá»›i tá»· lá»‡ phá»¥c há»“i chá»‰ 18,1%. Trong khi Ä‘Ã³, tá»· lá»‡ nÃ y á»Ÿ cÃ¡c nÆ°á»›c lÃ¡ng giá»ng nhÆ° ThÃ¡i Lan, Singapore, Malaysia hay Campuchia Ä‘á»u Ä‘áº¡t tá»« 26 Ä‘áº¿n 31%. Vá»›i thá»±c táº¿ nÃ y, náº¿u khÃ´ng cÃ³ giáº£i phÃ¡p Ä‘á»™t phÃ¡, thÃ¡o gá»¡ nhá»¯ng rÃ o cáº£n hiá»‡n há»¯u, thÃ¬ má»¥c tiÃªu Ä‘Ã³n 8 triá»‡u lÆ°á»£t du khÃ¡ch quá»‘c táº¿ nÄƒm 2023 cá»§a nÆ°á»›c ta khÃ³ lÃ²ng Ä‘áº¡t Ä‘Æ°á»£c. Äá»“ng nghÄ©a, du lá»‹ch Viá»‡t Nam sáº½ vuá»™t cÆ¡ há»™i hÃºt nguá»“n doanh thu lá»›n tá»« â€œmá» vÃ ngâ€ khÃ¡ch quá»‘c táº¿ Ä‘á»ƒ nhanh chÃ³ng phá»¥c há»“i.Theo giá»›i chuyÃªn mÃ´n, má»™t trong nhá»¯ng rÃ o cáº£n lá»›n Ä‘áº§u tiÃªn khiáº¿n du khÃ¡ch quá»‘c táº¿ khÃ´ng máº·n mÃ  Ä‘áº¿n Viá»‡t Nam lÃ  háº¡n cháº¿ trong chÃ­nh sÃ¡ch visa.Nhá»¯ng báº¥t cáº­p trong chÃ­nh sÃ¡ch visaAnh Yan Nang Oo, du khÃ¡ch Myanmar, cÃ¹ng báº¡n Ä‘i du lá»‹ch Viá»‡t Nam. Do chá»‰ Ä‘Æ°á»£c cáº¥p visa 14 ngÃ y trong khi nhÃ³m báº¡n muá»‘n khÃ¡m phÃ¡ nhiá»u Ä‘iá»ƒm Ä‘áº¿n táº¡i Viá»‡t Nam nÃªn há» pháº£i lÃªn káº¿ hoáº¡ch di chuyá»ƒn gáº¥p tá»« Háº¡ Long, HÃ  Ná»™i, Huáº¿, ÄÃ  Náºµng, PhÃº Quá»‘c. â€œHÃ´m ra sÃ¢n bay Ä‘á»ƒ check-out vá» nÆ°á»›c vá»«a háº¿t 14 ngÃ y, cÅ©ng may thá»§ tá»¥c xuáº¥t cáº£nh thuáº­n lá»£i, khÃ´ng gáº·p váº¥n Ä‘á» gÃ¬â€, du khÃ¡ch nÃ y thá»Ÿ phÃ o.Nhiá»u khÃ¡ch quá»‘c táº¿ mong muá»‘n chÃ­nh sÃ¡ch visa cá»§a Viá»‡t Nam Ä‘Æ°á»£c ná»›i lá»ng, cho phÃ©p du khÃ¡ch kÃ©o dÃ i thá»i háº¡n Ä‘á»ƒ cÃ³ nhiá»u thá»i gian khÃ¡m phÃ¡ vÃ  tráº£i nghiá»‡m. ÄÃ³ cÅ©ng lÃ  mong muá»‘n cá»§a ráº¥t nhiá»u du khÃ¡ch quá»‘c táº¿. Theo thá»‘ng kÃª, Viá»‡t Nam chá»‰ miá»…n thá»‹ thá»±c cho cÃ´ng dÃ¢n cá»§a 24 quá»‘c gia. ÃÃ¢y lÃ  con sá»‘ quÃ¡ Ã­t á»i khi so sÃ¡nh vá»›i cÃ¡c nÆ°á»›c lÃ¡ng giá»ng nhÆ° Singapore miá»…n visa cho 162 nÆ°á»›c, Philippines lÃ  157 nÆ°á»›c, Malaysia lÃ  162 nÆ°á»›c, ThÃ¡i Lan lÃ  64 nÆ°á»›c\"\"\"\n",
    "    init_assistant = \"\"\"#Reputation 0.0#Company Communication 0.0#Appointment 0.0#Financial -0.3#Regulatory -0.4#Sales -0.3#M&A 0.0#Legal -0.1#Dividend Policy 0.0#Risks -0.5#Rumors 0.0#Strategy -0.4#Options 0.0#IPO 0.0#Signal -0.2#Coverage 0.0#Fundamentals -0.2#Insider Activity 0.0#Price Action 0.0#Buyside 0.0#Technical Analysis 0.0#Trade -0.1#Central Banks 0.0#Currency 0.0#Conditions -0.5#Market -0.3#Volatility -0.4#Investor Sentiment -0.6#Retail Investor Behavior -0.4#Speculation 0.0#Domestic Institutional Behavior -0.2#Foreign Institutional Behavior -0.3#Black Swan Event 0.0\"\"\"\n",
    "    init_system_prompt = f'''Báº¡n lÃ  trá»£ lÃ½ há»¯u Ã­ch giÃºp táº¡o cÃ¡c cáº·p 'khÃ­a cáº¡nh' - 'cáº£m xÃºc' tá»« má»™t 'Ä‘oáº¡n bÃ i bÃ¡o thá»i sá»±' cho cá»• phiáº¿u {ticker}. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c yÃªu cáº§u khi trÃ­ch xuáº¥t khÃ­a cáº¡nh vÃ  cáº£m xÃºc:\n",
    "1. CÃ¡c khÃ­a cáº¡nh Ä‘Æ°á»£c trÃ­ch xuáº¥t bao gá»“m: Reputation (Danh tiáº¿ng cá»§a cÃ´ng ty); Company Communication (Truyá»n thÃ´ng cá»§a cÃ´ng ty); Appointment (Bá»• nhiá»‡m); Financial (TÃ i chÃ­nh); Regulatory (CÆ¡ quan quáº£n lÃ­, chÃ­nh sÃ¡ch); Sales (BÃ¡n hÃ ng); M&A (Merge & Acquisition): SÃ¡p nháº­p; Legal (Luáº­t, tÃ­nh há»£p phÃ¡p); Dividend Policy (ChÃ­nh sÃ¡ch cá»• tá»©c); Risks (rá»§i ro - vÃ­ dá»¥ operation risk, credit risk - rá»§i ro tÃ­n dá»¥ng, reputation risk); Rumors (tin Ä‘á»“n); Strategy (chiáº¿n lÆ°á»£c); Options (quyá»n chá»n); IPO (initial public offering): lÃªn sÃ n, niÃªm yáº¿t; Signal (tÃ­n hiá»‡u Ä‘á»ƒ mua bÃ¡n cá»• phiáº¿u); Coverage (bÃ¡o cÃ¡o khuyáº¿n nghá»‹ buy sell hold cá»§a analysts); Fundamentals (cÃ¡c chá»‰ sá»‘ trong phÃ¢n tÃ­ch cÆ¡ báº£n nhÆ° P/E, P/B, Liabilites to Asset ratio); Insider Activity (cÃ¡c hoáº¡t Ä‘á»™ng ná»™i bá»™ cá»§a cÃ´ng ty); Price Action (biáº¿n Ä‘á»™ng giÃ¡); Buyside (cÃ¡c quÄ© Ä‘áº§u tÆ°, cÃ´ng ty nhá») ngÆ°á»£c vá»›i sellside IB (Investment Banks); Technical Analysis (PhÃ¢n tÃ­ch kÄ© thuáº­t); Trade (thÆ°Æ¡ng máº¡i, xuáº¥t nháº­p kháº©u); Central Banks (NgÃ¢n hÃ ng Trung Æ°Æ¡ng, á»Ÿ VN: NgÃ¢n hÃ ng nhÃ  nÆ°á»›c, SBV); Currency (tiá»n tá»‡, tá»‰ giÃ¡); Conditions (Ä‘iá»u kiá»‡n); Market (thá»‹ trÆ°á»ng); Volatility (Ä‘á»™ biáº¿n Ä‘á»™ng, rá»§i ro); Investor Sentiment (TÃ¢m lÃ½ chung cá»§a nhÃ  Ä‘áº§u tÆ°) láº¡c quan, lo láº¯ng, chá» Ä‘á»£i...; Retail Investor Behavior (HÃ nh vi cá»§a nhÃ  Ä‘áº§u tÆ° cÃ¡ nhÃ¢n) FOMO, hoáº£ng loáº¡n, giá»¯ tiá»n máº·tâ€¦; Speculation (Äáº§u cÆ¡, hÃ nh vi Ä‘áº§u tÆ° theo tin Ä‘á»“n hoáº·c ká»³ vá»ng ngáº¯n háº¡n); Domestic Institutional Behavior (HÃ nh vi cá»§a tá»• chá»©c trong nÆ°á»›c: tá»± doanh, báº£o hiá»ƒm, quá»¹ trong nÆ°á»›c) mua rÃ²ng, bÃ¡n rÃ²ng, giáº£m tá»· trá»ng, rÃºt khá»i cá»• phiáº¿u, cÃ¡c giao dá»‹ch thá»a thuáº­n lá»›n giá»¯a cÃ¡c tá»• chá»©c ná»™i bá»™; Foreign Institutional Behavior (HÃ nh vi cá»§a nhÃ  Ä‘áº§u tÆ° tá»• chá»©c nÆ°á»›c ngoÃ i: ETF, quá»¹ ngoáº¡i, nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i) mua rÃ²ng, bÃ¡n rÃ²ng, tin tá»©c Ä‘Æ°a cá»• phiáº¿u vÃ o, ra danh má»¥c ETF, cáº£nh bÃ¡o rá»§i ro, háº¡ má»©c xáº¿p háº¡ng cá»• phiáº¿u tá»« cÃ¡c tá»• chá»©c nÆ°á»›c ngoÃ i; Black Swan Event (Sá»± kiá»‡n thiÃªn nga Ä‘en: hiáº¿m gáº·p, khÃ³ dá»± Ä‘oÃ¡n, gÃ¢y áº£nh hÆ°á»Ÿng nghiÃªm trá»ng nhÆ° khá»§ng hoáº£ng, chiáº¿n tranh, dá»‹ch bá»‡nh...).\n",
    "2. áº¢nh hÆ°á»Ÿng ná»™i dung Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh tÆ°Æ¡ng á»©ng. Vá»›i má»—i chá»‰ sá»‘ áº£nh hÆ°á»Ÿng nÃªn lÃ  sá»‘ thá»±c trong khoáº£ng tá»« -1 (tháº­t sá»± tiÃªu cá»±c) Ä‘áº¿n 1 (tháº­t sá»± tÃ­ch cá»±c)\n",
    "3. Äáº£m báº£o ráº±ng cÃ¡c khÃ­a cáº¡nh mÃ  báº¡n trÃ­ch xuáº¥t liÃªn quan Ä‘áº¿n váº¥n Ä‘á» cá»§a Ä‘oáº¡n bÃ i bÃ¡o thá»i sá»±. VÃ  khÃ­a cáº¡nh nÃ o báº¡n phÃ¢n vÃ¢n hÃ£y xem khÃ´ng áº£nh hÆ°á»Ÿng thÃ¬ Ä‘Ã¡nh lÃ  0.\n",
    "4. ChÃº Ã½ lÃ  dá»±a vÃ o áº£nh hÆ°á»Ÿng cá»§a ná»™i dung Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh. Khi cho tÃ´i káº¿t quáº£ hÃ£y theo format: cÃ³ kÃ­ tá»± '#' trÆ°á»›c khÃ­a cáº¡nh, phÃ­a sau khÃ­a cáº¡nh lÃ  con sá»‘ áº£nh hÆ°á»Ÿng ná»™i dung, vÃ­ dá»¥ vá» format '#khÃ­a cáº¡nh1 1#khÃ­a cáº¡nh2 -1#khÃ­a cáº¡nh3 0'. TuÃ¢n thá»§ theo format cá»§a tÃ´i, khÃ´ng lÃ m khÃ¡c.'''\n",
    "    print(init_system_prompt)\n",
    "\n",
    "    init_messages = [\n",
    "      {\"role\": \"system\", \"content\": init_system_prompt},\n",
    "      {\"role\": \"user\", \"content\": init_user_prompt},\n",
    "      {\"role\": \"assistant\", \"content\": init_assistant},\n",
    "    ]\n",
    "    try:\n",
    "      df_result = score_sentiment(df, init_messages)\n",
    "      return df_result\n",
    "    except Exception as e:\n",
    "      print(f\"Lá»—i khi xá»­ lÃ½ cá»• phiáº¿u {ticker}: {e}\")\n",
    "      return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho cÃ¡c official stament pdf ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5825124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\nhatk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdf2image) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45fc8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fa9d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, os, gc, re\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3848447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONVERT PDF PAGES TO IMAGES IN RAM ---\n",
    "def convert_pdf_to_images_in_memory(pdf_path, dpi=200):\n",
    "    return convert_from_path(pdf_path, dpi=dpi)\n",
    "# --- CONVERT PIL IMAGE TO BASE64 ---\n",
    "def image_to_base64_pil(pil_image):\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "# --- BUILD EXAMPLE FOR GPT FROM PDF IN MEMORY ---\n",
    "def create_image_content_from_pdf(pdf_path, max_pages=3):\n",
    "    images = convert_pdf_to_images_in_memory(pdf_path)\n",
    "    image_contents = []\n",
    "    for i, img in enumerate(images):\n",
    "        if i >= max_pages:  # chá»‰ láº¥y tá»‘i Ä‘a max_pages trang\n",
    "            break\n",
    "        base64_img = image_to_base64_pil(img)\n",
    "        image_contents.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}\n",
    "        })\n",
    "    return image_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f0986042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gá»¬I GPT ---\n",
    "def analyze_pdf_scan_in_memory(pdf_path, init_user_prompt):\n",
    "    init_system_prompt = f'''Báº¡n lÃ  trá»£ lÃ½ há»¯u Ã­ch giÃºp táº¡o cÃ¡c cáº·p 'khÃ­a cáº¡nh' - 'cáº£m xÃºc' tá»« má»™t 'vÄƒn báº£n chÃ­nh thá»©c', 'vÄƒn báº£n phÃ¡p lÃ½', 'nghá»‹ quyáº¿t', 'bÃ¡o cÃ¡o chÃ­nh thá»©c' cá»§a cÃ´ng ty do cÃ´ng ty cÃ´ng bá»‘. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c yÃªu cáº§u khi trÃ­ch xuáº¥t khÃ­a cáº¡nh vÃ  cáº£m xÃºc:\n",
    "1. ChÃºng tÃ´i Ä‘Ã£ chuyá»ƒn file pdf scan thÃ nh danh sÃ¡ch cÃ¡c áº£nh. Báº¡n cáº§n dÃ¹ng kháº£ nÄƒng xá»­ lÃ½ áº£nh (vision) cá»§a mÃ¬nh Ä‘á»ƒ â€œnhÃ¬nâ€, â€œÄ‘á»câ€ vÃ  hiá»ƒu rÃµ ná»™i dung trong cÃ¡c áº£nh PNG Ä‘á»ƒ cháº¥m Ä‘iá»ƒm cÃ¡c khÃ­a cáº¡nh cáº£m xÃºc.\n",
    "2. CÃ¡c khÃ­a cáº¡nh Ä‘Æ°á»£c trÃ­ch xuáº¥t bao gá»“m: Reputation (Danh tiáº¿ng cá»§a cÃ´ng ty); Company Communication (Truyá»n thÃ´ng cá»§a cÃ´ng ty); Appointment (Bá»• nhiá»‡m); Financial (TÃ i chÃ­nh); Regulatory (CÆ¡ quan quáº£n lÃ­, chÃ­nh sÃ¡ch); Sales (BÃ¡n hÃ ng); M&A (Merge & Acquisition): SÃ¡p nháº­p; Legal (Luáº­t, tÃ­nh há»£p phÃ¡p); Dividend Policy (ChÃ­nh sÃ¡ch cá»• tá»©c); Risks (rá»§i ro - vÃ­ dá»¥ operation risk, credit risk - rá»§i ro tÃ­n dá»¥ng, reputation risk); Rumors (tin Ä‘á»“n); Strategy (chiáº¿n lÆ°á»£c); Options (quyá»n chá»n); IPO (initial public offering): lÃªn sÃ n, niÃªm yáº¿t; Signal (tÃ­n hiá»‡u Ä‘á»ƒ mua bÃ¡n cá»• phiáº¿u); Coverage (bÃ¡o cÃ¡o khuyáº¿n nghá»‹ buy sell hold cá»§a analysts); Fundamentals (cÃ¡c chá»‰ sá»‘ trong phÃ¢n tÃ­ch cÆ¡ báº£n nhÆ° P/E, P/B, Liabilites to Asset ratio); Insider Activity (cÃ¡c hoáº¡t Ä‘á»™ng ná»™i bá»™ cá»§a cÃ´ng ty); Price Action (biáº¿n Ä‘á»™ng giÃ¡); Buyside (cÃ¡c quÄ© Ä‘áº§u tÆ°, cÃ´ng ty nhá») ngÆ°á»£c vá»›i sellside IB (Investment Banks); Technical Analysis (PhÃ¢n tÃ­ch kÄ© thuáº­t); Trade (thÆ°Æ¡ng máº¡i, xuáº¥t nháº­p kháº©u); Central Banks (NgÃ¢n hÃ ng Trung Æ°Æ¡ng, á»Ÿ VN: NgÃ¢n hÃ ng nhÃ  nÆ°á»›c, SBV); Currency (tiá»n tá»‡, tá»‰ giÃ¡); Conditions (Ä‘iá»u kiá»‡n); Market (thá»‹ trÆ°á»ng); Volatility (Ä‘á»™ biáº¿n Ä‘á»™ng, rá»§i ro); Investor Sentiment (TÃ¢m lÃ½ chung cá»§a nhÃ  Ä‘áº§u tÆ°) láº¡c quan, lo láº¯ng, chá» Ä‘á»£i...; Retail Investor Behavior (HÃ nh vi cá»§a nhÃ  Ä‘áº§u tÆ° cÃ¡ nhÃ¢n) FOMO, hoáº£ng loáº¡n, giá»¯ tiá»n máº·tâ€¦; Speculation (Äáº§u cÆ¡, hÃ nh vi Ä‘áº§u tÆ° theo tin Ä‘á»“n hoáº·c ká»³ vá»ng ngáº¯n háº¡n); Domestic Institutional Behavior (HÃ nh vi cá»§a tá»• chá»©c trong nÆ°á»›c: tá»± doanh, báº£o hiá»ƒm, quá»¹ trong nÆ°á»›c) mua rÃ²ng, bÃ¡n rÃ²ng, giáº£m tá»· trá»ng, rÃºt khá»i cá»• phiáº¿u, cÃ¡c giao dá»‹ch thá»a thuáº­n lá»›n giá»¯a cÃ¡c tá»• chá»©c ná»™i bá»™; Foreign Institutional Behavior (HÃ nh vi cá»§a nhÃ  Ä‘áº§u tÆ° tá»• chá»©c nÆ°á»›c ngoÃ i: ETF, quá»¹ ngoáº¡i, nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i) mua rÃ²ng, bÃ¡n rÃ²ng, tin tá»©c Ä‘Æ°a cá»• phiáº¿u vÃ o, ra danh má»¥c ETF, cáº£nh bÃ¡o rá»§i ro, háº¡ má»©c xáº¿p háº¡ng cá»• phiáº¿u tá»« cÃ¡c tá»• chá»©c nÆ°á»›c ngoÃ i; Black Swan Event (Sá»± kiá»‡n thiÃªn nga Ä‘en: hiáº¿m gáº·p, khÃ³ dá»± Ä‘oÃ¡n, gÃ¢y áº£nh hÆ°á»Ÿng nghiÃªm trá»ng nhÆ° khá»§ng hoáº£ng, chiáº¿n tranh, dá»‹ch bá»‡nh...).\n",
    "3. áº¢nh hÆ°á»Ÿng ná»™i dung Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh tÆ°Æ¡ng á»©ng. Vá»›i má»—i chá»‰ sá»‘ áº£nh hÆ°á»Ÿng nÃªn lÃ  sá»‘ thá»±c trong khoáº£ng tá»« -1 (tháº­t sá»± tiÃªu cá»±c) Ä‘áº¿n 1 (tháº­t sá»± tÃ­ch cá»±c)\n",
    "4. Äáº£m báº£o ráº±ng cÃ¡c khÃ­a cáº¡nh mÃ  báº¡n trÃ­ch xuáº¥t liÃªn quan Ä‘áº¿n váº¥n Ä‘á» cá»§a vÄƒn báº£n trÃªn. VÃ  khÃ­a cáº¡nh nÃ o báº¡n phÃ¢n vÃ¢n hÃ£y xem khÃ´ng áº£nh hÆ°á»Ÿng thÃ¬ Ä‘Ã¡nh lÃ  0.\n",
    "5. ChÃº Ã½ lÃ  dá»±a vÃ o áº£nh hÆ°á»Ÿng cá»§a ná»™i dung Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh. Khi cho tÃ´i káº¿t quáº£ hÃ£y theo format: cÃ³ kÃ­ tá»± '#' trÆ°á»›c khÃ­a cáº¡nh, phÃ­a sau khÃ­a cáº¡nh lÃ  con sá»‘ áº£nh hÆ°á»Ÿng ná»™i dung, vÃ­ dá»¥ vá» format '#khÃ­a cáº¡nh1 1#khÃ­a cáº¡nh2 -1#khÃ­a cáº¡nh3 0'. TuÃ¢n thá»§ theo format cá»§a tÃ´i, khÃ´ng lÃ m khÃ¡c.'''\n",
    "    query = \"Dá»±a vÃ o cÃ¡c hÃ¬nh áº£nh sau, hÃ£y Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a ná»™i dung trong áº£nh Ä‘áº¿n tá»«ng khÃ­a cáº¡nh theo yÃªu cáº§u Ä‘Ã£ mÃ´ táº£.\"\n",
    "    \n",
    "    init_assistant = \"\"\"#Reputation 0.1#Company Communication 0.3#Appointment 0.0#Financial 0.5#Regulatory 0.0#Sales 0.0#M&A 0.0#Legal 0.0#Dividend Policy 0.0#Risks -0.1#Rumors 0.0#Strategy 0.0#Options 0.0#IPO 0.0#Signal 0.1#Coverage 0.0#Fundamentals 0.4#Insider Activity 0.0#Price Action 0.0#Buyside 0.0#Technical Analysis 0.0#Trade 0.0#Central Banks 0.0#Currency 0.0#Conditions 0.1#Market 0.0#Volatility 0.1#Investor Sentiment 0.2#Retail Investor Behavior 0.0#Speculation 0.0#Domestic Institutional Behavior 0.0#Foreign Institutional Behavior 0.0#Black Swan Event 0.0\"\"\"\n",
    "    image_contents = create_image_content_from_pdf(pdf_path)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": init_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}] + init_user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": init_assistant},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}] + image_contents},\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPT lá»—i: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        del image_contents\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7797a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYZE Tá»ªNG FILE PDF TRONG EXCEL ---\n",
    "def analyze_official_statements_for_stock(folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt):\n",
    "    df = pd.read_excel(folder_excel_input)\n",
    "\n",
    "    sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "    for col in sentiment_columns:\n",
    "        df[col] = None\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            pdf_name = row.get(\"official_statement_pdf_name\", \"\")\n",
    "            if not isinstance(pdf_name, str) or not pdf_name.endswith(\".pdf\"):\n",
    "                continue\n",
    "            pdf_path = os.path.join(folder_pdf_input, pdf_name)\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"âŒ Máº¥t file: {pdf_name}\")\n",
    "                continue\n",
    "            \n",
    "            init_user_prompt = create_image_content_from_pdf(pdf_path_init_user_prompt)\n",
    "            sentiment_str = analyze_pdf_scan_in_memory(pdf_path, init_user_prompt)\n",
    "            if sentiment_str is None:\n",
    "                continue\n",
    "\n",
    "            sentiment_data = parse_sentiment_result(sentiment_str)\n",
    "            for col in sentiment_columns:\n",
    "                if col in sentiment_data:\n",
    "                    df.at[i, col] = sentiment_data[col]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Lá»—i á»Ÿ {pdf_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"ÄÃ£ cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho cÃ¡c nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cá»§a cÃ´ng ty\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "161fb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score_for_all(ticker, df_news, folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt):\n",
    "    print(\"Äang cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho cÃ¡c bÃ i bÃ¡o\")\n",
    "    df_news_sentiemt_scored = score_sentiment_for_stock(ticker, df_news)\n",
    "    print(\"Äang cháº¥m Ä‘iá»ƒm cÃ¡c tuyÃªn bá»‘ chÃ­nh thá»©c\")\n",
    "    df_official_statement_scored = analyze_official_statements_for_stock(folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt)\n",
    "    print(\"Äang káº¿t há»£p cÃ¡c káº¿t quáº£ cháº¥m Ä‘iá»ƒm\")\n",
    "    df = pd.concat([df_news_sentiemt_scored, df_official_statement_scored], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "79642611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äang cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho cÃ¡c bÃ i bÃ¡o\n",
      "Báº¡n lÃ  trá»£ lÃ½ há»¯u Ã­ch giÃºp táº¡o cÃ¡c cáº·p 'khÃ­a cáº¡nh' - 'cáº£m xÃºc' tá»« má»™t 'Ä‘oáº¡n bÃ i bÃ¡o thá»i sá»±' cho cá»• phiáº¿u VHM. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c yÃªu cáº§u khi trÃ­ch xuáº¥t khÃ­a cáº¡nh vÃ  cáº£m xÃºc:\n",
      "1. CÃ¡c khÃ­a cáº¡nh Ä‘Æ°á»£c trÃ­ch xuáº¥t bao gá»“m: Reputation (Danh tiáº¿ng cá»§a cÃ´ng ty); Company Communication (Truyá»n thÃ´ng cá»§a cÃ´ng ty); Appointment (Bá»• nhiá»‡m); Financial (TÃ i chÃ­nh); Regulatory (CÆ¡ quan quáº£n lÃ­, chÃ­nh sÃ¡ch); Sales (BÃ¡n hÃ ng); M&A (Merge & Acquisition): SÃ¡p nháº­p; Legal (Luáº­t, tÃ­nh há»£p phÃ¡p); Dividend Policy (ChÃ­nh sÃ¡ch cá»• tá»©c); Risks (rá»§i ro - vÃ­ dá»¥ operation risk, credit risk - rá»§i ro tÃ­n dá»¥ng, reputation risk); Rumors (tin Ä‘á»“n); Strategy (chiáº¿n lÆ°á»£c); Options (quyá»n chá»n); IPO (initial public offering): lÃªn sÃ n, niÃªm yáº¿t; Signal (tÃ­n hiá»‡u Ä‘á»ƒ mua bÃ¡n cá»• phiáº¿u); Coverage (bÃ¡o cÃ¡o khuyáº¿n nghá»‹ buy sell hold cá»§a analysts); Fundamentals (cÃ¡c chá»‰ sá»‘ trong phÃ¢n tÃ­ch cÆ¡ báº£n nhÆ° P/E, P/B, Liabilites to Asset ratio); Insider Activity (cÃ¡c hoáº¡t Ä‘á»™ng ná»™i bá»™ cá»§a cÃ´ng ty); Price Action (biáº¿n Ä‘á»™ng giÃ¡); Buyside (cÃ¡c quÄ© Ä‘áº§u tÆ°, cÃ´ng ty nhá») ngÆ°á»£c vá»›i sellside IB (Investment Banks); Technical Analysis (PhÃ¢n tÃ­ch kÄ© thuáº­t); Trade (thÆ°Æ¡ng máº¡i, xuáº¥t nháº­p kháº©u); Central Banks (NgÃ¢n hÃ ng Trung Æ°Æ¡ng, á»Ÿ VN: NgÃ¢n hÃ ng nhÃ  nÆ°á»›c, SBV); Currency (tiá»n tá»‡, tá»‰ giÃ¡); Conditions (Ä‘iá»u kiá»‡n); Market (thá»‹ trÆ°á»ng); Volatility (Ä‘á»™ biáº¿n Ä‘á»™ng, rá»§i ro); Investor Sentiment (TÃ¢m lÃ½ chung cá»§a nhÃ  Ä‘áº§u tÆ°) láº¡c quan, lo láº¯ng, chá» Ä‘á»£i...; Retail Investor Behavior (HÃ nh vi cá»§a nhÃ  Ä‘áº§u tÆ° cÃ¡ nhÃ¢n) FOMO, hoáº£ng loáº¡n, giá»¯ tiá»n máº·tâ€¦; Speculation (Äáº§u cÆ¡, hÃ nh vi Ä‘áº§u tÆ° theo tin Ä‘á»“n hoáº·c ká»³ vá»ng ngáº¯n háº¡n); Domestic Institutional Behavior (HÃ nh vi cá»§a tá»• chá»©c trong nÆ°á»›c: tá»± doanh, báº£o hiá»ƒm, quá»¹ trong nÆ°á»›c) mua rÃ²ng, bÃ¡n rÃ²ng, giáº£m tá»· trá»ng, rÃºt khá»i cá»• phiáº¿u, cÃ¡c giao dá»‹ch thá»a thuáº­n lá»›n giá»¯a cÃ¡c tá»• chá»©c ná»™i bá»™; Foreign Institutional Behavior (HÃ nh vi cá»§a nhÃ  Ä‘áº§u tÆ° tá»• chá»©c nÆ°á»›c ngoÃ i: ETF, quá»¹ ngoáº¡i, nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i) mua rÃ²ng, bÃ¡n rÃ²ng, tin tá»©c Ä‘Æ°a cá»• phiáº¿u vÃ o, ra danh má»¥c ETF, cáº£nh bÃ¡o rá»§i ro, háº¡ má»©c xáº¿p háº¡ng cá»• phiáº¿u tá»« cÃ¡c tá»• chá»©c nÆ°á»›c ngoÃ i; Black Swan Event (Sá»± kiá»‡n thiÃªn nga Ä‘en: hiáº¿m gáº·p, khÃ³ dá»± Ä‘oÃ¡n, gÃ¢y áº£nh hÆ°á»Ÿng nghiÃªm trá»ng nhÆ° khá»§ng hoáº£ng, chiáº¿n tranh, dá»‹ch bá»‡nh...).\n",
      "2. áº¢nh hÆ°á»Ÿng ná»™i dung Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh tÆ°Æ¡ng á»©ng. Vá»›i má»—i chá»‰ sá»‘ áº£nh hÆ°á»Ÿng nÃªn lÃ  sá»‘ thá»±c trong khoáº£ng tá»« -1 (tháº­t sá»± tiÃªu cá»±c) Ä‘áº¿n 1 (tháº­t sá»± tÃ­ch cá»±c)\n",
      "3. Äáº£m báº£o ráº±ng cÃ¡c khÃ­a cáº¡nh mÃ  báº¡n trÃ­ch xuáº¥t liÃªn quan Ä‘áº¿n váº¥n Ä‘á» cá»§a Ä‘oáº¡n bÃ i bÃ¡o thá»i sá»±. VÃ  khÃ­a cáº¡nh nÃ o báº¡n phÃ¢n vÃ¢n hÃ£y xem khÃ´ng áº£nh hÆ°á»Ÿng thÃ¬ Ä‘Ã¡nh lÃ  0.\n",
      "4. ChÃº Ã½ lÃ  dá»±a vÃ o áº£nh hÆ°á»Ÿng cá»§a ná»™i dung Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh. Khi cho tÃ´i káº¿t quáº£ hÃ£y theo format: cÃ³ kÃ­ tá»± '#' trÆ°á»›c khÃ­a cáº¡nh, phÃ­a sau khÃ­a cáº¡nh lÃ  con sá»‘ áº£nh hÆ°á»Ÿng ná»™i dung, vÃ­ dá»¥ vá» format '#khÃ­a cáº¡nh1 1#khÃ­a cáº¡nh2 -1#khÃ­a cáº¡nh3 0'. TuÃ¢n thá»§ theo format cá»§a tÃ´i, khÃ´ng lÃ m khÃ¡c.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PhÃ¢n tÃ­ch tin tá»©c:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Äang cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho bÃ i bÃ¡o thá»© 1/1: TiÃªu Ä‘á»: Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 thÃ¡ng Ä‘áº§u nÄƒm 2025, cáº§m gáº§n 49.000 tá»· Ä‘á»“ng tiá»n máº·t\n",
      "response ChatCompletion(id='chatcmpl-CCLlxeRfwHvT42K5qjCQSzRmgn9W8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='#Reputation 0.5#Company Communication 0.3#Appointment 0.0#Financial 0.7#Regulatory 0.0#Sales 0.6#M&A 0.0#Legal 0.0#Dividend Policy 0.0#Risks 0.0#Rumors 0.0#Strategy 0.6#Options 0.0#IPO 0.0#Signal 0.5#Coverage 0.0#Fundamentals 0.6#Insider Activity 0.0#Price Action 0.4#Buyside 0.0#Technical Analysis 0.0#Trade 0.0#Central Banks 0.0#Currency 0.0#Conditions 0.0#Market 0.5#Volatility 0.0#Investor Sentiment 0.6#Retail Investor Behavior 0.0#Speculation 0.0#Domestic Institutional Behavior 0.0#Foreign Institutional Behavior 0.0#Black Swan Event 0.0', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757059665, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_cbf1785567', usage=CompletionUsage(completion_tokens=225, prompt_tokens=2696, total_tokens=2921, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "result #Reputation 0.5#Company Communication 0.3#Appointment 0.0#Financial 0.7#Regulatory 0.0#Sales 0.6#M&A 0.0#Legal 0.0#Dividend Policy 0.0#Risks 0.0#Rumors 0.0#Strategy 0.6#Options 0.0#IPO 0.0#Signal 0.5#Coverage 0.0#Fundamentals 0.6#Insider Activity 0.0#Price Action 0.4#Buyside 0.0#Technical Analysis 0.0#Trade 0.0#Central Banks 0.0#Currency 0.0#Conditions 0.0#Market 0.5#Volatility 0.0#Investor Sentiment 0.6#Retail Investor Behavior 0.0#Speculation 0.0#Domestic Institutional Behavior 0.0#Foreign Institutional Behavior 0.0#Black Swan Event 0.0\n",
      "sentiment_data {'Reputation': 0.5, 'Company Communication': 0.3, 'Appointment': 0.0, 'Financial': 0.7, 'Regulatory': 0.0, 'Sales': 0.6, 'M&A': 0.0, 'Legal': 0.0, 'Dividend Policy': 0.0, 'Risks': 0.0, 'Rumors': 0.0, 'Strategy': 0.6, 'Options': 0.0, 'IPO': 0.0, 'Signal': 0.5, 'Coverage': 0.0, 'Fundamentals': 0.6, 'Insider Activity': 0.0, 'Price Action': 0.4, 'Buyside': 0.0, 'Technical Analysis': 0.0, 'Trade': 0.0, 'Central Banks': 0.0, 'Currency': 0.0, 'Conditions': 0.0, 'Market': 0.5, 'Volatility': 0.0, 'Investor Sentiment': 0.6, 'Retail Investor Behavior': 0.0, 'Speculation': 0.0, 'Domestic Institutional Behavior': 0.0, 'Foreign Institutional Behavior': 0.0, 'Black Swan Event': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PhÃ¢n tÃ­ch tin tá»©c: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ cháº¥m Ä‘iá»ƒm xÃºc cáº£m Ä‘a khÃ­a cáº¡nh cho táº¥t cáº£ tin tá»©c\n",
      "Äang cháº¥m Ä‘iá»ƒm cÃ¡c tuyÃªn bá»‘ chÃ­nh thá»©c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ cháº¥m Ä‘iá»ƒm xÃºc cáº£m cho cÃ¡c nghá»‹ quyáº¿t, nghá»‹ Ä‘á»‹nh cá»§a cÃ´ng ty\n",
      "Äang káº¿t há»£p cÃ¡c káº¿t quáº£ cháº¥m Ä‘iá»ƒm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>time</th>\n",
       "      <th>tittle</th>\n",
       "      <th>url</th>\n",
       "      <th>Content</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>Company Communication</th>\n",
       "      <th>Appointment</th>\n",
       "      <th>Financial</th>\n",
       "      <th>Regulatory</th>\n",
       "      <th>...</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Market</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Investor Sentiment</th>\n",
       "      <th>Retail Investor Behavior</th>\n",
       "      <th>Speculation</th>\n",
       "      <th>Domestic Institutional Behavior</th>\n",
       "      <th>Foreign Institutional Behavior</th>\n",
       "      <th>Black Swan Event</th>\n",
       "      <th>official_statement_pdf_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VHM</td>\n",
       "      <td>30/07/2025 15:18</td>\n",
       "      <td>Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 th...</td>\n",
       "      <td>https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...</td>\n",
       "      <td>Táº¡i thá»i Ä‘iá»ƒm 30/06/2025, tá»•ng tÃ i sáº£n VHM Ä‘áº¡t...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VHM</td>\n",
       "      <td>31/07/2025 17:18</td>\n",
       "      <td>VHM: BÃ¡o cÃ¡o tÃ¬nh hÃ¬nh quáº£n trá»‹ 6 thÃ¡ng Ä‘áº§u nÄƒ...</td>\n",
       "      <td>https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...</td>\n",
       "      <td>https://cafef1.mediacdn.vn/download/310725/vhm...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker              time                                             tittle  \\\n",
       "0    VHM  30/07/2025 15:18  Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 th...   \n",
       "1    VHM  31/07/2025 17:18  VHM: BÃ¡o cÃ¡o tÃ¬nh hÃ¬nh quáº£n trá»‹ 6 thÃ¡ng Ä‘áº§u nÄƒ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://cafef.vn/vinhomes-vhm-bao-lai-11000-ty...   \n",
       "1  https://cafef.vn/du-lieu/VHM-2260054/vhm-bao-c...   \n",
       "\n",
       "                                             Content Reputation  \\\n",
       "0  Táº¡i thá»i Ä‘iá»ƒm 30/06/2025, tá»•ng tÃ i sáº£n VHM Ä‘áº¡t...        0.5   \n",
       "1  https://cafef1.mediacdn.vn/download/310725/vhm...        0.2   \n",
       "\n",
       "  Company Communication Appointment Financial Regulatory  ... Conditions  \\\n",
       "0                   0.3         0.0       0.7        0.0  ...        0.0   \n",
       "1                   0.2         0.3       0.0        0.1  ...        0.0   \n",
       "\n",
       "  Market Volatility Investor Sentiment Retail Investor Behavior Speculation  \\\n",
       "0    0.5        0.0                0.6                      0.0         0.0   \n",
       "1    0.0        0.0                0.0                      0.0         0.0   \n",
       "\n",
       "  Domestic Institutional Behavior Foreign Institutional Behavior  \\\n",
       "0                             0.0                            0.0   \n",
       "1                             0.0                            0.0   \n",
       "\n",
       "  Black Swan Event                        official_statement_pdf_name  \n",
       "0              0.0                                                NaN  \n",
       "1              0.0  vhm-bao-cao-tinh-hinh-quan-tri-6-thang-dau-nam...  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_excel_input = rf\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\df_news_official_stament.xlsx\"\n",
    "folder_pdf_input = rf\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\official_statement_pdf\"\n",
    "pdf_path_init_user_prompt = r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\SA\\data\\v2\\sentiment_score_data\\official_statement\\example_pdf_score\\acb-giai-trinh-chenh-lech-lnst-quy-1-2023-so-voi-cung-ky-nam-truoc-0.pdf\"\n",
    "df_news_official_statement_scored = sentiment_score_for_all('VHM',df_news,folder_excel_input, folder_pdf_input, pdf_path_init_user_prompt)\n",
    "df_news_official_statement_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "efdc361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_norm(sentiment_df):\n",
    "    # XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t khÃ­a cáº¡nh cáº£m xÃºc (giáº£ sá»­ cÃ¡c cá»™t nÃ y cÃ³ tÃªn nhÆ° dÆ°á»›i)\n",
    "    sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "\n",
    "    # Äáº£m báº£o cÃ¡c cá»™t nÃ y Ä‘á»u tá»“n táº¡i trong DataFrame\n",
    "    existing_columns = [col for col in sentiment_columns if col in sentiment_df.columns]\n",
    "\n",
    "    # Thay NaN báº±ng 0 náº¿u chÆ°a lÃ m\n",
    "    sentiment_df[existing_columns] = sentiment_df[existing_columns].fillna(0)\n",
    "\n",
    "    # TÃ­nh Euclidean norm cho tá»«ng hÃ ng\n",
    "    sentiment_df['euclidean norm'] = np.linalg.norm(sentiment_df[existing_columns].values, axis=1)\n",
    "\n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c14bde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(name: str) -> str:\n",
    "    return name.strip().lower().replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c1a1b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_for_ta_fa(ta_fa_df, sentiment_df):\n",
    "    ta_fa_df = ta_fa_df.reset_index(drop=True)\n",
    "    # Xá»­ lÃ½ thiáº¿u vÃ  chuáº©n hÃ³a\n",
    "    sentiment_df.fillna(0, inplace=True)\n",
    "    sentiment_df = compute_sentiment_norm(sentiment_df)\n",
    "    # sentiment_df.to_excel(r\"D:\\sentiment_df.xlsx\", index=False)\n",
    "\n",
    "    # Äá»‹nh dáº¡ng thá»i gian\n",
    "    sentiment_df['time'] = pd.to_datetime(sentiment_df['time'], dayfirst=True, errors='coerce')\n",
    "    ta_fa_df['time'] = pd.to_datetime(ta_fa_df['time'], dayfirst=True)\n",
    "\n",
    "    # Danh sÃ¡ch cá»™t cáº£m xÃºc\n",
    "    sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "\n",
    "    # ThÃªm cÃ¡c cá»™t cáº£m xÃºc vÃ o `ta_fa_df`, máº·c Ä‘á»‹nh lÃ  0\n",
    "    for col in sentiment_columns:\n",
    "        ta_fa_df[col] = 0.0\n",
    "\n",
    "    for i in range(len(ta_fa_df)):\n",
    "        current_date = ta_fa_df.at[i, 'time']\n",
    "        # next_trading_date = ta_fa_df.at[i + 1, 'time']\n",
    "        try:\n",
    "            next_trading_date = ta_fa_df.at[i + 1, 'time']\n",
    "        except:\n",
    "            next_trading_date = current_date + timedelta(days=1)\n",
    "        from_time = datetime.combine(current_date, datetime.min.time()) + timedelta(hours=14, minutes=45)\n",
    "        to_time = datetime.combine(next_trading_date, datetime.min.time()) + timedelta(hours=14, minutes=45)\n",
    "\n",
    "        mask = (sentiment_df['time'] >= from_time) & (sentiment_df['time'] < to_time)\n",
    "        news_in_range = sentiment_df[mask]\n",
    "\n",
    "        print(f\"\\nðŸ“… NgÃ y Ä‘ang xÃ©t: {current_date.date()}\")\n",
    "        print(f\"ðŸ“… NgÃ y giao dá»‹ch tiáº¿p theo: {next_trading_date.date()}\")\n",
    "        print(f\"ðŸ“° CÃ¡c tin tá»©c tá»« {from_time} Ä‘áº¿n {to_time} sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n nhÃ£n táº¡i ngÃ y {current_date.date()}:\")\n",
    "\n",
    "        if news_in_range.empty:\n",
    "            print(\" - KhÃ´ng cÃ³ tin tá»©c áº£nh hÆ°á»Ÿng.\")\n",
    "            print(\"ðŸ“Š CÃ¡c khÃ­a cáº¡nh cáº£m xÃºc (máº·c Ä‘á»‹nh 0):\")\n",
    "            continue\n",
    "\n",
    "        max_norm_idx = news_in_range['euclidean norm'].idxmax()\n",
    "        max_news = news_in_range.loc[max_norm_idx]\n",
    "\n",
    "        for idx, row in news_in_range.iterrows():\n",
    "            title = row.get('tittle', '[KhÃ´ng cÃ³ tiÃªu Ä‘á»]')\n",
    "            time_str = row['time']\n",
    "            print(f\" - {time_str}: {title}\")\n",
    "            for col in sentiment_columns:\n",
    "                    print(f\"   - {col}: {row[col]}\")\n",
    "            if idx == max_norm_idx:\n",
    "                print(\"  ðŸ” ÄÃ¢y lÃ  tin cÃ³ euclidean norm cao nháº¥t.\")\n",
    "                print(\"  ðŸ“Š CÃ¡c khÃ­a cáº¡nh cáº£m xÃºc:\")\n",
    "                for col in sentiment_columns:\n",
    "                    print(f\"   - {col}: {row[col]}\")\n",
    "\n",
    "        # Náº¿u muá»‘n tÃ­nh tá»•ng há»£p nhiá»u tin trong ngÃ y\n",
    "        ta_fa_df.at[i, 'num_sa_news'] = len(news_in_range)\n",
    "        ta_fa_df.at[i, 'mean_sentiment_norm'] = news_in_range['euclidean norm'].mean()\n",
    "        ta_fa_df.at[i, 'max_sentiment_norm'] = news_in_range['euclidean norm'].max()\n",
    "        ta_fa_df.at[i, 'std_sentiment_score'] = news_in_range[sentiment_columns].std().mean()\n",
    "\n",
    "        # TÃ­nh tá»•ng cÃ¡c cáº£m xÃºc tÃ­ch cá»±c / tiÃªu cá»±c\n",
    "        pos_sum = news_in_range[sentiment_columns].applymap(lambda x: x if x > 0 else 0).sum().sum()\n",
    "        neg_sum = news_in_range[sentiment_columns].applymap(lambda x: x if x < 0 else 0).sum().sum()\n",
    "\n",
    "        ta_fa_df.at[i, 'sum_positive_sa'] = pos_sum\n",
    "        ta_fa_df.at[i, 'sum_negative_sa'] = neg_sum\n",
    "\n",
    "        # âž• TÃ­nh khoáº£ng cÃ¡ch Ä‘áº¿n 14:45 hÃ´m Ä‘Ã³ (Ã¢m náº¿u tin xáº£y ra trÆ°á»›c giá» Ä‘Ã³ng cá»­a)\n",
    "        news_time = max_news['time']\n",
    "        cutoff_time = datetime.combine(current_date, datetime.min.time()) + timedelta(hours=14, minutes=45)\n",
    "        time_distance_minutes = (cutoff_time - news_time).total_seconds() / 60.0\n",
    "        ta_fa_df.at[i, 'time_distance_from_sa_to_close_minutes'] = time_distance_minutes\n",
    "\n",
    "        # GÃ¡n cÃ¡c giÃ¡ trá»‹ cáº£m xÃºc vÃ o ta_fa_df táº¡i dÃ²ng i\n",
    "        for col in sentiment_columns:\n",
    "            ta_fa_df.at[i, col] = max_news[col]\n",
    "    \n",
    "    # Táº¡o cÃ¡c cá»™t hiá»‡u á»©ng cháº­m tá»« cáº£m xÃºc hÃ´m trÆ°á»›c\n",
    "\n",
    "    for col in sentiment_columns:\n",
    "        ta_fa_df[f\"{to_snake_case(col)}_p1d\"] = ta_fa_df[col].shift(1)\n",
    "\n",
    "    meta_cols = ['num_sa_news', 'mean_sentiment_norm', 'max_sentiment_norm', 'std_sentiment_score', 'sum_positive_sa', 'sum_negative_sa', 'time_distance_from_sa_to_close_minutes']\n",
    "    for col in meta_cols:\n",
    "        ta_fa_df[f\"{col}_p1d\"] = ta_fa_df[col].shift(1)\n",
    "    ta_fa_df.fillna(0, inplace=True)\n",
    "\n",
    "    return ta_fa_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1fb4c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“… NgÃ y Ä‘ang xÃ©t: 2025-07-30\n",
      "ðŸ“… NgÃ y giao dá»‹ch tiáº¿p theo: 2025-07-31\n",
      "ðŸ“° CÃ¡c tin tá»©c tá»« 2025-07-30 14:45:00 Ä‘áº¿n 2025-07-31 14:45:00 sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n nhÃ£n táº¡i ngÃ y 2025-07-30:\n",
      " - 2025-07-30 15:18:00: Vinhomes (VHM) bÃ¡o lÃ£i 11.000 tá»· Ä‘á»“ng sau 6 thÃ¡ng Ä‘áº§u nÄƒm 2025, cáº§m gáº§n 49.000 tá»· Ä‘á»“ng tiá»n máº·t\n",
      "   - Reputation: 0.5\n",
      "   - Company Communication: 0.3\n",
      "   - Appointment: 0.0\n",
      "   - Financial: 0.7\n",
      "   - Regulatory: 0.0\n",
      "   - Sales: 0.6\n",
      "   - M&A: 0.0\n",
      "   - Legal: 0.0\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.6\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.5\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.6\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.4\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.5\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.6\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n",
      "  ðŸ” ÄÃ¢y lÃ  tin cÃ³ euclidean norm cao nháº¥t.\n",
      "  ðŸ“Š CÃ¡c khÃ­a cáº¡nh cáº£m xÃºc:\n",
      "   - Reputation: 0.5\n",
      "   - Company Communication: 0.3\n",
      "   - Appointment: 0.0\n",
      "   - Financial: 0.7\n",
      "   - Regulatory: 0.0\n",
      "   - Sales: 0.6\n",
      "   - M&A: 0.0\n",
      "   - Legal: 0.0\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.6\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.5\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.6\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.4\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.5\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.6\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n",
      "\n",
      "ðŸ“… NgÃ y Ä‘ang xÃ©t: 2025-07-31\n",
      "ðŸ“… NgÃ y giao dá»‹ch tiáº¿p theo: 2025-08-01\n",
      "ðŸ“° CÃ¡c tin tá»©c tá»« 2025-07-31 14:45:00 Ä‘áº¿n 2025-08-01 14:45:00 sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n nhÃ£n táº¡i ngÃ y 2025-07-31:\n",
      " - 2025-07-31 17:18:00: VHM: BÃ¡o cÃ¡o tÃ¬nh hÃ¬nh quáº£n trá»‹ 6 thÃ¡ng Ä‘áº§u nÄƒm 2025\n",
      "   - Reputation: 0.2\n",
      "   - Company Communication: 0.2\n",
      "   - Appointment: 0.3\n",
      "   - Financial: 0.0\n",
      "   - Regulatory: 0.1\n",
      "   - Sales: 0.0\n",
      "   - M&A: 0.2\n",
      "   - Legal: 0.1\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.1\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.0\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.0\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.0\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.0\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.0\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n",
      "  ðŸ” ÄÃ¢y lÃ  tin cÃ³ euclidean norm cao nháº¥t.\n",
      "  ðŸ“Š CÃ¡c khÃ­a cáº¡nh cáº£m xÃºc:\n",
      "   - Reputation: 0.2\n",
      "   - Company Communication: 0.2\n",
      "   - Appointment: 0.3\n",
      "   - Financial: 0.0\n",
      "   - Regulatory: 0.1\n",
      "   - Sales: 0.0\n",
      "   - M&A: 0.2\n",
      "   - Legal: 0.1\n",
      "   - Dividend Policy: 0.0\n",
      "   - Risks: 0.0\n",
      "   - Rumors: 0.0\n",
      "   - Strategy: 0.1\n",
      "   - Options: 0.0\n",
      "   - IPO: 0.0\n",
      "   - Signal: 0.0\n",
      "   - Coverage: 0.0\n",
      "   - Fundamentals: 0.0\n",
      "   - Insider Activity: 0.0\n",
      "   - Price Action: 0.0\n",
      "   - Buyside: 0.0\n",
      "   - Technical Analysis: 0.0\n",
      "   - Trade: 0.0\n",
      "   - Central Banks: 0.0\n",
      "   - Currency: 0.0\n",
      "   - Conditions: 0.0\n",
      "   - Market: 0.0\n",
      "   - Volatility: 0.0\n",
      "   - Investor Sentiment: 0.0\n",
      "   - Retail Investor Behavior: 0.0\n",
      "   - Speculation: 0.0\n",
      "   - Domestic Institutional Behavior: 0.0\n",
      "   - Foreign Institutional Behavior: 0.0\n",
      "   - Black Swan Event: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>p/b_previous_quarter</th>\n",
       "      <th>P/B_Same_Period_Last_Year</th>\n",
       "      <th>P/B_d_1</th>\n",
       "      <th>P/B_d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>domestic_institutional_behavior_p1d</th>\n",
       "      <th>foreign_institutional_behavior_p1d</th>\n",
       "      <th>black_swan_event_p1d</th>\n",
       "      <th>num_sa_news_p1d</th>\n",
       "      <th>mean_sentiment_norm_p1d</th>\n",
       "      <th>max_sentiment_norm_p1d</th>\n",
       "      <th>std_sentiment_score_p1d</th>\n",
       "      <th>sum_positive_sa_p1d</th>\n",
       "      <th>sum_negative_sa_p1d</th>\n",
       "      <th>time_distance_from_sa_to_close_minutes_p1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.5</td>\n",
       "      <td>5198000</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>89.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>88.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7811500</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.027068</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>1.776398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.711724</td>\n",
       "      <td>1.711724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 396 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  open  high   low  close   volume  p/b_previous_quarter  \\\n",
       "0 2025-07-30  92.1  92.5  89.0   91.5  5198000              1.971749   \n",
       "1 2025-07-31  89.7  91.5  88.2   90.0  7811500              1.971749   \n",
       "\n",
       "   P/B_Same_Period_Last_Year   P/B_d_1   P/B_d_2  ...  \\\n",
       "0                   1.027068  1.971749  1.776398  ...   \n",
       "1                   1.027068  1.971749  1.776398  ...   \n",
       "\n",
       "   domestic_institutional_behavior_p1d  foreign_institutional_behavior_p1d  \\\n",
       "0                                  0.0                                 0.0   \n",
       "1                                  0.0                                 0.0   \n",
       "\n",
       "   black_swan_event_p1d  num_sa_news_p1d  mean_sentiment_norm_p1d  \\\n",
       "0                   0.0              0.0                 0.000000   \n",
       "1                   0.0              1.0                 1.711724   \n",
       "\n",
       "   max_sentiment_norm_p1d  std_sentiment_score_p1d  sum_positive_sa_p1d  \\\n",
       "0                0.000000                      0.0                  0.0   \n",
       "1                1.711724                      0.0                  5.3   \n",
       "\n",
       "   sum_negative_sa_p1d  time_distance_from_sa_to_close_minutes_p1d  \n",
       "0                  0.0                                         0.0  \n",
       "1                  0.0                                       -33.0  \n",
       "\n",
       "[2 rows x 396 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta_fa_sa_df = apply_sentiment_for_ta_fa(df_ta_fa, df_news_official_statement_scored)\n",
    "ta_fa_sa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6d9acb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_fa_sa_df.to_excel(r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\data_experiment_predict\\ta_fa_sa_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d934ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "01aac4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ load xong 35865 dÃ²ng dá»¯ liá»‡u tá»« 30 cá»• phiáº¿u.\n",
      "âœ… ÄÃ£ load xong 7380 dÃ²ng dá»¯ liá»‡u tá»« 30 cá»• phiáº¿u.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:51:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6493224932249323\n"
     ]
    }
   ],
   "source": [
    "# traning model\n",
    "def dataframe_to_x_y(df, feature, target):\n",
    "  X_train, y_train = [], []\n",
    "  X_val, y_val = [], []\n",
    "  n = len(df)\n",
    "  split_index = int(n * 0.8)\n",
    "  X_train = df[feature][:split_index]\n",
    "  y_train = df[target][:split_index]\n",
    "  X_val = df[feature][split_index:]\n",
    "  y_val = df[target][split_index:]\n",
    "  return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "sentiment_columns = [\n",
    "        'Reputation', 'Company Communication', 'Appointment', 'Financial', 'Regulatory',\n",
    "        'Sales', 'M&A', 'Legal', 'Dividend Policy', 'Risks', 'Rumors', 'Strategy',\n",
    "        'Options', 'IPO', 'Signal', 'Coverage', 'Fundamentals', 'Insider Activity',\n",
    "        'Price Action', 'Buyside', 'Technical Analysis', 'Trade', 'Central Banks',\n",
    "        'Currency', 'Conditions', 'Market', 'Volatility', 'Investor Sentiment', 'Retail Investor Behavior',\n",
    "        'Speculation', 'Domestic Institutional Behavior', 'Foreign Institutional Behavior', 'Black Swan Event',\n",
    "    ]\n",
    "\n",
    "sentiment_p1d_columns = [\n",
    "        'reputation_p1d', 'company_communication_p1d', 'appointment_p1d', 'financial_p1d', 'regulatory_p1d',\n",
    "        'sales_p1d', 'm&a_p1d', 'legal_p1d', 'dividend_policy_p1d', 'risks_p1d', 'rumors_p1d', 'strategy_p1d',\n",
    "        'options_p1d', 'ipo_p1d', 'signal_p1d', 'coverage_p1d', 'fundamentals_p1d', 'insider_activity_p1d',\n",
    "        'price_action_p1d', 'buyside_p1d', 'technical_analysis_p1d', 'trade_p1d', 'central_banks_p1d',\n",
    "        'currency_p1d', 'conditions_p1d', 'market_p1d', 'volatility_p1d', 'investor_sentiment_p1d', 'retail_investor_behavior_p1d',\n",
    "        'speculation_p1d', 'domestic_institutional_behavior_p1d', 'foreign_institutional_behavior_p1d', 'black_swan_event_p1d',\n",
    "    ]\n",
    "meta_cols = ['num_sa_news', 'mean_sentiment_norm', 'max_sentiment_norm', 'std_sentiment_score', 'sum_positive_sa', 'sum_negative_sa', 'time_distance_from_sa_to_close_minutes']\n",
    "meta_p1d_cols = ['num_sa_news_p1d', 'mean_sentiment_norm_p1d', 'max_sentiment_norm_p1d', 'std_sentiment_score_p1d', 'sum_positive_sa_p1d', 'sum_negative_sa_p1d', 'time_distance_from_sa_to_close_minutes_p1d']\n",
    "\n",
    "sentiment_feature_selected =  sentiment_columns + sentiment_p1d_columns + meta_cols + meta_p1d_cols\n",
    "ta_features = ['volume_ma','volume_to_volume_ma_ratio','ema_12','ema_26','sma_20','sma_50','roc_5','roc_1','roc_9','%K','%R','cci','obv','macd','signal_line','macd_histogram','rsi','rsi_base_ma','rsi_rsi_base_ma_ratio','bb_bbm','bb_bbh','bb_bbl','bb_bbp','bb_bbh_bb_bbl_ratio','hl_ratio', 'co_ratio', 'price_range', 'sma_ratio_20_50', 'ema_ratio_12_26', 'bb_width', 'bb_position', 'rsi_overbought', 'rsi_oversold', 'rsi_neutral', 'macd_bullish', 'momentum_5', 'momentum_10','log_return','volatility_5d','volatility_10d','volatility_20d','volatility_30d','mean_log_return_5d','mean_log_return_10d','mean_log_return_20d','mean_log_return_30d','sharpe_like_5d','sharpe_like_10d','sharpe_like_20d','sharpe_like_30d','up_streak','pos_log_return_ratio_20d','z_score_5d','z_score_10d','z_score_20d','z_score_30d','annual_return','daily_return','sharpe_ratio',\n",
    "               'rsi_vn30','rsi_base_ma_vn30','rsi_rsi_base_ma_ratio_vn30','volume_ma_vn30','volume_to_volume_ma_ratio_vn30','bb_bbm_vn30','bb_bbh_vn30','bb_bbl_vn30','bb_bbp_vn30','bb_bbh_bb_bbl_ratio_vn30','roc_1_vn30', 'roc_5_vn30', 'roc_9_vn30','%K_vn30','%R_vn30','cci_vn30','obv_vn30','ema_12_vn30','ema_26_vn30','sma_20_vn30','sma_50_vn30', 'hl_ratio_vn30', 'co_ratio_vn30', 'price_range_vn30', 'sma_ratio_20_50_vn30', 'ema_ratio_12_26_vn30', 'bb_width_vn30', 'bb_position_vn30', 'rsi_overbought_vn30', 'rsi_oversold_vn30', 'rsi_neutral_vn30', 'momentum_5_vn30', 'momentum_10_vn30', 'log_return_vn30','volatility_5d_vn30','volatility_10d_vn30','volatility_20d_vn30','volatility_30d_vn30','mean_log_return_5d_vn30','mean_log_return_10d_vn30','mean_log_return_20d_vn30','mean_log_return_30d_vn30','sharpe_like_5d_vn30','sharpe_like_10d_vn30','sharpe_like_20d_vn30','sharpe_like_30d_vn30','up_streak_vn30','pos_log_return_ratio_20d_vn30','z_score_5d_vn30','z_score_10d_vn30','z_score_20d_vn30','z_score_30d_vn30','annual_return_vn30','daily_return_vn30','sharpe_ratio_vn30',\n",
    "               'rsi_vni','rsi_base_ma_vni','rsi_rsi_base_ma_ratio_vni','volume_ma_vni','volume_to_volume_ma_ratio_vni','bb_bbm_vni','bb_bbh_vni','bb_bbl_vni','bb_bbp_vni','bb_bbh_bb_bbl_ratio_vni','roc_1_vni', 'roc_5_vni', 'roc_9_vni','%K_vni','%R_vni','cci_vni','obv_vni','ema_12_vni','ema_26_vni','sma_20_vni','sma_50_vni', 'hl_ratio_vni', 'co_ratio_vni', 'price_range_vni', 'sma_ratio_20_50_vni', 'ema_ratio_12_26_vni', 'bb_width_vni', 'bb_position_vni', 'rsi_overbought_vni', 'rsi_oversold_vni', 'rsi_neutral_vni', 'momentum_5_vni', 'momentum_10_vni','log_return_vni','volatility_5d_vni','volatility_10d_vni','volatility_20d_vni','volatility_30d_vni','mean_log_return_5d_vni','mean_log_return_10d_vni','mean_log_return_20d_vni','mean_log_return_30d_vni','sharpe_like_5d_vni','sharpe_like_10d_vni','sharpe_like_20d_vni','sharpe_like_30d_vni','up_streak_vni','pos_log_return_ratio_20d_vni','z_score_5d_vni','z_score_10d_vni','z_score_20d_vni','z_score_30d_vni','annual_return_vni','daily_return_vni','sharpe_ratio_vni']\n",
    "fa_features = ['p/b_previous_quarter', 'p/b_change_rate','p/b_change_rate_flag','p/e_previous_quarter','p/e_change_rate','p/e_change_rate_flag','p/s_previous_quarter','p/s_change_rate','p/s_change_rate_flag','p/cash_flow_previous_quarter','p/cash_flow_change_rate','p/cash_flow_change_rate_flag','eps_previous_quarter','eps_change_rate', 'eps_change_rate_flag','bvps_previous_quarter','bvps_change_rate', 'bvps_change_rate_flag','roe_previous_quarter','roe_change_rate','roe_change_rate_flag','roa_previous_quarter','roa_change_rate','roa_change_rate_flag','coefficient_p/b','coefficient_p/e','coefficient_p/s','coefficient_p/cash_flow','coefficient_eps','coefficient_bvps','coefficient_roe','coefficient_roa','distance_to_nearest_quarter']\n",
    "ta_fa_feature_selected = ta_features + fa_features\n",
    "features = ta_fa_feature_selected + sentiment_feature_selected\n",
    "target = 'target'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def load_data_for_analysis(tickers, folder_path):\n",
    "\n",
    "    df_all = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        file_path = os.path.join(folder_path, f\"{ticker}.xlsx\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            df[\"ticker\"] = ticker  # thÃªm cá»™t ticker\n",
    "            df_all.append(df)\n",
    "        else:\n",
    "            print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y: {file_path}\")\n",
    "\n",
    "    # Gá»™p láº¡i thÃ nh 1 DataFrame\n",
    "    df_global = pd.concat(df_all, ignore_index=True)\n",
    "    print(f\"âœ… ÄÃ£ load xong {len(df_global)} dÃ²ng dá»¯ liá»‡u tá»« {len(df_all)} cá»• phiáº¿u.\")\n",
    "    return df_global\n",
    "\n",
    "tickers = ['ACB', 'BCM', 'BID','BVH','CTG','FPT','GAS','GVR','HDB','HPG',\n",
    "                    'LPB','MBB','MSN','MWG','PLX','SAB','SHB','SSB','SSI','STB',\n",
    "                    'TCB','TPB','VCB','VHM','VIB','VIC','VJC','VNM','VPB','VRE']\n",
    "folder_path_train = r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA\\TA_FA_SA\\data\\v6\\stock_trend\\train_data\" \n",
    "folder_path_test = r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA\\TA_FA_SA\\data\\v6\\stock_trend\\test_data\" \n",
    "df_global_train = load_data_for_analysis(tickers, folder_path_train)\n",
    "df_global_test = load_data_for_analysis(tickers, folder_path_test)\n",
    "\n",
    "X_train_with_sa = df_global_train[features]  # features = ta + fa + sa\n",
    "X_test_with_sa = df_global_test[features]\n",
    "y_train = df_global_train[target]\n",
    "y_test = df_global_test[target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_with_sa_scaled = scaler.fit_transform(X_train_with_sa)   # Fit scaler trÃªn train, scale train luÃ´n\n",
    "X_test_with_sa_scaled = scaler.transform(X_test_with_sa)\n",
    "xgb_model = XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=42, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='mlogloss',\n",
    "        n_estimators = 1089,\n",
    "        max_depth = 8, \n",
    "        learning_rate = 0.010501883895575981, \n",
    "        subsample = 0.9005472962597326, \n",
    "        colsample_bytree = 0.22675172202304014\n",
    "    )\n",
    "xgb_model.fit(X_train_with_sa_scaled, y_train)\n",
    "y_pred = xgb_model.predict(X_test_with_sa_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4187f924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\thacsi\\\\TAILIEULUANVAN\\\\code\\\\PredictStock_TA_FA_SA - Copy\\\\TA_FA_SA\\\\code\\\\v5\\\\stock_trend\\\\model_scaler\\\\scaler.pkl']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# LÆ°u model\n",
    "joblib.dump(xgb_model, r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\xgb_model.pkl\")\n",
    "\n",
    "# LÆ°u scaler\n",
    "joblib.dump(scaler, r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca4468c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u: 2\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "loaded_model = joblib.load(r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\xgb_model.pkl\")\n",
    "\n",
    "# Load scaler\n",
    "loaded_scaler = joblib.load(r\"D:\\thacsi\\TAILIEULUANVAN\\code\\PredictStock_TA_FA_SA - Copy\\TA_FA_SA\\code\\v5\\stock_trend\\model_scaler\\scaler.pkl\")\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n dá»¯ liá»‡u má»›i\n",
    "X_new = ta_fa_sa_df.tail(1)[features]\n",
    "X_new_scaled = loaded_scaler.transform(X_new)  # scale trÆ°á»›c\n",
    "y_new_pred = loaded_model.predict(X_new_scaled)\n",
    "print(\"Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u:\", y_new_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533f065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
